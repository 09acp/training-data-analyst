{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation using tensor2tensor on Cloud ML Engine\n",
    "\n",
    "This notebook illustrates using the <a href=\"https://github.com/tensorflow/tensor2tensor\">tensor2tensor</a> library to do from-scratch, distributed training of a English-German translator. Then, the trained model is deployed to Cloud ML Engine and used to translate new pieces of text.\n",
    "<p/>\n",
    "### Install tensor2tensor, and specify Google Cloud Platform project and bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "pip install tensor2tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT = 'cloud-training-demos' # REPLACE WITH YOUR PROJECT ID\n",
    "BUCKET = 'cloud-training-demos-ml' # REPLACE WITH YOUR BUCKET NAME\n",
    "REGION = 'us-central1' # REPLACE WITH YOUR BUCKET REGION e.g. us-central1\n",
    "\n",
    "# for bash\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "wget http://data.statmt.org/wmt17/translation-task/training-parallel-nc-v12.tgz\n",
    "wget http://data.statmt.org/wmt17/translation-task/dev.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "gsutil cp -m training-parallel-nc-v12.tgz dev.tgz gs://${BUCKET}/translate_ende/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a Problem\n",
    "The Problem in tensor2tensor is where you specify parameters like the size of your vocabulary and where to get the training data from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "rm -rf ende\n",
    "mkdir ende\n",
    "touch ende/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ende/problem.py\n"
     ]
    }
   ],
   "source": [
    "%writefile ende/problem.py\n",
    "import tensorflow as tf\n",
    "from tensor2tensor.data_generators import generator_utils\n",
    "from tensor2tensor.data_generators import problem\n",
    "from tensor2tensor.data_generators import text_encoder\n",
    "from tensor2tensor.data_generators import wsj_parsing\n",
    "from tensor2tensor.data_generators.wmt import TranslateProblem\n",
    "from tensor2tensor.utils import registry\n",
    "\n",
    "_ENDE_TRAIN_DATASETS = [\n",
    "    [\n",
    "        \"./training-parallel-nc-v12.tgz\",\n",
    "        (\"training/news-commentary-v12.de-en.en\",\n",
    "         \"training/news-commentary-v12.de-en.de\")\n",
    "    ],\n",
    "]\n",
    "_ENDE_TEST_DATASETS = [\n",
    "    [\n",
    "        \"./dev.tgz\",\n",
    "        (\"dev/newstest2013.en\", \"dev/newstest2013.de\")\n",
    "    ],\n",
    "]\n",
    "\n",
    "@registry.register_problem\n",
    "class MyTranslateProblem(TranslateProblem):\n",
    "  @property\n",
    "  def targeted_vocab_size(self):\n",
    "    return 2**13  # 8192\n",
    "\n",
    "  def generator(self, data_dir, tmp_dir, train):\n",
    "    symbolizer_vocab = generator_utils.get_or_generate_vocab(\n",
    "        data_dir, tmp_dir, self.vocab_file, self.targeted_vocab_size)\n",
    "    datasets = _ENDE_TRAIN_DATASETS if train else _ENDE_TEST_DATASETS\n",
    "    tag = \"train\" if train else \"dev\"\n",
    "    data_path = _compile_data(tmp_dir, datasets, \"wmt_ende_tok_%s\" % tag)\n",
    "    return token_generator(data_path + \".lang1\", data_path + \".lang2\",\n",
    "                           symbolizer_vocab, EOS)\n",
    "\n",
    "  @property\n",
    "  def input_space_id(self):\n",
    "    return problem.SpaceID.EN_TOK\n",
    "\n",
    "  @property\n",
    "  def target_space_id(self):\n",
    "    return problem.SpaceID.DE_TOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__.py  problem.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls ende"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Importing user module ende from path .\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/t2t-datagen\", line 213, in <module>\n",
      "    tf.app.run()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 48, in run\n",
      "    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\n",
      "  File \"/usr/local/bin/t2t-datagen\", line 160, in main\n",
      "    raise ValueError(error_msg)\n",
      "ValueError: You must specify one of the supported problems to generate data for:\n",
      "  * algorithmic_addition_binary40\n",
      "  * algorithmic_addition_decimal40\n",
      "  * algorithmic_algebra_inverse\n",
      "  * algorithmic_cipher_shift200\n",
      "  * algorithmic_cipher_shift5\n",
      "  * algorithmic_cipher_vigenere200\n",
      "  * algorithmic_cipher_vigenere5\n",
      "  * algorithmic_identity_binary40\n",
      "  * algorithmic_identity_decimal40\n",
      "  * algorithmic_multiplication_binary40\n",
      "  * algorithmic_multiplication_decimal40\n",
      "  * algorithmic_reverse_binary40\n",
      "  * algorithmic_reverse_decimal40\n",
      "  * algorithmic_reverse_nlplike32k\n",
      "  * algorithmic_reverse_nlplike8k\n",
      "  * algorithmic_shift_decimal40\n",
      "  * audio_timit_characters_test\n",
      "  * audio_timit_characters_tune\n",
      "  * audio_timit_tokens8k_test\n",
      "  * audio_timit_tokens8k_tune\n",
      "  * audio_timit_tokens_32k_test\n",
      "  * audio_timit_tokens_8k_test\n",
      "  * image_celeba_tune\n",
      "  * image_cifar10\n",
      "  * image_cifar10_plain\n",
      "  * image_cifar10_tune\n",
      "  * image_fsns\n",
      "  * image_imagenet\n",
      "  * image_imagenet32\n",
      "  * image_mnist\n",
      "  * image_mnist_tune\n",
      "  * image_ms_coco_characters\n",
      "  * image_ms_coco_tokens32k\n",
      "  * image_ms_coco_tokens8k\n",
      "  * img2img_imagenet\n",
      "  * inference_snli32k\n",
      "  * languagemodel_lm1b32k\n",
      "  * languagemodel_lm1b_characters\n",
      "  * languagemodel_ptb10k\n",
      "  * languagemodel_ptb_characters\n",
      "  * languagemodel_wiki_full32k\n",
      "  * languagemodel_wiki_scramble128\n",
      "  * languagemodel_wiki_scramble1k50\n",
      "  * languagemodel_wiki_scramble8k50\n",
      "  * parsing_english_ptb16k\n",
      "  * parsing_english_ptb16k\n",
      "  * parsing_english_ptb8k\n",
      "  * parsing_english_ptb8k\n",
      "  * parsing_icelandic16k\n",
      "  * programming_desc2code_cpp\n",
      "  * programming_desc2code_py\n",
      "  * sentiment_imdb\n",
      "  * summarize_cnn_dailymail32k\n",
      "  * translate_encs_wmt32k\n",
      "  * translate_encs_wmt_characters\n",
      "  * translate_ende_wmt32k\n",
      "  * translate_ende_wmt8k\n",
      "  * translate_ende_wmt_bpe32k\n",
      "  * translate_ende_wmt_characters\n",
      "  * translate_enfr_wmt32k\n",
      "  * translate_enfr_wmt8k\n",
      "  * translate_enfr_wmt_characters\n",
      "  * translate_enmk_setimes32k\n",
      "  * translate_enzh_wmt8k\n",
      "TIMIT and parsing need data_sets specified with --timit_paths and --parsing_path.\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "PROBLEM=MyTranslateProblem\n",
    "#PROBLEM=translate_ende_wmt8k\n",
    "DATA_DIR=./t2t_data\n",
    "TMP_DIR=/tmp/t2t_datagen\n",
    "rm -rf $DATA_DIR $TMP_DIR\n",
    "mkdir -p $DATA_DIR $TMP_DIR\n",
    "# Generate data\n",
    "t2t-datagen \\\n",
    "  --data_dir=$DATA_DIR \\\n",
    "  --tmp_dir=$TMP_DIR \\\n",
    "  --problem=$PROBLEM \\\n",
    "  --t2t_usr_dir=./ende"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
