{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing Cloud Machine Learning Engine (CMLE)\n",
    "**Learning Objectives:**\n",
    "  - Learn how to make code compatible with CMLE\n",
    "  - Train your model using cloud infrastructure via CMLE\n",
    "  - Deploy your model behind a production grade REST API using CMLE\n",
    "\n",
    "In this notebook we'll make the jump from training and predicting locally, to do doing both in the cloud. We'll take advantage of Google Cloud's [Cloud Machine Learning Engine](https://cloud.google.com/ml-engine/). \n",
    "\n",
    "CMLE is a managed service that allows the training and deployment of ML models without having to provision or maintain servers. The infrastructure is handles seamless by the managed service for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Make Code Compatible with CMLE\n",
    "In order to make our code compatible with CMLE we need to make the following changes:\n",
    "\n",
    "1. Upload data to Google Cloud Storage \n",
    "2. Move code into a Python package\n",
    "3. Modify code to read data from and write checkpoint files to GCS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Upload Data to Google Cloud Storage (GCS)\n",
    "\n",
    "Cloud services don't have access to our local files, so we need to upload them to a location the Cloud servers can read from. In this case we'll use GCS.\n",
    "\n",
    "Specify your project name and bucket name in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT='cloud-training-demos' # CHANGE TO YOUR PROJECT\n",
    "BUCKET = PROJECT # optionally change, a GCS bucket with this name will be created \n",
    "REGION = 'us-central1' # optionally change, see https://cloud.google.com/ml-engine/docs/tensorflow/regions\n",
    "TFVERSION = '1.12' # TF version for CMLE to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter allows the subsitution of python variables into bash commands when using the `!<cmd>` format.\n",
    "It is also possible using the `%%bash` magic but requires an [additional parameter](https://stackoverflow.com/questions/19579546/can-i-access-python-variables-within-a-bash-or-script-ipython-notebook-c). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud config set project {PROJECT}\n",
    "!gsutil mb -l {REGION} gs://{BUCKET}\n",
    "!gsutil -m cp *.csv gs://{BUCKET}/taxifare/smallinput/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Move code into a Python package\n",
    "\n",
    "When you execute a CMLE training job, the service zips up your code and ships it to the Cloud so it can be run on Cloud infrastructure. In order to do this CMLE requires your code to be a Python package.\n",
    "\n",
    "A Python package is simply a collection of one or more `.py` files along with an `__init__.py` file to identify the containing directory as a package. The `__init__.py` sometimes contains initialization code but for our purposes an empty file suffices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Create Package Directory and \\_\\_init\\_\\_.py\n",
    "\n",
    "The bash command `touch` creates an empty file in the specified location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘taxifaremodel’: File exists\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "mkdir taxifaremodel\n",
    "touch taxifaremodel/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Paste existing code into model.py\n",
    "\n",
    "A Python package requires our code to be in a `.py` file, as opposed to notebook cells. So we simply copy and paste our existing code for the previous notebook into a single file.\n",
    "\n",
    "the `%%writefile` magic writes the contents of its cell to disk with the specified name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting taxifaremodel/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile taxifaremodel/model.py\n",
    "import tensorflow as tf\n",
    "import shutil\n",
    "print(tf.__version__)\n",
    "\n",
    "#1. Train and Evaluate Input Functions\n",
    "CSV_COLUMN_NAMES = ['fare_amount','dayofweek','hourofday','pickuplon','pickuplat','dropofflon','dropofflat','passengers']\n",
    "CSV_DEFAULTS = [[0.0],[1],[0],[-74.0], [40.0], [-74.0], [40.7], [1]]\n",
    "\n",
    "def read_dataset(csv_path):\n",
    "    def _parse_row(row):\n",
    "        # Decode the CSV row into list of TF tensors\n",
    "        fields = tf.decode_csv(row, record_defaults=CSV_DEFAULTS)\n",
    "\n",
    "        # Pack the result into a dictionary\n",
    "        features = dict(zip(CSV_COLUMN_NAMES, fields))\n",
    "        \n",
    "        # Separate the label from the features\n",
    "        label = features.pop('fare_amount') # remove label from features and store\n",
    "\n",
    "        return features, label\n",
    "    \n",
    "    # Create a dataset containing the text lines.\n",
    "    dataset = tf.data.Dataset.list_files(csv_path) # (i.e. data_file_*.csv)\n",
    "    dataset = dataset.flat_map(lambda filename:tf.data.TextLineDataset(filename).skip(1))\n",
    "\n",
    "    # Parse each CSV row into correct (features,label) format for Estimator API\n",
    "    dataset = dataset.map(_parse_row)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def train_input_fn(csv_path, batch_size=128):\n",
    "    #1. Convert CSV into tf.data.Dataset  with (features,label) format\n",
    "    dataset = read_dataset(csv_path)\n",
    "      \n",
    "    #2. Shuffle, repeat, and batch the examples.\n",
    "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "   \n",
    "    return dataset\n",
    "\n",
    "def eval_input_fn(csv_path, batch_size=128):\n",
    "    #1. Convert CSV into tf.data.Dataset  with (features,label) format\n",
    "    dataset = read_dataset(csv_path)\n",
    "\n",
    "    #2.Batch the examples.\n",
    "    dataset = dataset.batch(batch_size)\n",
    "   \n",
    "    return dataset\n",
    "  \n",
    "#2. Feature Columns\n",
    "FEATURE_NAMES = CSV_COLUMN_NAMES[1:] # all but first column\n",
    "feature_cols = [tf.feature_column.numeric_column(k) for k in FEATURE_NAMES]\n",
    "\n",
    "#3. Serving Input Receiver Function\n",
    "def serving_input_receiver_fn():\n",
    "    receiver_tensors = {\n",
    "        'dayofweek' : tf.placeholder(tf.int32, shape=[None]), # shape is vector to allow batch of requests\n",
    "        'hourofday' : tf.placeholder(tf.int32, shape=[None]),\n",
    "        'pickuplon' : tf.placeholder(tf.float32, shape=[None]), \n",
    "        'pickuplat' : tf.placeholder(tf.float32, shape=[None]),\n",
    "        'dropofflat' : tf.placeholder(tf.float32, shape=[None]),\n",
    "        'dropofflon' : tf.placeholder(tf.float32, shape=[None]),\n",
    "        'passengers' : tf.placeholder(tf.int32, shape=[None]),\n",
    "    }\n",
    "    # Note:\n",
    "    # You would transform data here from the reiever format to the format expected\n",
    "    # by your model, although in this case no transformation is needed.\n",
    "    \n",
    "    features = receiver_tensors # 'features' is what is passed on to the model\n",
    "    return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)\n",
    "  \n",
    "#4. Train and Evaluate\n",
    "def train_and_evaluate(params):\n",
    "  OUTDIR = params['output_dir']\n",
    "\n",
    "  model = tf.estimator.DNNRegressor(\n",
    "    hidden_units = [10,10], # specify neural architecture\n",
    "    feature_columns = feature_cols, \n",
    "    model_dir = OUTDIR,\n",
    "    config = tf.estimator.RunConfig(\n",
    "          tf_random_seed=1, # for reproducibility\n",
    "          save_checkpoints_steps=max(10,params['train_steps']//10) # checkpoint every N steps\n",
    "    ) \n",
    "  )\n",
    "  \n",
    "  # Add custom evaluation metric\n",
    "  def my_rmse(labels, predictions):\n",
    "    pred_values = tf.squeeze(predictions['predictions'],axis=-1)\n",
    "    return {'rmse': tf.metrics.root_mean_squared_error(labels, pred_values)}\n",
    "  model = tf.contrib.estimator.add_metrics(model, my_rmse)  \n",
    "\n",
    "  train_spec=tf.estimator.TrainSpec(\n",
    "                     input_fn = lambda:train_input_fn(params['train_data_path']),\n",
    "                     max_steps = params['train_steps'])\n",
    "\n",
    "  exporter = tf.estimator.FinalExporter('exporter', serving_input_receiver_fn) # export SavedModel once at the end of training\n",
    "  # Note: alternatively use tf.estimator.BestExporter to export at every checkpoint that has lower loss than the previous checkpoint\n",
    "\n",
    "\n",
    "  eval_spec=tf.estimator.EvalSpec(\n",
    "                     input_fn=lambda:eval_input_fn(params['eval_data_path']),\n",
    "                     steps = None,\n",
    "                     start_delay_secs=1, # wait at least N seconds before first evaluation (default 120)\n",
    "                     throttle_secs=1, # wait at least N seconds before each subsequent evaluation (default 600)\n",
    "                     exporters = exporter) # export SavedModel once at the end of training\n",
    "\n",
    "  # Note:\n",
    "  # We set start_delay_secs and throttle_secs to 1 because we want to evaluate after every checkpoint.\n",
    "  # As long as checkpoints are > 1 sec apart this ensures the throttling never kicks in.\n",
    "\n",
    "  tf.logging.set_verbosity(tf.logging.INFO) # so loss is printed during training\n",
    "  shutil.rmtree(OUTDIR, ignore_errors = True) # start fresh each time\n",
    "\n",
    "  tf.estimator.train_and_evaluate(model, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Modify code to read data from and write checkpoint files to GCS \n",
    "\n",
    "If you look closely above, you'll notice two changes to the code\n",
    "\n",
    "1. The input function now supports reading a list of files matching a file name pattern instead of just a single CSV\n",
    "  - This is useful because large datasets tend to exist in shards.\n",
    "2. The train and evaluate portion is wrapped in a function that takes a parameter dictionary as an argument.\n",
    "  - This is useful because the output directory, data paths and number of train steps will be different depending on whether we're training locally or in the cloud. Parametrizing allows us to use the same code for both.\n",
    "\n",
    "We specify these parameters at run time via the command line. Which means we need to add code to parse command line parameters and invoke `train_and_evaluate()` with those params. This is the job of the `task.py` file. \n",
    "\n",
    "Exposing parameters to the command line also allows us to use CMLE's automatic hyperparameter tuning feature which we'll cover in a future lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting taxifaremodel/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile taxifaremodel/task.py\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "#import model # for python2\n",
    "from . import model # for python3\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--train_data_path',\n",
    "        help = 'GCS or local path to training data',\n",
    "        required = True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--train_steps',\n",
    "        help = 'Steps to run the training job for (default: 1000)',\n",
    "        type = int,\n",
    "        default = 1000\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--eval_data_path',\n",
    "        help = 'GCS or local path to evaluation data',\n",
    "        required = True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--output_dir',\n",
    "        help = 'GCS location to write checkpoints and export models',\n",
    "        required = True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--job-dir',\n",
    "        help='This is not used by our model, but it is required by gcloud',\n",
    "    )\n",
    "    args = parser.parse_args().__dict__\n",
    "\n",
    "    # Run the training job\n",
    "    model.train_and_evaluate(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Train Using CMLE (Local)\n",
    "\n",
    "CMLE comes with a local test tool ([`gcloud ml-engine local train`](https://cloud.google.com/sdk/gcloud/reference/ml-engine/local/train)) to ensure we've packaged our code directly. It's best to first run that for a few steps before trying a Cloud job. \n",
    "\n",
    "The arguments before `-- \\` are for CMLE\n",
    "- package-path: speficies the location of the Python package\n",
    "- module-name: specifies which `.py` file should be run within the package. `task.py` is our entry point so we specify that\n",
    "\n",
    "The arguments after `-- \\` are sent to our `task.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py3env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "1.12.0\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 10 or save_checkpoints_secs None.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "2019-01-06 16:24:05.525939: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into taxi_trained/model.ckpt.\n",
      "INFO:tensorflow:loss = 51218.37, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 5 into taxi_trained/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-06-16:24:06\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from taxi_trained/model.ckpt-5\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-06-16:24:13\n",
      "INFO:tensorflow:Saving dict for global step 5: average_loss = 87.178925, global_step = 5, label/mean = 11.2297125, loss = 11140.697, prediction/mean = 10.322646, rmse = 9.336966\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5: taxi_trained/model.ckpt-5\n",
      "INFO:tensorflow:Performing the final export in the end of training.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures EXCLUDED from export because they cannot be be served via TensorFlow Serving APIs:\n",
      "INFO:tensorflow:'serving_default' : Regression input must be a single string Tensor; got {'passengers': <tf.Tensor 'Placeholder_6:0' shape=(?,) dtype=int32>, 'dayofweek': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=int32>, 'hourofday': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=int32>, 'pickuplat': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=float32>, 'dropofflat': <tf.Tensor 'Placeholder_4:0' shape=(?,) dtype=float32>, 'pickuplon': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=float32>, 'dropofflon': <tf.Tensor 'Placeholder_5:0' shape=(?,) dtype=float32>}\n",
      "INFO:tensorflow:'regression' : Regression input must be a single string Tensor; got {'passengers': <tf.Tensor 'Placeholder_6:0' shape=(?,) dtype=int32>, 'dayofweek': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=int32>, 'hourofday': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=int32>, 'pickuplat': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=float32>, 'dropofflat': <tf.Tensor 'Placeholder_4:0' shape=(?,) dtype=float32>, 'pickuplon': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=float32>, 'dropofflon': <tf.Tensor 'Placeholder_5:0' shape=(?,) dtype=float32>}\n",
      "WARNING:tensorflow:Export includes no default signature!\n",
      "INFO:tensorflow:Restoring parameters from taxi_trained/model.ckpt-5\n",
      "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py:1044: calling SavedModelBuilder.add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Pass your op to the equivalent parameter main_op instead.\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: taxi_trained/export/exporter/temp-b'1546791853'/saved_model.pb\n",
      "INFO:tensorflow:Loss for final step: 6522.596.\n",
      "CPU times: user 360 ms, sys: 170 ms, total: 530 ms\n",
      "Wall time: 18.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!gcloud ml-engine local train \\\n",
    "--package-path=taxifaremodel \\\n",
    "--module-name=taxifaremodel.task \\\n",
    "-- \\\n",
    "--train_data_path=taxi-train.csv \\\n",
    "--eval_data_path=taxi-valid.csv  \\\n",
    "--train_steps=5 \\\n",
    "--output_dir=taxi_trained "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Train using CMLE (Cloud)\n",
    "\n",
    "To submit to the Cloud we use [`gcloud ml-engine jobs submit training [jobname]`](https://cloud.google.com/sdk/gcloud/reference/ml-engine/jobs/submit/training) and simply specify some additional parameters for CMLE:\n",
    "- jobname: A unique identifier for the Cloud job. We usually append system time to ensure uniqueness\n",
    "- job-dir: A GCS location to upload the Python package to\n",
    "- runtime-version: Version of TF to use. Defaults to 1.0 if not specified\n",
    "- python-version: Version of Python to use. Defaults to 2.7 if not specified\n",
    "- region: Cloud region to train in. See [here](https://cloud.google.com/ml-engine/docs/tensorflow/regions) for supported CMLE regions\n",
    "\n",
    "Below the `-- \\` note how we've changed our `task.py` args to be GCS locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.datalab.ml import TensorBoard\n",
    "OUTDIR='gs://{}/taxifare/trained_small'.format(BUCKET)\n",
    "TensorBoard().start(OUTDIR) # TensorBoard supports gs:// URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil -m rm -rf {OUTDIR} # start fresh each time\n",
    "!gcloud ml-engine jobs submit training taxifare_$(date -u +%y%m%d_%H%M%S) \\\n",
    "   --package-path=taxifaremodel \\\n",
    "   --module-name=taxifaremodel.task \\\n",
    "   --job-dir=gs://{BUCKET}/taxifare \\\n",
    "   --python-version=3.5 \\\n",
    "   --runtime-version={TFVERSION} \\\n",
    "   --region={REGION} \\\n",
    "   -- \\\n",
    "   --train_data_path=gs://{BUCKET}/taxifare/smallinput/taxi-train.csv \\\n",
    "   --eval_data_path=gs://{BUCKET}/taxifare/smallinput/taxi-valid.csv  \\\n",
    "   --train_steps=1000 \\\n",
    "   --output_dir={OUTDIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can track your job and view logs using [cloud console](https://console.cloud.google.com/mlengine/jobs). It will take 5-10 minutes to complete. **Wait till the job finishes before moving on.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Deploy model\n",
    "\n",
    "Now let's take our exported SavedModel and deploy it behind a REST API. To do so we'll use CMLE's managed TF Serving feature which auto-scales based on load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://vijays-sandbox/taxifare/trained_small/export/exporter/\r\n",
      "gs://vijays-sandbox/taxifare/trained_small/export/exporter/1546792089/\r\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://{BUCKET}/taxifare/trained_small/export/exporter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CMLE uses a model versioning system. First you create a model folder, and within the folder you create versions of the model. \n",
    "\n",
    "Note: You will see an error below if the model folder already exists, it is safe to ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION='v1'\n",
    "!gcloud ml-engine models create taxifare --regions us-central1\n",
    "!gcloud ml-engine versions delete {VERSION} --model taxifare --quiet\n",
    "!gcloud ml-engine versions create {VERSION} --model taxifare \\\n",
    "  --origin $(gsutil ls gs://{BUCKET}/taxifare/trained_small/export/exporter | tail -1) \\\n",
    "  --python-version=3.5 \\\n",
    "  --runtime-version {TFVERSION}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Online Prediction\n",
    "\n",
    "Now that we have deployed our model behind a production grade REST API, we can invoke it remotely. \n",
    "\n",
    "We could invoke it directly calling the REST API with an HTTP POST request [reference docs](https://cloud.google.com/ml-engine/reference/rest/v1/projects/predict), however CMLE provides an easy way to invoke it via command line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Invoke Prediction REST API via Command Line\n",
    "First we write our prediction requests to file in json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./test.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./test.json\n",
    "{\"dayofweek\":1,\"hourofday\":0,\"pickuplon\": -73.885262,\"pickuplat\": 40.773008,\"dropofflon\": -73.987232,\"dropofflat\": 40.732403,\"passengers\": 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use [`gcloud ml-engine predict`](https://cloud.google.com/sdk/gcloud/reference/ml-engine/predict) and specify the model name and location of the json file. Since we don't explicitly specify `--version`, the default model version will be used. \n",
    "\n",
    "Since we only have one version it is already the default, but if we had multiple model versions we can designate the default using [`gcloud ml-engine versions set-default`](https://cloud.google.com/sdk/gcloud/reference/ml-engine/versions/set-default) or using [cloud console](https://pantheon.corp.google.com/mlengine/models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTIONS\r\n",
      "[8.362519264221191]\r\n"
     ]
    }
   ],
   "source": [
    "!gcloud ml-engine predict --model=taxifare --json-instances=./test.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Invoke Prediction REST API via Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response={'predictions': [{'predictions': [8.362519264221191]}]}\n"
     ]
    }
   ],
   "source": [
    "from googleapiclient import discovery\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import json\n",
    "\n",
    "credentials = GoogleCredentials.get_application_default()\n",
    "api = discovery.build('ml', 'v1', credentials=credentials,\n",
    "            discoveryServiceUrl='https://storage.googleapis.com/cloud-ml/discovery/ml_v1_discovery.json')\n",
    "\n",
    "request_data = {'instances':\n",
    "  [\n",
    "      {\n",
    "        'dayofweek':1,\n",
    "        'hourofday':0,\n",
    "        'pickuplon': -73.885262,\n",
    "        'pickuplat': 40.773008,\n",
    "        'dropofflon': -73.987232,\n",
    "        'dropofflat': 40.732403,\n",
    "        'passengers': 2,\n",
    "      }\n",
    "  ]\n",
    "}\n",
    "\n",
    "parent = 'projects/{}/models/taxifare'.format(PROJECT) # use default version\n",
    "#parent = 'projects/{}/models/taxifare/versions/{}'.format(PROJECT,VERSION) # specify version\n",
    "\n",
    "response = api.projects().predict(body=request_data, name=parent).execute()\n",
    "print(\"response={0}\".format(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(TensorBoard.list())>0:\n",
    "  [TensorBoard().stop(pid)for pid in TensorBoard.list()['pid']]\n",
    "else: print('No TensorBoard instances to stop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Exercise\n",
    "\n",
    "Modify your solution to the challenge exercise in d_trainandevaluate.ipynb appropriately. Make sure that you implement training and deployment. Increase the size of your dataset by 10x since you are running on the cloud. Does your accuracy improve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2016 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
