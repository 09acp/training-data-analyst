{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End ML Pipelines with Cloud Composer\n",
    "\n",
    "In this advanced lab you will learn how to create and run an [Apache Airflow](http://airflow.apache.org/) workflow in Cloud Composer that completes the following tasks:\n",
    "- Watches for new CSV data to be uploaded to a [Cloud Storage](https://cloud.google.com/storage/docs/) bucket\n",
    "- A [Cloud Function](https://cloud.google.com/composer/docs/how-to/using/triggering-with-gcf#getting_the_client_id) call triggers the [Cloud Composer Airflow DAG](https://cloud.google.com/composer/docs/how-to/using/writing-dags) to run when a new file is detected \n",
    "- The workflow finds the input file that triggered the workflow and executes a [Cloud Dataflow](https://cloud.google.com/dataflow/) job to transform and output the data to BigQuery  \n",
    "- Moves the original input file to a different Cloud Storage bucket for storing processed files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Cloud Composer environment\n",
    "First, create a Cloud Composer environment by doing the following:\n",
    "1. In the Navigation menu under Big Data, select **Composer**\n",
    "2. Select **Create**\n",
    "3. Set the following parameters:\n",
    "    - Name: composer-workflow\n",
    "    - Location: us-central1\n",
    "    - Other values at defaults\n",
    "4. Select **Create**\n",
    "\n",
    "The environment creation process is completed when the green checkmark displays to the left of the environment name on the Environments page in the GCP Console.\n",
    "It can take up to 20 minutes for the environment to complete the setup process. Move on to the next section - Create Cloud Storage buckets and BigQuery dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Cloud Storage buckets\n",
    "Create two Cloud Storage Multi-Regional buckets in your project. \n",
    "- project-id_input\n",
    "- project-id_output\n",
    "\n",
    "Run the below to automatically create the buckets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change these to try this notebook out\n",
    "BUCKET = 'cloud-training-demos-ml'\n",
    "PROJECT = 'cloud-training-demos'\n",
    "REGION = 'us-central1'\n",
    "\n",
    "INPUT = PROJECT + '_input'\n",
    "OUTPUT = PROJECT + '_output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO export using os.environ\n",
    "import os\n",
    "os.environ['BUCKET'] = BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check if the folder exists\n",
    "%%bash\n",
    "if ! gsutil ls | grep -q gs://${INPUT}; then\n",
    "  gsutil mb -l ${REGION} gs://${INPUT}\n",
    "fi\n",
    "\n",
    "if ! gsutil ls | grep -q g.... output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create BigQuery Destination Dataset and Table\n",
    "Next, we'll create a data sink to store the ingested data from GCS<br><br>\n",
    "\n",
    "### Create a new Dataset\n",
    "1. In the Navigation menu, select **BigQuery**\n",
    "2. Then click on your qwiklabs project ID\n",
    "3. Click **Create Dataset**\n",
    "4. Name your dataset **ml_pipeline** and leave other values at defaults\n",
    "5. Click **Create Dataset**\n",
    "\n",
    "\n",
    "### Create a new empty table\n",
    "1. Click on the newly created dataset\n",
    "2. Click **Create Table**\n",
    "3. For Destination Table name specify **ingest_table**\n",
    "4. For schema click **Edit as Text** and paste in the below schema\n",
    "\n",
    "    state:\tSTRING,<br>\n",
    "    gender:\tSTRING,<br>\n",
    "    year:\tSTRING,<br>\n",
    "    name:\tSTRING,<br>\n",
    "    number:\tSTRING,<br>\n",
    "    created_date:\tSTRING,<br>\n",
    "    filename:\tSTRING,<br>\n",
    "    load_dt:\tDATE<br><br>\n",
    "\n",
    "5. Click **Create Table**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Review of Airflow concepts\n",
    "While your Cloud Composer environment is building, let’s discuss the sample file you’ll be using in this lab.\n",
    "<br><br>\n",
    "[Airflow](https://airflow.apache.org/) is a platform to programmatically author, schedule and monitor workflows\n",
    "<br><br>\n",
    "Use airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The airflow scheduler executes your tasks on an array of workers while following the specified dependencies.\n",
    "<br><br>\n",
    "### Core concepts\n",
    "- [DAG](https://airflow.apache.org/concepts.html#dags) - A Directed Acyclic Graph  is a collection of tasks, organised to reflect their relationships and dependencies.\n",
    "- [Operator](https://airflow.apache.org/concepts.html#operators) - The description of a single task, it is usually atomic. For example, the BashOperator is used to execute bash command.\n",
    "- [Task](https://airflow.apache.org/concepts.html#tasks) - A parameterised instance of an Operator;  a node in the DAG.\n",
    "- [Task Instance](https://airflow.apache.org/concepts.html#task-instances) - A specific run of a task; characterised as: a DAG, a Task, and a point in time. It has an indicative state: *running, success, failed, skipped, …*<br><br>\n",
    "The rest of the Airflow concepts can be found [here](https://airflow.apache.org/concepts.html#).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete the code in the Cloud Composer workflow\n",
    "Cloud Composer workflows are comprised of [DAGs (Directed Acyclic Graphs)](https://airflow.incubator.apache.org/concepts.html#dags). The code shown in simple_load_dag.py is the workflow code, also referred to as the DAG. \n",
    "<br><br>\n",
    "Open the file now to see how it is built. Next will be a detailed look at some of the key components of the file.\n",
    "<br><br>\n",
    "To orchestrate all the workflow tasks, the DAG imports the following operators:\n",
    "- DataFlowPythonOperator\n",
    "- PythonOperator\n",
    "<br><br>\n",
    "Action: <span style=\"color:red\">**Complete the # TODOs in the [simple_load_dag.py](simple_load_dag.py)**</span> file while you wait for your Composer environment to be setup. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing environment information\n",
    "Now that you have a completed DAG, it's time to upload it to your Cloud Composer environment and finish the setup of your workflow.<br><br>\n",
    "1. Go back to **Composer** to check on the status of your environment.\n",
    "2. Once your environment has been created, click the **name of the environment** to see its details.\n",
    "<br><br>\n",
    "The Environment details page provides information, such as the Airflow web UI URL, Google Kubernetes Engine cluster ID, name of the Cloud Storage bucket connected to the DAGs folder.\n",
    "<br><br>\n",
    "Cloud Composer uses Cloud Storage to store Apache Airflow DAGs, also known as workflows. Each environment has an associated Cloud Storage bucket. Cloud Composer schedules only the DAGs in the Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Airflow variables\n",
    "Our DAG relies on variables to pass in values like the GCP Project. We can set these in the Admin UI.\n",
    "\n",
    "Airflow variables are an Airflow-specific concept that is distinct from [environment variables](https://cloud.google.com/composer/docs/how-to/managing/environment-variables). In this step, you'll set the following six [Airflow variables](https://airflow.apache.org/concepts.html#variables) used by the DAG we will deploy.\n",
    "\n",
    "\n",
    "**Key**|**Value**|**Example**\n",
    ":-----:|:-----:|:-----:\n",
    "gcp\\_project|your-gcp-project-id|qwiklabs-gcp-123123\n",
    "gcp\\_input\\_location|gcs-bucket-for-dataflow-input-files|gs://qwiklabs-gcp-123123_input\n",
    "gcp\\_temp\\_location|gcs-bucket-for-dataflow-temp-files|gs://qwiklabs-gcp-123123_output/tmp\n",
    "gcs\\_completion\\_bucket|output-gcs-bucket|gs://qwiklabs-gcp-123123_output\n",
    "input\\_field\\_names|comma-separated-field-names-for-delimited-file|state,gender,year,name,number,created\\_date\n",
    "bq\\_output\\_table|bigquery-output-table|ml\\_pipeline.ingest\\_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Set the variables using the Airflow webserver UI\n",
    "1. In your Airflow environment, select **Admin** > **Variables**\n",
    "2. Populate each key value in the table with the required variables from the above table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Set the variables using the Airflow CLI\n",
    "The next gcloud composer command executes the Airflow CLI sub-command [variables](https://airflow.apache.org/cli.html#variables). The sub-command passes the arguments to the gcloud command line tool.<br><br>\n",
    "To set the three variables, run the gcloud composer command once for each row from the above table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "gcloud composer environments run ENVIRONMENT_NAME \\\n",
    " --location LOCATION variables -- \\\n",
    " --set KEY VALUE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading the DAG and dependencies to Cloud Storage\n",
    "1. After you have completed the # TODOs in the simple_load_dag.py, **upload the file** to the DAGs folder in your Airflow environent inside of the /dags/ sub-folder\n",
    "2. Next, in the same /dags/ folder create a subfolder titled **dataflow**\n",
    "3. Upload process_delimited.py from your repository into /dags/dataflow/ \n",
    "<br><br>\n",
    "Cloud Composer registers the DAG in your Airflow environment automatically, DAG changes occur within 3-5 minutes. You can see task status in the Airflow web interface and confirm the DAG is not scheduled as per the settings. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Navigating Using the Airflow UI\n",
    "To access the Airflow web interface using the GCP Console:\n",
    "1. Go back to the **Composer Environments** page.\n",
    "2. In the **Airflow webserver** column for the environment, click the new window icon. \n",
    "3. The Airflow web UI opens in a new browser window. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigger DAG run manually\n",
    "Running your DAG manually ensures that it operates successfully even in the absence of triggered events. \n",
    "1. Trigger the DAG manually **click the play button** under Links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Trigger DAG run automatically from a file upload to GCS\n",
    "Now that your manual workflow runs successfully, you will now trigger it based on an external event. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Cloud Function to trigger your workflow\n",
    "We will be following this [reference guide](https://cloud.google.com/composer/docs/how-to/using/triggering-with-gcf) to setup our Cloud Function\n",
    "1. In the below code block, replace your-project-id with your project id\n",
    "2. Run the code to grant blob **permissions** to your service account "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bindings:\n",
      "- members:\n",
      "  - serviceAccount:qwiklabs-gcp-77c0e3b62eaf4101@appspot.gserviceaccount.com\n",
      "  role: roles/iam.serviceAccountTokenCreator\n",
      "etag: BwV5aLqo9Ck=\n",
      "bindings:\n",
      "- members:\n",
      "  - serviceAccount:qwiklabs-gcp-77c0e3b62eaf4101@appspot.gserviceaccount.com\n",
      "  role: roles/iam.serviceAccountTokenCreator\n",
      "etag: BwV5aLqo9Ck=\n",
      "bindings:\n",
      "- members:\n",
      "  - serviceAccount:qwiklabs-gcp-77c0e3b62eaf4101@appspot.gserviceaccount.com\n",
      "  role: roles/iam.serviceAccountTokenCreator\n",
      "etag: BwV5aLqo9Ck=\n",
      "bindings:\n",
      "- members:\n",
      "  - serviceAccount:qwiklabs-gcp-77c0e3b62eaf4101@appspot.gserviceaccount.com\n",
      "  role: roles/iam.serviceAccountTokenCreator\n",
      "etag: BwV5aLqo9Ck=\n",
      "bindings:\n",
      "- members:\n",
      "  - serviceAccount:qwiklabs-gcp-77c0e3b62eaf4101@appspot.gserviceaccount.com\n",
      "  role: roles/iam.serviceAccountTokenCreator\n",
      "etag: BwV5aLqo9Ck=\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gcloud iam service-accounts add-iam-policy-binding \\\n",
    "qwiklabs-gcp-77c0e3b62eaf4101@appspot.gserviceaccount.com \\\n",
    "--member=serviceAccount:qwiklabs-gcp-77c0e3b62eaf4101@appspot.gserviceaccount.com \\\n",
    "--role=roles/iam.serviceAccountTokenCreator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. In the code block below, uncomment the project_id, location, and composer_environment and populate them\n",
    "3. Run the below code to get your **CLIENT_ID** (needed later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5741407462-6inpo3s2jsfuat4vanffe8ho6tlfmj5q.apps.googleusercontent.com\n",
      "5741407462-6inpo3s2jsfuat4vanffe8ho6tlfmj5q.apps.googleusercontent.com\n",
      "5741407462-6inpo3s2jsfuat4vanffe8ho6tlfmj5q.apps.googleusercontent.com\n",
      "5741407462-6inpo3s2jsfuat4vanffe8ho6tlfmj5q.apps.googleusercontent.com\n",
      "5741407462-6inpo3s2jsfuat4vanffe8ho6tlfmj5q.apps.googleusercontent.com\n"
     ]
    }
   ],
   "source": [
    "import google.auth\n",
    "import google.auth.transport.requests\n",
    "import requests\n",
    "import six.moves.urllib.parse\n",
    "\n",
    "# Authenticate with Google Cloud.\n",
    "# See: https://cloud.google.com/docs/authentication/getting-started\n",
    "credentials, _ = google.auth.default(\n",
    "    scopes=['https://www.googleapis.com/auth/cloud-platform'])\n",
    "authed_session = google.auth.transport.requests.AuthorizedSession(\n",
    "    credentials)\n",
    "\n",
    "project_id = 'qwiklabs-gcp-123123'\n",
    "location = 'us-central1'\n",
    "composer_environment = 'composer'\n",
    "\n",
    "environment_url = (\n",
    "    'https://composer.googleapis.com/v1beta1/projects/{}/locations/{}'\n",
    "    '/environments/{}').format(project_id, location, composer_environment)\n",
    "composer_response = authed_session.request('GET', environment_url)\n",
    "environment_data = composer_response.json()\n",
    "airflow_uri = environment_data['config']['airflowUri']\n",
    "\n",
    "# The Composer environment response does not include the IAP client ID.\n",
    "# Make a second, unauthenticated HTTP request to the web server to get the\n",
    "# redirect URI.\n",
    "redirect_response = requests.get(airflow_uri, allow_redirects=False)\n",
    "redirect_location = redirect_response.headers['location']\n",
    "\n",
    "# Extract the client_id query parameter from the redirect.\n",
    "parsed = six.moves.urllib.parse.urlparse(redirect_location)\n",
    "query_string = six.moves.urllib.parse.parse_qs(parsed.query)\n",
    "print(query_string['client_id'][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Cloud Function\n",
    "\n",
    "1. Navigate to Compute > **Cloud Functions**\n",
    "2. Select **Create function**\n",
    "3. For name specify **'gcs-dag-trigger-function'**\n",
    "4. For trigger type select **'Cloud Storage'**\n",
    "5. For event type select '**Finalize/Create'**\n",
    "6. For bucket, **specify the input bucket** you created earlier \n",
    "\n",
    "Important: be sure to select the input bucket and not the output bucket to avoid an endless triggering loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### populate index.js\n",
    "Complete the four required constants defined below in index.js code and **paste it into the Cloud Function editor** (the js code will not run in this notebook). The constants are: \n",
    "- PROJECT_ID\n",
    "- CLIENT_ID (from earlier)\n",
    "- WEBSERVER_ID (part of Airflow webserver URL) \n",
    "- DAG_NAME (GcsToBigQueryTriggered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'use strict';\n",
    "\n",
    "const fetch = require('node-fetch');\n",
    "const FormData = require('form-data');\n",
    "\n",
    "/**\n",
    " * Triggered from a message on a Cloud Storage bucket.\n",
    " *\n",
    " * IAP authorization based on:\n",
    " * https://stackoverflow.com/questions/45787676/how-to-authenticate-google-cloud-functions-for-access-to-secure-app-engine-endpo\n",
    " * and\n",
    " * https://cloud.google.com/iap/docs/authentication-howto\n",
    " *\n",
    " * @param {!Object} event The Cloud Functions event.\n",
    " * @param {!Function} callback The callback function.\n",
    " */\n",
    "exports.triggerDag = function triggerDag (event, callback) {\n",
    "  // Fill in your Composer environment information here.\n",
    "\n",
    "  // The project that holds your function\n",
    "  const PROJECT_ID = 'qwiklabs-gcp-123123'; \n",
    "  // example: qwiklabs-gcp-97d55fb651b04b20\n",
    "\n",
    "  // Navigate to your webserver's login page and get this from the URL\n",
    "  const CLIENT_ID = '';\n",
    "  // example: 954510698485-gde6id87qtdn9itl7809uj8s6a60n9gl\n",
    "\n",
    "  // This should be part of your webserver's URL:\n",
    "  // {tenant-project-id}.appspot.com\n",
    "  const WEBSERVER_ID = '';\n",
    "  // example: b93193d731fd74d3f-tp\n",
    "\n",
    "  // The name of the DAG you wish to trigger\n",
    "  const DAG_NAME = 'GcsToBigQueryTriggered';\n",
    "  // example: GcsToBigQueryTriggered\n",
    "\n",
    "  ///////////////////////\n",
    "  // DO NOT EDIT BELOW //\n",
    "\n",
    "  // Other constants\n",
    "  const WEBSERVER_URL = `https://${WEBSERVER_ID}.appspot.com/api/experimental/dags/${DAG_NAME}/dag_runs`;\n",
    "  const USER_AGENT = 'gcf-event-trigger';\n",
    "  const BODY = {'conf': JSON.stringify(event.data)};\n",
    "\n",
    "  // Make the request\n",
    "  authorizeIap(CLIENT_ID, PROJECT_ID, USER_AGENT)\n",
    "    .then(function iapAuthorizationCallback (iap) {\n",
    "      makeIapPostRequest(WEBSERVER_URL, BODY, iap.idToken, USER_AGENT, iap.jwt);\n",
    "    })\n",
    "    .then(_ => callback(null))\n",
    "    .catch(callback);\n",
    "};\n",
    "\n",
    "/**\n",
    "   * @param {string} clientId The client id associated with the Composer webserver application.\n",
    "   * @param {string} projectId The id for the project containing the Cloud Function.\n",
    "   * @param {string} userAgent The user agent string which will be provided with the webserver request.\n",
    "   */\n",
    "function authorizeIap (clientId, projectId, userAgent) {\n",
    "  const SERVICE_ACCOUNT = `${projectId}@appspot.gserviceaccount.com`;\n",
    "  const JWT_HEADER = Buffer.from(JSON.stringify({alg: 'RS256', typ: 'JWT'}))\n",
    "    .toString('base64');\n",
    "\n",
    "  var jwt = '';\n",
    "  var jwtClaimset = '';\n",
    "\n",
    "  // Obtain an Oauth2 access token for the appspot service account\n",
    "  return fetch(\n",
    "    `http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/${SERVICE_ACCOUNT}/token`,\n",
    "    {\n",
    "      headers: {'User-Agent': userAgent, 'Metadata-Flavor': 'Google'}\n",
    "    })\n",
    "    .then(res => res.json())\n",
    "    .then(function obtainAccessTokenCallback (tokenResponse) {\n",
    "      if (tokenResponse.error) {\n",
    "        return Promise.reject(tokenResponse.error);\n",
    "      }\n",
    "      var accessToken = tokenResponse.access_token;\n",
    "      var iat = Math.floor(new Date().getTime() / 1000);\n",
    "      var claims = {\n",
    "        iss: SERVICE_ACCOUNT,\n",
    "        aud: 'https://www.googleapis.com/oauth2/v4/token',\n",
    "        iat: iat,\n",
    "        exp: iat + 60,\n",
    "        target_audience: clientId\n",
    "      };\n",
    "      jwtClaimset = Buffer.from(JSON.stringify(claims)).toString('base64');\n",
    "      var toSign = [JWT_HEADER, jwtClaimset].join('.');\n",
    "\n",
    "      return fetch(\n",
    "        `https://iam.googleapis.com/v1/projects/${projectId}/serviceAccounts/${SERVICE_ACCOUNT}:signBlob`,\n",
    "        {\n",
    "          method: 'POST',\n",
    "          body: JSON.stringify({'bytesToSign': Buffer.from(toSign).toString('base64')}),\n",
    "          headers: {\n",
    "            'User-Agent': userAgent,\n",
    "            'Authorization': `Bearer ${accessToken}`\n",
    "          }\n",
    "        });\n",
    "    })\n",
    "    .then(res => res.json())\n",
    "    .then(function signJsonClaimCallback (body) {\n",
    "      if (body.error) {\n",
    "        return Promise.reject(body.error);\n",
    "      }\n",
    "      // Request service account signature on header and claimset\n",
    "      var jwtSignature = body.signature;\n",
    "      jwt = [JWT_HEADER, jwtClaimset, jwtSignature].join('.');\n",
    "      var form = new FormData();\n",
    "      form.append('grant_type', 'urn:ietf:params:oauth:grant-type:jwt-bearer');\n",
    "      form.append('assertion', jwt);\n",
    "      return fetch(\n",
    "        'https://www.googleapis.com/oauth2/v4/token', {\n",
    "          method: 'POST',\n",
    "          body: form\n",
    "        });\n",
    "    })\n",
    "    .then(res => res.json())\n",
    "    .then(function returnJwt (body) {\n",
    "      if (body.error) {\n",
    "        return Promise.reject(body.error);\n",
    "      }\n",
    "      return {\n",
    "        jwt: jwt,\n",
    "        idToken: body.id_token\n",
    "      };\n",
    "    });\n",
    "}\n",
    "\n",
    "/**\n",
    "   * @param {string} url The url that the post request targets.\n",
    "   * @param {string} body The body of the post request.\n",
    "   * @param {string} idToken Bearer token used to authorize the iap request.\n",
    "   * @param {string} userAgent The user agent to identify the requester.\n",
    "   * @param {string} jwt A Json web token used to authenticate the request.\n",
    "   */\n",
    "function makeIapPostRequest (url, body, idToken, userAgent, jwt) {\n",
    "  var form = new FormData();\n",
    "  form.append('grant_type', 'urn:ietf:params:oauth:grant-type:jwt-bearer');\n",
    "  form.append('assertion', jwt);\n",
    "\n",
    "  return fetch(\n",
    "    url, {\n",
    "      method: 'POST',\n",
    "      body: form\n",
    "    })\n",
    "    .then(function makeIapPostRequestCallback () {\n",
    "      return fetch(url, {\n",
    "        method: 'POST',\n",
    "        headers: {\n",
    "          'User-Agent': userAgent,\n",
    "          'Authorization': `Bearer ${idToken}`\n",
    "        },\n",
    "        body: JSON.stringify(body)\n",
    "      });\n",
    "    });\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### populate package.json\n",
    "Copy and paste the below into **package.json**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"name\": \"nodejs-docs-samples-functions-composer-storage-trigger\",\n",
    "  \"version\": \"0.0.1\",\n",
    "  \"dependencies\": {\n",
    "    \"form-data\": \"^2.3.2\",\n",
    "    \"node-fetch\": \"^2.2.0\"\n",
    "  },\n",
    "  \"engines\": {\n",
    "    \"node\": \">=4.3.2\"\n",
    "  },\n",
    "  \"private\": true,\n",
    "  \"license\": \"Apache-2.0\",\n",
    "  \"author\": \"Google Inc.\",\n",
    "  \"repository\": {\n",
    "    \"type\": \"git\",\n",
    "    \"url\": \"https://github.com/GoogleCloudPlatform/nodejs-docs-samples.git\"\n",
    "  },\n",
    "  \"devDependencies\": {\n",
    "    \"@google-cloud/nodejs-repo-tools\": \"^2.2.5\",\n",
    "    \"ava\": \"0.25.0\",\n",
    "    \"proxyquire\": \"2.0.0\",\n",
    "    \"semistandard\": \"^12.0.1\",\n",
    "    \"sinon\": \"4.4.2\"\n",
    "  },\n",
    "  \"scripts\": {\n",
    "    \"lint\": \"repo-tools lint\",\n",
    "    \"test\": \"ava -T 20s --verbose test/*.test.js\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. For **Function to execute**, specify **triggerDag** (note: case sensitive)\n",
    "11. Select **Create**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload CSVs and Monitor\n",
    "1. Practice uploading and editing CSVs into your input bucket (note: the DAG filters to only ingest CSVs with 'usa_names.csv' as the filepath. Adjust this as needed in the DAG code.)\n",
    "2. Troubleshoot Cloud Function call errors by monitoring the [logs](https://console.cloud.google.com/logs/viewer?)\n",
    "3. Troubleshoot Airflow workflow errors by monitoring the **Browse** > **DAG Runs** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! \n",
    "You’ve have completed this advanced lab on triggering a workflow with a Cloud Function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
