{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Create TensorFlow Wide and Deep Model\n",
    "\n",
    "**Learning Objective**\n",
    "- Create a Wide and Deep model using the high-level Estimator API \n",
    "- Determine which features to use as wide columns and which to use as deep columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "We'll begin by modeling our data using a Deep Neural Network. To achieve this we will use the high-level Estimator API in Tensorflow. Have a look at the various models available through the Estimator API in [the documentation here](https://www.tensorflow.org/api_docs/python/tf/estimator). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "PROJECT = 'munn-sandbox'  # Replace with your PROJECT\n",
    "BUCKET = 'munn-bucket'  # Replace with your BUCKET\n",
    "REGION = 'us-central1'            # Choose an available region for Cloud MLE \n",
    "TFVERSION = '1.12'                # TF version for CMLE to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "if ! gsutil ls | grep -q gs://${BUCKET}/; then\n",
    "  gsutil mb -l ${REGION} gs://${BUCKET}\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval.csv\n",
      "test.csv\n",
      "train.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls *.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Create TensorFlow model using TensorFlow's Estimator API ##\n",
    "\n",
    "First, we'll write an input_fn to read the data and define the csv column names, label and key columns. We'll also set the default csv column values and set the number of training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "CSV_COLUMNS = 'weight_pounds,is_male,mother_age,plurality,gestation_weeks,key'.split(',')\n",
    "LABEL_COLUMN = 'weight_pounds'\n",
    "KEY_COLUMN = 'key'\n",
    "\n",
    "# Set default values for each CSV column\n",
    "DEFAULTS = [[0.0], ['null'], [0.0], ['null'], [0.0], ['nokey']]\n",
    "TRAIN_STEPS = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the input function\n",
    "\n",
    "Now we are ready to create an input function using the Dataset API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def read_dataset(filename, mode, batch_size = 512):\n",
    "  def _input_fn():\n",
    "    def decode_csv(value_column):\n",
    "      columns = tf.decode_csv(value_column, record_defaults=DEFAULTS)\n",
    "      features = dict(zip(CSV_COLUMNS, columns))\n",
    "      label = features.pop(LABEL_COLUMN)\n",
    "      return features, label\n",
    "    \n",
    "    # Create list of files that match pattern\n",
    "    file_list = tf.gfile.Glob(filename)\n",
    "\n",
    "    # Create dataset from file list\n",
    "    dataset = (tf.data.TextLineDataset(file_list)  # Read text file\n",
    "                 .map(decode_csv))  # Transform each elem by applying decode_csv fn\n",
    "      \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        num_epochs = None # indefinitely\n",
    "        dataset = dataset.shuffle(buffer_size=10*batch_size)\n",
    "    else:\n",
    "        num_epochs = 1 # end-of-input after this\n",
    " \n",
    "    dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "    return dataset\n",
    "  return _input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create the feature columns\n",
    "\n",
    "Next, define the feature columns. For a wide and deep model, we need to determine which features we will use as wide features and which to pass as deep features. The function `get_wide_deep` below will return a tuple containing the wide feature columns and deep feature columns. Have a look at [this blog post on wide and deep models](https://ai.googleblog.com/2016/06/wide-deep-learning-better-together-with.html) to remind yourself how best to describe the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_wide_deep():\n",
    "  # Define column types\n",
    "  is_male,mother_age,plurality,gestation_weeks = \\\n",
    "      [tf.feature_column.categorical_column_with_vocabulary_list('is_male', \n",
    "                      ['True', 'False', 'Unknown']),\n",
    "       tf.feature_column.numeric_column('mother_age'),\n",
    "       tf.feature_column.categorical_column_with_vocabulary_list('plurality',\n",
    "                      ['Single(1)', 'Twins(2)', 'Triplets(3)',\n",
    "                       'Quadruplets(4)', 'Quintuplets(5)','Multiple(2+)']),\n",
    "       tf.feature_column.numeric_column('gestation_weeks')\n",
    "      ]\n",
    "\n",
    "  # Discretize\n",
    "  age_buckets = tf.feature_column.bucketized_column(mother_age, \n",
    "                      boundaries=np.arange(15,45,1).tolist())\n",
    "  gestation_buckets = tf.feature_column.bucketized_column(gestation_weeks, \n",
    "                      boundaries=np.arange(17,47,1).tolist())\n",
    "\n",
    "  # Sparse columns are wide, have a linear relationship with the output\n",
    "  wide = [is_male,\n",
    "          plurality,\n",
    "          age_buckets,\n",
    "          gestation_buckets]\n",
    "\n",
    "  # Feature cross all the wide columns and embed into a lower dimension\n",
    "  crossed = tf.feature_column.crossed_column(wide, hash_bucket_size=20000)\n",
    "  embed = tf.feature_column.embedding_column(crossed, 3)\n",
    "\n",
    "  # Continuous columns are deep, have a complex relationship with the output\n",
    "  deep = [mother_age,\n",
    "          gestation_weeks,\n",
    "          embed]\n",
    "  return wide, deep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create the serving input function \n",
    "\n",
    "To predict with the TensorFlow model, we also need a serving input function. This will allow us to serve prediction later using the predetermined inputs. We will want all the inputs from our user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def serving_input_fn():\n",
    "    feature_placeholders = {\n",
    "        'is_male': tf.placeholder(tf.string, [None]),\n",
    "        'mother_age': tf.placeholder(tf.float32, [None]),\n",
    "        'plurality': tf.placeholder(tf.string, [None]),\n",
    "        'gestation_weeks': tf.placeholder(tf.float32, [None])\n",
    "    }\n",
    "    features = {\n",
    "        key: tf.expand_dims(tensor, -1)\n",
    "        for key, tensor in feature_placeholders.items()\n",
    "    }\n",
    "    return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model and train\n",
    "\n",
    "Lastly, we'll create the estimator to train and evaluate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(output_dir):\n",
    "  wide, deep = get_wide_deep()\n",
    "  EVAL_INTERVAL = 300\n",
    "  run_config = tf.estimator.RunConfig(save_checkpoints_secs = EVAL_INTERVAL,\n",
    "                                      keep_checkpoint_max = 3)\n",
    "  estimator = tf.estimator.DNNLinearCombinedRegressor(\n",
    "                       model_dir = output_dir,\n",
    "                       linear_feature_columns = wide,\n",
    "                       dnn_feature_columns = deep,\n",
    "                       dnn_hidden_units = [64, 32],\n",
    "                       config = run_config)\n",
    "  train_spec = tf.estimator.TrainSpec(\n",
    "                       input_fn = read_dataset('train.csv', mode = tf.estimator.ModeKeys.TRAIN),\n",
    "                       max_steps = TRAIN_STEPS)\n",
    "  exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n",
    "  eval_spec = tf.estimator.EvalSpec(\n",
    "                       input_fn = read_dataset('eval.csv', mode = tf.estimator.ModeKeys.EVAL),\n",
    "                       steps = None,\n",
    "                       start_delay_secs = 60, # start evaluating after N seconds\n",
    "                       throttle_secs = EVAL_INTERVAL,  # evaluate every N seconds\n",
    "                       exporters = exporter)\n",
    "  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally, train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_keep_checkpoint_max': 3, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_task_type': 'worker', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff6e0e7e588>, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_task_id': 0, '_save_checkpoints_steps': None, '_num_worker_replicas': 1, '_service': None, '_protocol': None, '_save_checkpoints_secs': 300, '_device_fn': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_train_distribute': None, '_experimental_distribute': None, '_save_summary_steps': 100, '_log_step_count_steps': 100, '_is_chief': True, '_global_id_in_cluster': 0, '_eval_distribute': None, '_model_dir': 'babyweight_trained'}\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 300.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into babyweight_trained/model.ckpt.\n",
      "INFO:tensorflow:loss = 37661.207, step = 1\n",
      "INFO:tensorflow:global_step/sec: 37.2685\n",
      "INFO:tensorflow:loss = 728.735, step = 101 (2.685 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.3784\n",
      "INFO:tensorflow:loss = 632.0045, step = 201 (2.416 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.1577\n",
      "INFO:tensorflow:loss = 620.6789, step = 301 (2.167 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.1005\n",
      "INFO:tensorflow:loss = 664.57574, step = 401 (2.433 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.1592\n",
      "INFO:tensorflow:loss = 702.76996, step = 501 (2.317 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.9224\n",
      "INFO:tensorflow:loss = 601.475, step = 601 (2.131 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.2571\n",
      "INFO:tensorflow:loss = 605.88947, step = 701 (2.484 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.1333\n",
      "INFO:tensorflow:loss = 640.5563, step = 801 (2.318 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.2666\n",
      "INFO:tensorflow:loss = 546.76184, step = 901 (2.161 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into babyweight_trained/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-02-26-03:10:39\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from babyweight_trained/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-02-26-03:10:40\n",
      "INFO:tensorflow:Saving dict for global step 1000: average_loss = 1.1640317, global_step = 1000, label/mean = 7.2687864, loss = 592.32587, prediction/mean = 7.279896\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1000: babyweight_trained/model.ckpt-1000\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\n",
      "INFO:tensorflow:Signatures EXCLUDED from export because they cannot be be served via TensorFlow Serving APIs:\n",
      "INFO:tensorflow:'serving_default' : Regression input must be a single string Tensor; got {'mother_age': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=float32>, 'gestation_weeks': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=float32>, 'is_male': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=string>, 'plurality': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=string>}\n",
      "INFO:tensorflow:'regression' : Regression input must be a single string Tensor; got {'mother_age': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=float32>, 'gestation_weeks': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=float32>, 'is_male': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=string>, 'plurality': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=string>}\n",
      "WARNING:tensorflow:Export includes no default signature!\n",
      "INFO:tensorflow:Restoring parameters from babyweight_trained/model.ckpt-1000\n",
      "WARNING:tensorflow:From /home/jupyter/.local/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py:1044: calling SavedModelBuilder.add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Pass your op to the equivalent parameter main_op instead.\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: babyweight_trained/export/exporter/temp-b'1551150640'/saved_model.pb\n",
      "INFO:tensorflow:Loss for final step: 595.16437.\n"
     ]
    }
   ],
   "source": [
    "# Run the model\n",
    "shutil.rmtree('babyweight_trained', ignore_errors = True) # start fresh each time\n",
    "train_and_evaluate('babyweight_trained')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "When I ran it, the final RMSE (the average_loss) is about 1.164. The exporter directory contains the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Copyright 2017-2018 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
