{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing tf.estimator.train_and_evaluate()\n",
    "**Learning Objectives**\n",
    "- Introduce new type of input function (`serving_input_reciever_fn()`) which supports remote access to our model via REST API\n",
    "- Use the `tf.estimator.train_and_evaluate()` method to periodically evaluate *during* training\n",
    "- Practice using TensorBoard to visualize training and evaluation loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py3env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import shutil\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Train and Evaluate Input Functions\n",
    "Same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_COLUMN_NAMES = ['fare_amount','dayofweek','hourofday','pickuplon','pickuplat','dropofflon','dropofflat','passengers']\n",
    "CSV_DEFAULTS = [[0.0],[1],[0],[-74.0], [40.0], [-74.0], [40.7], [1]]\n",
    "\n",
    "def read_dataset(csv_path):\n",
    "    def _parse_row(row):\n",
    "        # Decode the CSV row into list of TF tensors\n",
    "        fields = tf.decode_csv(row, record_defaults=CSV_DEFAULTS)\n",
    "\n",
    "        # Pack the result into a dictionary\n",
    "        features = dict(zip(CSV_COLUMN_NAMES, fields))\n",
    "        \n",
    "        # Separate the label from the features\n",
    "        label = features.pop('fare_amount') # remove label from features and store\n",
    "\n",
    "        return features, label\n",
    "    \n",
    "    # Create a dataset containing the text lines.\n",
    "    dataset = tf.data.TextLineDataset(csv_path).skip(1) # skip header\n",
    "\n",
    "    # Parse each CSV row into correct (features,label) format for Estimator API\n",
    "    dataset = dataset.map(_parse_row)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def train_input_fn(csv_path, batch_size=128):\n",
    "    #1. Convert CSV into tf.data.Dataset  with (features,label) format\n",
    "    dataset = read_dataset(csv_path)\n",
    "      \n",
    "    #2. Shuffle, repeat, and batch the examples.\n",
    "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "   \n",
    "    return dataset\n",
    "\n",
    "def eval_input_fn(csv_path, batch_size=128):\n",
    "    #1. Convert CSV into tf.data.Dataset  with (features,label) format\n",
    "    dataset = read_dataset(csv_path)\n",
    "\n",
    "    #2.Batch the examples.\n",
    "    dataset = dataset.batch(batch_size)\n",
    "   \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Feature Columns\n",
    "\n",
    "Same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[_NumericColumn(key='dayofweek', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='hourofday', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='pickuplon', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='pickuplat', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='dropofflon', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='dropofflat', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " _NumericColumn(key='passengers', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FEATURE_NAMES = CSV_COLUMN_NAMES[1:] # all but first column\n",
    "\n",
    "feature_cols = [tf.feature_column.numeric_column(k) for k in FEATURE_NAMES]\n",
    "feature_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Serving Input Receiver Function \n",
    "\n",
    "In a prior notebook we used the `estimator.predict()` function to get taxifare predictions. This worked fine because we had done our model training on the same machine. \n",
    "\n",
    "However in a production setting this won't usually be the case. Our clients may be remote web servers, mobile apps and more. Instead of having to ship our model files to every client, it would be better to host our model in one place, and make it remotely accesible for prediction requests using a REST API.\n",
    "\n",
    "The TensorFlow solution for this is a project called [TF Serving](https://www.tensorflow.org/serving/), which is part of the larger [Tensorflow Extended (TFX)](https://www.tensorflow.org/tfx/) platform that extends TensorFlow for production environments. \n",
    "\n",
    "The interface between TensorFlow and TF Serving is a `serving_input_receiver_fn()`. It has two jobs:\n",
    "- To add `tf.placeholder`s to the graph to specify what type of tensors TF Serving should recieve during inference requests.  The placeholders are specified as a dictionary object\n",
    "- To add any additional ops needed to convert data from the client into the tensors expected by the model.\n",
    "\n",
    "The function must return a `tf.estimator.export.ServingInputReceiver` object, which packages the placeholders and the neccesary transformations together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_receiver_fn():\n",
    "    receiver_tensors = {\n",
    "        'dayofweek' : tf.placeholder(tf.int32, shape=[None]), # shape is vector to allow batch of requests\n",
    "        'hourofday' : tf.placeholder(tf.int32, shape=[None]),\n",
    "        'pickuplon' : tf.placeholder(tf.float32, shape=[None]), \n",
    "        'pickuplat' : tf.placeholder(tf.float32, shape=[None]),\n",
    "        'dropofflat' : tf.placeholder(tf.float32, shape=[None]),\n",
    "        'dropofflon' : tf.placeholder(tf.float32, shape=[None]),\n",
    "        'passengers' : tf.placeholder(tf.int32, shape=[None]),\n",
    "    }\n",
    "    # Note:\n",
    "    # You would transform data here from the receiver format to the format expected\n",
    "    # by your model, although in this case no transformation is needed.\n",
    "    \n",
    "    features = receiver_tensors # 'features' is what is passed on to the model\n",
    "    return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Monitoring with TensorBoard \n",
    "\n",
    "[TensorBoard](https://www.tensorflow.org/guide/summaries_and_tensorboard) is a web UI that allows us to visualize various aspects of our model, including the training and evaluation loss curves. Although you won't see the loss curves yet, it is best to launch TensorBoard *before* you start training so that you may see them update during a long running training process.\n",
    "\n",
    "*Warning:* There is an issue with DataLab that causes TensorBoard to only work correctly the first time it's launched per session. So if you have issues try resetting the kernel and launching TensorBoard again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>TensorBoard was started successfully with pid 61404. Click <a href=\"/_proxy/44539/\" target=\"_blank\">here</a> to access it.</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "61404"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.datalab.ml import TensorBoard\n",
    "TensorBoard().start('taxi_trained')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Train and Evaluate\n",
    "\n",
    "One issue with the previous notebooks is we only evaluate on our validation data once training is complete. This means we can't tell at what point overfitting began. What we really want is to evaluate at specified intervals *during* the training phase.\n",
    "\n",
    "The Estimator API way of doing this is to replace `estimator.train()` and `estimator.evaluate()` with `estimator.train_and_evaluate()`. This causes an evaluation to be done after every training checkpoint. However by default Tensorflow only checkpoints once every  10 minutes. Since this is less than the length of our total training we'd end up with the same behavior as before which is just one evaluation at the end of training. \n",
    "\n",
    "To remedy this we speciy in the `tf.estimator.RunConfig()` that TensorFlow should checkpoint every 100 steps.\n",
    "\n",
    "The default evaluation metric `average_loss` is MSE, but we want RMSE. Previously we just took the square root of the final `average_loss`. However it would be better if we could calculate RMSE not just at the end, but for every intermediate checkpoint and plot the change over time in TensorBoard. [`tf.contrib.estimator.add_metrics()`](https://www.tensorflow.org/api_docs/python/tf/contrib/estimator/add_metrics) allows us to do this. We wrap our estimator with it, and provide a custom evaluation function.\n",
    "\n",
    "`train_and_evaluate()` also allows us to use our `serving_input_receiver_fn()` to export our models in the SavedModel format required by TF Serving.\n",
    "\n",
    "*Note: Training will be slower than the last notebook because we are now evaluating after every 100 train steps. Previously we didn't evaluate at all during training, only after.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_log_step_count_steps': 100, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f92c8627ef0>, '_save_checkpoints_steps': 100, '_task_id': 0, '_global_id_in_cluster': 0, '_model_dir': 'taxi_trained', '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_service': None, '_keep_checkpoint_max': 5, '_num_worker_replicas': 1, '_eval_distribute': None, '_protocol': None, '_is_chief': True, '_num_ps_replicas': 0, '_device_fn': None, '_evaluation_master': '', '_train_distribute': None, '_experimental_distribute': None, '_save_checkpoints_secs': None, '_task_type': 'worker', '_keep_checkpoint_every_n_hours': 10000, '_save_summary_steps': 100, '_master': '', '_tf_random_seed': 1}\n",
      "INFO:tensorflow:Using config: {'_log_step_count_steps': 100, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f92c48042b0>, '_save_checkpoints_steps': 100, '_task_id': 0, '_global_id_in_cluster': 0, '_model_dir': 'taxi_trained', '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_service': None, '_keep_checkpoint_max': 5, '_num_worker_replicas': 1, '_eval_distribute': None, '_protocol': None, '_is_chief': True, '_num_ps_replicas': 0, '_device_fn': None, '_evaluation_master': '', '_train_distribute': None, '_experimental_distribute': None, '_save_checkpoints_secs': None, '_task_type': 'worker', '_keep_checkpoint_every_n_hours': 10000, '_save_summary_steps': 100, '_master': '', '_tf_random_seed': 1}\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 100 or save_checkpoints_secs None.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into taxi_trained/model.ckpt.\n",
      "INFO:tensorflow:loss = 11701.152, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 100 into taxi_trained/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-06-16:18:29\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from taxi_trained/model.ckpt-100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-06-16:18:36\n",
      "INFO:tensorflow:Saving dict for global step 100: average_loss = 88.41773, global_step = 100, label/mean = 11.2297125, loss = 11299.005, prediction/mean = 12.828847, rmse = 9.40307\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 100: taxi_trained/model.ckpt-100\n",
      "INFO:tensorflow:global_step/sec: 8.8225\n",
      "INFO:tensorflow:loss = 5478.3735, step = 101 (11.337 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 200 into taxi_trained/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-06-16:18:40\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from taxi_trained/model.ckpt-200\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-06-16:18:47\n",
      "INFO:tensorflow:Saving dict for global step 200: average_loss = 85.5533, global_step = 200, label/mean = 11.2297125, loss = 10932.956, prediction/mean = 11.073815, rmse = 9.249502\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 200: taxi_trained/model.ckpt-200\n",
      "INFO:tensorflow:global_step/sec: 9.27324\n",
      "INFO:tensorflow:loss = 6598.8813, step = 201 (10.783 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 300 into taxi_trained/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-06-16:18:51\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from taxi_trained/model.ckpt-300\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-06-16:18:59\n",
      "INFO:tensorflow:Saving dict for global step 300: average_loss = 85.572624, global_step = 300, label/mean = 11.2297125, loss = 10935.426, prediction/mean = 11.099365, rmse = 9.250547\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 300: taxi_trained/model.ckpt-300\n",
      "INFO:tensorflow:global_step/sec: 8.78361\n",
      "INFO:tensorflow:loss = 10363.135, step = 301 (11.386 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 400 into taxi_trained/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-06-16:19:03\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from taxi_trained/model.ckpt-400\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-06-16:19:10\n",
      "INFO:tensorflow:Saving dict for global step 400: average_loss = 86.1619, global_step = 400, label/mean = 11.2297125, loss = 11010.7295, prediction/mean = 11.966151, rmse = 9.282344\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 400: taxi_trained/model.ckpt-400\n",
      "INFO:tensorflow:global_step/sec: 8.7356\n",
      "INFO:tensorflow:loss = 13453.012, step = 401 (11.446 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 500 into taxi_trained/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-06-16:19:14\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from taxi_trained/model.ckpt-500\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-06-16:19:21\n",
      "INFO:tensorflow:Saving dict for global step 500: average_loss = 85.744705, global_step = 500, label/mean = 11.2297125, loss = 10957.416, prediction/mean = 10.764917, rmse = 9.259844\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 500: taxi_trained/model.ckpt-500\n",
      "INFO:tensorflow:Performing the final export in the end of training.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures EXCLUDED from export because they cannot be be served via TensorFlow Serving APIs:\n",
      "INFO:tensorflow:'serving_default' : Regression input must be a single string Tensor; got {'hourofday': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=int32>, 'passengers': <tf.Tensor 'Placeholder_6:0' shape=(?,) dtype=int32>, 'dropofflat': <tf.Tensor 'Placeholder_4:0' shape=(?,) dtype=float32>, 'dropofflon': <tf.Tensor 'Placeholder_5:0' shape=(?,) dtype=float32>, 'pickuplon': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=float32>, 'dayofweek': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=int32>, 'pickuplat': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=float32>}\n",
      "INFO:tensorflow:'regression' : Regression input must be a single string Tensor; got {'hourofday': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=int32>, 'passengers': <tf.Tensor 'Placeholder_6:0' shape=(?,) dtype=int32>, 'dropofflat': <tf.Tensor 'Placeholder_4:0' shape=(?,) dtype=float32>, 'dropofflon': <tf.Tensor 'Placeholder_5:0' shape=(?,) dtype=float32>, 'pickuplon': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=float32>, 'dayofweek': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=int32>, 'pickuplat': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=float32>}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Export includes no default signature!\n",
      "INFO:tensorflow:Restoring parameters from taxi_trained/model.ckpt-500\n",
      "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py:1044: calling SavedModelBuilder.add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Pass your op to the equivalent parameter main_op instead.\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: taxi_trained/export/exporter/temp-b'1546791561'/saved_model.pb\n",
      "INFO:tensorflow:Loss for final step: 8451.508.\n",
      "CPU times: user 49.8 s, sys: 25.1 s, total: 1min 14s\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "OUTDIR = 'taxi_trained'\n",
    "shutil.rmtree(OUTDIR, ignore_errors = True) # start fresh each time\n",
    "\n",
    "model = tf.estimator.DNNRegressor(\n",
    "    hidden_units = [10,10], # specify neural architecture\n",
    "    feature_columns = feature_cols, \n",
    "    model_dir = OUTDIR,\n",
    "    config = tf.estimator.RunConfig(\n",
    "        tf_random_seed=1, # for reproducibility\n",
    "        save_checkpoints_steps=100 # checkpoint every 100 steps\n",
    "    ) \n",
    ")\n",
    "\n",
    "# Add custom evaluation metric\n",
    "def my_rmse(labels, predictions):\n",
    "  pred_values = tf.squeeze(predictions['predictions'],axis=-1)\n",
    "  return {'rmse': tf.metrics.root_mean_squared_error(labels, pred_values)}\n",
    "model = tf.contrib.estimator.add_metrics(model, my_rmse)  \n",
    "\n",
    "train_spec=tf.estimator.TrainSpec(\n",
    "                   input_fn = lambda:train_input_fn('./taxi-train.csv'),\n",
    "                   max_steps = 500)\n",
    "\n",
    "exporter = tf.estimator.FinalExporter('exporter', serving_input_receiver_fn) # export SavedModel once at the end of training\n",
    "# Note: alternatively use tf.estimator.BestExporter to export at every checkpoint that has lower loss than the previous checkpoint\n",
    "\n",
    "eval_spec=tf.estimator.EvalSpec(\n",
    "                   input_fn=lambda:eval_input_fn('./taxi-valid.csv'),\n",
    "                   steps = None,\n",
    "                   start_delay_secs=1, # wait at least N seconds before first evaluation (default 120)\n",
    "                   throttle_secs=1, # wait at least N seconds before each subsequent evaluation (default 600)\n",
    "                   exporters = exporter) # export SavedModel once at the end of training\n",
    "\n",
    "# Note:\n",
    "# We set start_delay_secs and throttle_secs to 1 because we want to evaluate after every checkpoint.\n",
    "# As long as checkpoints are > 1 sec apart this ensures the throttling never kicks in.\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO) # so loss is printed during training\n",
    "shutil.rmtree(OUTDIR, ignore_errors = True) # start fresh each time\n",
    "tf.estimator.train_and_evaluate(model, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Inspect Export Folder\n",
    "\n",
    "Now in the output directory, in addition to the checkpoint files, you'll see a subfolder called 'export'. This contains one or models in the SavedModel format which is compatible with TF Serving. In the next notebook we will deploy the SavedModel behind a production grade REST API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taxi_trained/export:\r\n",
      "exporter\r\n",
      "\r\n",
      "taxi_trained/export/exporter:\r\n",
      "1546791561\r\n",
      "\r\n",
      "taxi_trained/export/exporter/1546791561:\r\n",
      "saved_model.pb\tvariables\r\n",
      "\r\n",
      "taxi_trained/export/exporter/1546791561/variables:\r\n",
      "variables.data-00000-of-00002  variables.data-00001-of-00002  variables.index\r\n"
     ]
    }
   ],
   "source": [
    "!ls -R taxi_trained/export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(TensorBoard.list())>0:\n",
    "  [TensorBoard().stop(pid)for pid in TensorBoard.list()['pid']]\n",
    "else: print('No TensorBoard instances to stop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Exercise\n",
    "\n",
    "Modify your solution to the challenge exercise in c_dataset.ipynb appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2019 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
