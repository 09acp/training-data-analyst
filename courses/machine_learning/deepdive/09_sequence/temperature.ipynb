{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time-series prediction (temperature from weather stations)\n",
    "\n",
    "This notebook illustrates:\n",
    "\n",
    "* Predicting the \"next\" value of a long time-series\n",
    "* Using a LSTM model on numeric data\n",
    "* Serving a LSTM model\n",
    "\n",
    "<b>Note:</b>\n",
    "See [(Time series prediction with RNNs and TensorFlow)](../05_artandscience/d_customestimator.ipynb) for a very similar example, except that it works with multiple short sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# change these to try this notebook out\n",
    "BUCKET = 'cloud-training-demos-ml'\n",
    "PROJECT = 'cloud-training-demos'\n",
    "REGION = 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%datalab project set -p $PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration and cleanup\n",
    "\n",
    "The data are temperature data from US weather stations. This is a public dataset from NOAA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import google.datalab.bigquery as bq\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query=\"\"\"\n",
    "SELECT\n",
    "  stationid, date,\n",
    "  MAX(tmin) AS tmin,\n",
    "  MAX(tmax) AS tmax,\n",
    "  IF (MOD(ABS(FARM_FINGERPRINT(stationid)), 10) < 7, True, False) AS is_train\n",
    "FROM (\n",
    "  SELECT\n",
    "    wx.id as stationid,\n",
    "    wx.date as date,\n",
    "    CONCAT(wx.id, \" \", CAST(wx.date AS STRING)) AS recordid,\n",
    "    IF (wx.element = 'TMIN', wx.value/10, NULL) AS tmin,\n",
    "    IF (wx.element = 'TMAX', wx.value/10, NULL) AS tmax\n",
    "  FROM\n",
    "    `bigquery-public-data.ghcn_d.ghcnd_2016` AS wx\n",
    "  WHERE STARTS_WITH(id, 'USW000')\n",
    ")\n",
    "GROUP BY\n",
    "  stationid, date\n",
    "ORDER BY\n",
    "  stationid, date\n",
    "\"\"\"\n",
    "df = bq.Query(query).execute().result().to_dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, there are missing observations on some days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to fix this is to do a pivot table and then replace the nulls by filling it forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cleanup_nulls(df, variablename):\n",
    "  df2 = df.pivot_table(variablename, 'date', 'stationid', fill_value=np.nan)\n",
    "  print('Before: {} null values'.format(df2.isnull().sum().sum()))\n",
    "  df2.fillna(method='ffill', inplace=True)\n",
    "  df2.fillna(method='bfill', inplace=True)\n",
    "  df2.dropna(axis=1, inplace=True)\n",
    "  print('After: {} null values'.format(df2.isnull().sum().sum()))\n",
    "  return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "traindf = cleanup_nulls(df[df['is_train']], 'tmin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "traindf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq = traindf.iloc[:,0]\n",
    "print('{} values in the sequence'.format(len(seq)))\n",
    "ax = sns.tsplot(seq)\n",
    "ax.set(xlabel='day-number', ylabel='temperature');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq.to_string(index=False).replace('\\n', ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save the data to disk in such a way that each time series is on a single line\n",
    "def to_csv(indf, filename):\n",
    "  df = cleanup_nulls(indf, 'tmin')\n",
    "  print('Writing {} sequences to {}'.format(len(df.columns), filename))\n",
    "  with open(filename, 'w') as ofp:\n",
    "    for i in xrange(0, len(df.columns)):\n",
    "      if i%10 == 0:\n",
    "        print('{}'.format(i), end='...')\n",
    "      seq = traindf.iloc[:,i]\n",
    "      line = seq.to_string(index=False, header=False).replace('\\n', ',')\n",
    "      ofp.write(line + '\\n')\n",
    "    print('Done')\n",
    "to_csv(df[df['is_train']], 'train.csv')\n",
    "to_csv(df[~df['is_train']], 'eval.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "ls -l *.csv\n",
    "head -1 eval.csv | tr ',' ' ' | wc\n",
    "wc *.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our CSV file sequences consist of 366 numbers. Each number is one input and the prediction output is the next number given previous numbers as history. With 366 numbers (one instance) input, we will have 366 output numbers. For training, each instance's 0~364 numbers are inputs, and 1~365 are truth. For prediction, it is like \"given a series of numbers, predict next n numbers\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use TensorFlow's [Estimator](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/Estimator) to build our model. Estimators help construct the training/evaluation/prediction graph. They reuse the common graph, and fork only when needed (i.e. input_fn). They also handle model export. Models exported can be deployed to Google Cloud ML Engine for online prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import shutil\n",
    "import tensorflow.contrib.learn as tflearn\n",
    "import tensorflow.contrib.layers as tflayers\n",
    "from tensorflow.contrib.learn.python.learn import learn_runner\n",
    "from tensorflow.contrib.learn.python.learn.utils import saved_model_export_utils\n",
    "import tensorflow.contrib.rnn as rnn\n",
    "\n",
    "# tf.decode_csv requires DEFAULTS to infer data types and default values.\n",
    "SEQ_LEN = 366\n",
    "DEFAULTS = [[0.0] for x in xrange(0, SEQ_LEN)]\n",
    "\n",
    "# The Estimator API requires named features.\n",
    "TIMESERIES_FEATURE_NAME = 'rawdata'\n",
    "\n",
    "# Training batch size.\n",
    "BATCH_SIZE = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input\n",
    "\n",
    "Our CSV file structure is quite simple -- a bunch of floating point numbers (note the type of DEFAULTS). We ask for the data to be read BATCH_SIZE sequences at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_input_fn(filename, mode):  \n",
    "  \"\"\"Creates an input_fn for estimator in training or evaluation.\"\"\"\n",
    "  \n",
    "  def _input_fn():\n",
    "    \"\"\"Returns named features and labels, as required by Estimator.\"\"\"    \n",
    "    # could be a path to one file or a file pattern.\n",
    "    input_file_names = tf.train.match_filenames_once(filename)\n",
    "    \n",
    "    filename_queue = tf.train.string_input_producer(\n",
    "        input_file_names, num_epochs=None, shuffle=True)\n",
    "    reader = tf.TextLineReader()\n",
    "    _, value = reader.read_up_to(filename_queue, num_records=BATCH_SIZE)\n",
    "\n",
    "    # parse the csv values\n",
    "    batch_data = tf.decode_csv(value, record_defaults=DEFAULTS)\n",
    "    batch_data = tf.transpose(batch_data) # [BATCH_SIZE, SEQ_LEN]\n",
    "\n",
    "    # Get x and y. They are both of shape [BATCH_SIZE, SEQ_LEN - 1]\n",
    "    batch_len = tf.shape(batch_data)[0]\n",
    "    x = tf.slice(batch_data, [0, 0], [batch_len, SEQ_LEN-1])\n",
    "    y = tf.slice(batch_data, [0, 1], [batch_len, SEQ_LEN-1])\n",
    "    \n",
    "    return {TIMESERIES_FEATURE_NAME: x}, y   # dict of features, target\n",
    "\n",
    "  return _input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Graph\n",
    "\n",
    "Following Estimator's requirements, we will create a model_fn representing the inference model. Note that this function defines the graph that will be used in training, evaluation and prediction.\n",
    "\n",
    "To supply a model function to the Estimator API, you need to return a ModelFnOps. The rest of the function creates the necessary objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  Think of the  of LSTM units as how much history you want the network to remember\n",
    "LSTM_SIZE = [10, 20]\n",
    "DNN_SIZE  = [50, 25]\n",
    "  \n",
    "# scale the temperatures to make the optimization easier; tmin values are -58 to 38, scale it to be 0 to 1\n",
    "def scale_temperature(t):\n",
    "  return (t + 58) / (38+58)\n",
    "\n",
    "def unscale_temperature(sc):\n",
    "  return (sc*(38+58)) - 58\n",
    "\n",
    "def model_fn(features, targets, mode):\n",
    "  \"\"\"Define the inference model.\"\"\"\n",
    "  \n",
    "  # scale the input values to lie between 0-1. this will help optimization\n",
    "  input_seq = scale_temperature(features[TIMESERIES_FEATURE_NAME])\n",
    "  \n",
    "  #lat = features['latitude']\n",
    "\n",
    "  # RNN requires input tensor rank > 2. Adding one dimension.\n",
    "  input_seq = tf.expand_dims(input_seq, axis=-1)\n",
    "  \n",
    "  # LSTM output will be [BATCH_SIZE, SEQ_LEN - 1, lstm_output_size]\n",
    "  lstm_layer = [tf.nn.rnn_cell.LSTMCell(size) for size in LSTM_SIZE]\n",
    "  lstm_cell = tf.nn.rnn_cell.MultiRNNCell(lstm_layer)\n",
    "  lstm_outputs, _ = tf.nn.dynamic_rnn(cell=lstm_cell,\n",
    "                                      inputs=input_seq,\n",
    "                                      dtype=tf.float32)\n",
    "  \n",
    "  # Reshape to [BATCH_SIZE * (SEQ_LEN - 1), lstm_output] so it is 2-D and can\n",
    "  # be fed to next layer.\n",
    "  lstm_outputs = tf.reshape(lstm_outputs, [-1, lstm_cell.output_size])\n",
    "  \n",
    "  #extras = [lstm_outputs, lat, lon]\n",
    "  \n",
    "  # Add hidden layers on top of LSTM layer to add some \"nonlinear\" to the model.\n",
    "  prev_layer = [lstm_outputs]\n",
    "  for h in DNN_SIZE:\n",
    "    hidden1 = tf.contrib.layers.fully_connected(inputs=prev_layer[-1], num_outputs=h)\n",
    "    prev_layer.append(hidden1)\n",
    "    \n",
    "  uniform_initializer = tf.random_uniform_initializer(minval=-0.08, maxval=0.08)\n",
    "  predictions = tf.contrib.layers.fully_connected(inputs=prev_layer[-1],\n",
    "                                                  num_outputs=1,\n",
    "                                                  activation_fn=None,\n",
    "                                                  weights_initializer=uniform_initializer,\n",
    "                                                  biases_initializer=uniform_initializer)\n",
    "\n",
    "  # predictions are all we need when mode is not train/eval.\n",
    "  # but remember to unscale the values\n",
    "  predictions_dict = {\"predicted_temperature\": unscale_temperature(predictions)}\n",
    "\n",
    "  # If train/evaluation, we'll need to compute loss.\n",
    "  # If train, we will also need to create an optimizer.\n",
    "  loss, train_op, eval_metric_ops = None, None, None\n",
    "  if mode == tf.contrib.learn.ModeKeys.TRAIN or mode == tf.contrib.learn.ModeKeys.EVAL:\n",
    "    # scale the temperature so that we match the 0-1 scale of predictions\n",
    "    # it's better to do this rather than unscale the predictions because the\n",
    "    # learning rate and optimizers are all set up for small numbers\n",
    "    targets = scale_temperature(targets)\n",
    "      \n",
    "    # Note: The reshape below is needed because Estimator needs to know\n",
    "    # loss shape. Without reshaping below, loss's shape would be unknown.\n",
    "    targets = tf.reshape(targets, [tf.size(targets)])\n",
    "    predictions = tf.reshape(predictions, [tf.size(predictions)])\n",
    "    loss = tf.losses.mean_squared_error(targets, predictions)\n",
    "    eval_metric_ops = {\n",
    "      \"rmse_scaled\": tf.metrics.root_mean_squared_error(targets, predictions)\n",
    "    }\n",
    "\n",
    "    if mode == tf.contrib.learn.ModeKeys.TRAIN:\n",
    "      train_op = tf.contrib.layers.optimize_loss(\n",
    "          loss=loss,\n",
    "          global_step=tf.contrib.framework.get_global_step(),\n",
    "          learning_rate=0.01,\n",
    "          optimizer=\"Adagrad\")\n",
    "  \n",
    "  # return ModelFnOps as Estimator requires.\n",
    "  return tflearn.ModelFnOps(\n",
    "      mode=mode,\n",
    "      predictions=predictions_dict,\n",
    "      loss=loss,\n",
    "      train_op=train_op,\n",
    "      eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Distributed training is launched off using an Experiment.  The key line here is that we use tflearn.Estimator rather than, say tflearn.DNNRegressor.  This allows us to provide a model_fn, which will be our RNN defined above.  Note also that we specify a serving_input_fn -- this is how we parse the input data provided to us at prediction time using gcloud or Cloud ML Online Prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_train():\n",
    "  return create_input_fn('train.csv', mode=tf.contrib.learn.ModeKeys.TRAIN)\n",
    "\n",
    "\n",
    "def get_eval():\n",
    "  return create_input_fn('eval.csv', mode=tf.contrib.learn.ModeKeys.EVAL)\n",
    "\n",
    "\n",
    "def serving_input_fn():\n",
    "  feature_placeholders = {\n",
    "      TIMESERIES_FEATURE_NAME: tf.placeholder(tf.float32, [None, None])\n",
    "  }\n",
    "  return tflearn.utils.input_fn_utils.InputFnOps(\n",
    "      feature_placeholders,\n",
    "      None,\n",
    "      feature_placeholders\n",
    "  )\n",
    "\n",
    "\n",
    "def experiment_fn(output_dir):\n",
    "    \"\"\"An experiment_fn required for Estimator API to run training.\"\"\"\n",
    "\n",
    "    estimator = tflearn.Estimator(model_fn=model_fn,\n",
    "                                  model_dir=output_dir,\n",
    "                                  config=tf.contrib.learn.RunConfig(save_checkpoints_steps=500))\n",
    "    return tflearn.Experiment(\n",
    "        estimator,\n",
    "        train_input_fn=get_train(),\n",
    "        eval_input_fn=get_eval(),\n",
    "        export_strategies=[saved_model_export_utils.make_export_strategy(\n",
    "            serving_input_fn,\n",
    "            default_output_alternative_key=None,\n",
    "            exports_to_keep=1\n",
    "        )],\n",
    "        train_steps=1000\n",
    "    )\n",
    "\n",
    "\n",
    "shutil.rmtree('training', ignore_errors=True) # start fresh each time.\n",
    "learn_runner.run(experiment_fn, 'training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Summary\n",
    "\n",
    "We can plot model's training summary events using Datalab's ML library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from google.datalab.ml import Summary\n",
    "\n",
    "summary = Summary('./training')\n",
    "summary.plot(['OptimizeLoss/loss', 'loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "\n",
    "Let's pull up a curve and see how we do at predicting the last few values of the series. A week's forecast is reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_one_series(filename, dayno):\n",
    "  with open(filename) as fp:\n",
    "    fields = fp.readline().strip().split(',')\n",
    "    prediction_data = map(float, fields)\n",
    "  \n",
    "    # Upto dayno as input; upto dayno+7 as prediction\n",
    "    prediction_x = list(prediction_data[:dayno])\n",
    "    prediction_y = list(prediction_data[dayno:(dayno+7)])\n",
    "\n",
    "    sns.tsplot(prediction_x, color='blue')\n",
    "    y_truth_curve = [np.nan] * (len(prediction_x)-1) + [prediction_x[-1]] + prediction_y\n",
    "    sns.tsplot(y_truth_curve, color='green')\n",
    "    return prediction_x, prediction_y, y_truth_curve\n",
    "\n",
    "prediction_x, prediction_y, y_truth_curve = get_one_series('eval.csv', 285)\n",
    "print('{} inputs; expecting {} outputs'.format(len(prediction_x), len(prediction_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First prediction we will do is just sending x, and for each value in x it will return a predicted value which is for the very next time step. And then we can compare the predicted values with the truth (x+1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load model.\n",
    "estimator = tflearn.Estimator(model_fn=model_fn, model_dir='training')\n",
    "\n",
    "# Feed Prediction data.\n",
    "predict_input_fn = lambda: {TIMESERIES_FEATURE_NAME: tf.constant([prediction_x])}\n",
    "\n",
    "predicted = list(estimator.predict(input_fn=predict_input_fn))\n",
    "predicted = [p['predicted_temperature'] for p in predicted]\n",
    "\n",
    "# Plot prediction source.\n",
    "sns.tsplot(prediction_x, color='green')\n",
    "\n",
    "# Plot predicted values.\n",
    "sns.tsplot([prediction_x[0]] + predicted, color='red');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, let's send in x, and predict next n values.\n",
    "The way we do this is to invoke the prediction on x, take the prediction, append it to x and make another prediction.\n",
    "Repeat n times and we've created n predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "estimator = tflearn.Estimator(model_fn=model_fn, model_dir='training')\n",
    "\n",
    "# Prediction data starts with x.\n",
    "x_total = list(prediction_x)\n",
    "\n",
    "# Make n predictions.\n",
    "for i in range(len(prediction_y)):\n",
    "  predict_input_fn = lambda: {TIMESERIES_FEATURE_NAME: tf.constant([x_total])}\n",
    "  p = list(estimator.predict(input_fn=predict_input_fn))\n",
    "  # For each step, append the tail element of last predicted values.  \n",
    "  x_total.append(p[-1]['predicted_temperature'])\n",
    "\n",
    "# The first len(prediction_x) elements are prediction source. So remove them.\n",
    "y_predicted = x_total[len(prediction_x):]\n",
    "\n",
    "# Zero out prediction source (making them nan), add the last value of prediction source\n",
    "# so the first edge in the curve is plotted, and add predicted values.\n",
    "y_predicted_curve = [np.nan] * (len(prediction_x)-1) + [prediction_x[-1]] + y_predicted\n",
    "\n",
    "# Plot prediction source.\n",
    "sns.tsplot(prediction_x, color='blue')\n",
    "\n",
    "# Plot truth curve.\n",
    "sns.tsplot(y_truth_curve, color='green')\n",
    "\n",
    "# Plot predicted curve.\n",
    "sns.tsplot(y_predicted_curve, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
