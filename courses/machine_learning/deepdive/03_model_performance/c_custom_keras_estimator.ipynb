{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Estimator with Keras\n",
    "\n",
    "**WARNING: This notebook currently fails**\n",
    "\n",
    "- It's possible to use `tf.feature_column` with `tf.keras` as demonstrated [here](https://colab.sandbox.google.com/drive/1knOAOGwPQTguUj8kh8xSHu05AGlPQYk7)\n",
    "- It's also possible to use `tf.keras` with `tf.estimator` as demonstrated [here](https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/machine_learning/deepdive/09_sequence/txtclsmodel/trainer/model.py)\n",
    "- This notebook attemps to use `tf.feature_column`, `tf.keras` and `tf.estimator` all together, but **currently the code fails.** \n",
    "- It's possible that this is something we'll need to wait on TF 2.0 to fix, but I invite you to try to get it working with TF 1.12\n",
    "\n",
    "**Learning Objectives**\n",
    "- Learn how to create custom estimator using tf.keras\n",
    "    \n",
    "Up until now we've been limited in our model architectures to premade estimators. But what if we want more control over the model? \n",
    "\n",
    "We can use the popular Keras API to create a custom model and then convert it to an estimator using `tf.keras.estimator.model_to_estimator()`. \n",
    "\n",
    "This gives us access to all the flexibility of Keras for creating deep learning models, but also the production readiness of the estimator framework!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import shutil\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Train and Evaluate Input Functions\n",
    "\n",
    "Same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_COLUMN_NAMES = ['fare_amount','dayofweek','hourofday','pickuplon','pickuplat','dropofflon','dropofflat','passengers']\n",
    "CSV_DEFAULTS = [[0.0],[1],[0],[-74.0], [40.0], [-74.0], [40.7], [1]]\n",
    "\n",
    "def read_dataset(csv_path):\n",
    "    def _parse_row(row):\n",
    "        # Decode the CSV row into list of TF tensors\n",
    "        fields = tf.decode_csv(row, record_defaults=CSV_DEFAULTS)\n",
    "\n",
    "        # Pack the result into a dictionary\n",
    "        features = dict(zip(CSV_COLUMN_NAMES, fields))\n",
    "        \n",
    "        # NEW: Add engineered features\n",
    "        features = add_engineered_features(features)\n",
    "        \n",
    "        # Separate the label from the features\n",
    "        label = features.pop('fare_amount') # remove label from features and store\n",
    "\n",
    "        return features, label\n",
    "    \n",
    "    # Create a dataset containing the text lines.\n",
    "    dataset = tf.data.Dataset.list_files(csv_path) # (i.e. data_file_*.csv)\n",
    "    dataset = dataset.flat_map(lambda filename:tf.data.TextLineDataset(filename).skip(1))\n",
    "\n",
    "    # Parse each CSV row into correct (features,label) format for Estimator API\n",
    "    dataset = dataset.map(_parse_row)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def train_input_fn(csv_path, batch_size=128):\n",
    "    #1. Convert CSV into tf.data.Dataset  with (features,label) format\n",
    "    dataset = read_dataset(csv_path)\n",
    "      \n",
    "    #2. Shuffle, repeat, and batch the examples.\n",
    "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "   \n",
    "    return dataset\n",
    "\n",
    "def eval_input_fn(csv_path, batch_size=128):\n",
    "    #1. Convert CSV into tf.data.Dataset  with (features,label) format\n",
    "    dataset = read_dataset(csv_path)\n",
    "\n",
    "    #2.Batch the examples.\n",
    "    dataset = dataset.batch(batch_size)\n",
    "   \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Feature Engineering\n",
    "\n",
    "Same as before except we use `feature_column_v2` which has Keras support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.feature_column import feature_column_v2 as fc # NEW\n",
    "\n",
    "# One hot encode dayofweek and hourofday\n",
    "fc_dayofweek = fc.categorical_column_with_identity('dayofweek', num_buckets = 8)\n",
    "fc_hourofday = fc.categorical_column_with_identity('hourofday', num_buckets = 24)\n",
    "\n",
    "# Cross features to get combination of day and hour\n",
    "fc_day_hr = fc.crossed_column([fc_dayofweek, fc_hourofday], 24 * 7)\n",
    "\n",
    "# Bucketize latitudes and longitudes\n",
    "NBUCKETS = 16\n",
    "latbuckets = np.linspace(38.0, 42.0, NBUCKETS).tolist()\n",
    "lonbuckets = np.linspace(-76.0, -72.0, NBUCKETS).tolist()\n",
    "fc_bucketized_plat = fc.bucketized_column(fc.numeric_column('pickuplon'), lonbuckets)\n",
    "fc_bucketized_plon = fc.bucketized_column(fc.numeric_column('pickuplat'), latbuckets)\n",
    "fc_bucketized_dlat = fc.bucketized_column(fc.numeric_column('dropofflon'), lonbuckets)\n",
    "fc_bucketized_dlon = fc.bucketized_column(fc.numeric_column('dropofflat'), latbuckets)\n",
    "\n",
    "def add_engineered_features(features):\n",
    "    features['latdiff'] = features['pickuplat'] - features['dropofflat'] # East/West\n",
    "    features['londiff'] = features['pickuplon'] - features['dropofflon'] # North/South\n",
    "    features['euclidean_dist'] = tf.sqrt(features['latdiff']**2 + features['londiff']**2)\n",
    "\n",
    "    return features\n",
    "\n",
    "feature_cols = [\n",
    "  #1. Engineered using tf.feature_column module\n",
    "  fc.indicator_column(fc_day_hr),\n",
    "  fc_bucketized_plat,\n",
    "  fc_bucketized_plon,\n",
    "  fc_bucketized_dlat,\n",
    "  fc_bucketized_dlon,\n",
    "  #2. Engineered in input functions\n",
    "  fc.numeric_column('latdiff'),\n",
    "  fc.numeric_column('londiff'),\n",
    "  fc.numeric_column('euclidean_dist') \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Build Custom Keras Model\n",
    "\n",
    "Build a Keras model as described [here](https://www.tensorflow.org/guide/keras). The only special consideration is because we're using `tf.feature_column` our first layer must be `FeatureLayer` and takes the list of feature columns as input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(fc.FeatureLayer(feature_cols))\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation=None))\n",
    "\n",
    "def rmse(y_true, y_pred): # Root Mean Squared Error\n",
    "  return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
    "\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(),\n",
    "              loss='mean_squared_error',\n",
    "              metrics=[rmse])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Train and Evaluate\n",
    "\n",
    "Note the use of `tf.keras.estimator.model_to_estimator` to create our estimator. It takes as arguments the compiled keras model, the OUTDIR, and optionally a `tf.estimator.Runconfig`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>TensorBoard was started successfully with pid 12096. Click <a href=\"/_proxy/37981/\" target=\"_blank\">here</a> to access it.</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "12096"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.datalab.ml import TensorBoard\n",
    "TensorBoard().start('taxi_trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "OUTDIR = 'taxi_trained'\n",
    "shutil.rmtree(OUTDIR, ignore_errors = True) # start fresh each time\n",
    "\n",
    "estimator = tf.keras.estimator.model_to_estimator(\n",
    "    keras_model=model,\n",
    "    model_dir = OUTDIR,\n",
    "    config = tf.estimator.RunConfig(\n",
    "          tf_random_seed=1, # for reproducibility\n",
    "          save_checkpoints_steps=100 # checkpoint every N steps\n",
    "    )\n",
    ")\n",
    "    \n",
    "train_spec=tf.estimator.TrainSpec(\n",
    "                   input_fn = lambda:train_input_fn('./taxi-train.csv'),\n",
    "                   max_steps = 500)\n",
    "\n",
    "\n",
    "eval_spec=tf.estimator.EvalSpec(\n",
    "                   input_fn=lambda:eval_input_fn('./taxi-valid.csv'),\n",
    "                   steps = None,\n",
    "                   start_delay_secs=1, # wait at least N seconds before first evaluation (default 120)\n",
    "                   throttle_secs=1, # wait at least N seconds before each subsequent evaluation (default 600)\n",
    "                   exporters = None) # export SavedModel once at the end of training\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO) # so loss is printed during training\n",
    "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(TensorBoard.list())>0:\n",
    "  [TensorBoard().stop(pid)for pid in TensorBoard.list()['pid']]\n",
    "else: print('No TensorBoard instances to stop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2019 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
