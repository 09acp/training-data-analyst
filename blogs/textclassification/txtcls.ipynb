{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Text Classification using TensorFlow on Cloud ML Engine </h1>\n",
    "\n",
    "This notebook illustrates:\n",
    "<ol>\n",
    "<li> Creating datasets for Machine Learning using BigQuery\n",
    "<li> Creating a text classification model using the high-level Estimator API \n",
    "<li> Training on Cloud ML Engine\n",
    "<li> Deploying model\n",
    "<li> Predicting with model\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change these to try this notebook out\n",
    "BUCKET = 'cloud-training-demos-ml'\n",
    "PROJECT = 'cloud-training-demos'\n",
    "REGION = 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%datalab project set -p $PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to look at the title of a newspaper article and figure out whether the article came from the New York Times or from TechCrunch. There are very sophisticated approaches that we can try, but for now, let's go with something very simple.\n",
    "\n",
    "<h2> Data exploration and preprocessing in BigQuery </h2>\n",
    "<p>\n",
    "What does the Hacker News dataset look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bqtv\" id=\"1_149790510276\"><table><tr><th>url</th><th>title</th><th>score</th></tr><tr><td></td><td>Ask HN: What are some good gift ideas for hacker types?</td><td>44</td></tr><tr><td></td><td>Ask HN: After Google Apps and Outlook, which email provider for custom domain?</td><td>22</td></tr><tr><td>http://code.ipstenu.org/2011/the-legality-of-forking/</td><td>The Legality of Forking</td><td>17</td></tr><tr><td>https://medium.com/@polarrist/where-are-chernobyl-s-children-a-photojournalist-s-honest-project-in-the-age-of-disaster-tourism-4cd333ab80c7</td><td>Where Are Chernobylâ€™s Children?</td><td>50</td></tr><tr><td>http://www.bbc.co.uk/news/technology-19597437</td><td>Twitter hands over messages at heart of Occupy case</td><td>61</td></tr><tr><td>http://weblogs.asp.net/fbouma/archive/2013/08/13/windows-store-account-getting-rid-of-it-is-as-hard-as-signing-up.aspx</td><td>Windows Store dev account: getting rid of it is as hard as signing up </td><td>51</td></tr><tr><td>http://www.forbes.com/sites/andyellwood/2012/01/18/being-a-regular/</td><td>Being A Regular</td><td>28</td></tr><tr><td></td><td>Ask HN: The Road to Becoming an Angel or VC</td><td>12</td></tr><tr><td>http://personalmba.com/best-business-books/</td><td>Best Business Books</td><td>20</td></tr><tr><td>http://www.couch.io/migrating-to-couchdb</td><td>Migrating to CouchDB</td><td>48</td></tr></table></div>\n",
       "    <br />(rows: 10, time: 2.7s,   229MB processed, job: job_xEjVjj18rbx51BN1gSFipUuGdSE)<br />\n",
       "    <script>\n",
       "\n",
       "      require.config({\n",
       "        paths: {\n",
       "          d3: '//cdnjs.cloudflare.com/ajax/libs/d3/3.4.13/d3',\n",
       "          plotly: 'https://cdn.plot.ly/plotly-1.5.1.min.js?noext',\n",
       "          jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min'\n",
       "        },\n",
       "        map: {\n",
       "          '*': {\n",
       "            datalab: 'nbextensions/gcpdatalab'\n",
       "          }\n",
       "        },\n",
       "        shim: {\n",
       "          plotly: {\n",
       "            deps: ['d3', 'jquery'],\n",
       "            exports: 'plotly'\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "\n",
       "      require(['datalab/charting', 'datalab/element!1_149790510276', 'base/js/events',\n",
       "          'datalab/style!/nbextensions/gcpdatalab/charting.css'],\n",
       "        function(charts, dom, events) {\n",
       "          charts.render('gcharts', dom, events, 'table', [], {\"rows\": [{\"c\": [{\"v\": \"\"}, {\"v\": \"Ask HN: What are some good gift ideas for hacker types?\"}, {\"v\": 44}]}, {\"c\": [{\"v\": \"\"}, {\"v\": \"Ask HN: After Google Apps and Outlook, which email provider for custom domain?\"}, {\"v\": 22}]}, {\"c\": [{\"v\": \"http://code.ipstenu.org/2011/the-legality-of-forking/\"}, {\"v\": \"The Legality of Forking\"}, {\"v\": 17}]}, {\"c\": [{\"v\": \"https://medium.com/@polarrist/where-are-chernobyl-s-children-a-photojournalist-s-honest-project-in-the-age-of-disaster-tourism-4cd333ab80c7\"}, {\"v\": \"Where Are Chernobyl\\u2019s Children?\"}, {\"v\": 50}]}, {\"c\": [{\"v\": \"http://www.bbc.co.uk/news/technology-19597437\"}, {\"v\": \"Twitter hands over messages at heart of Occupy case\"}, {\"v\": 61}]}, {\"c\": [{\"v\": \"http://weblogs.asp.net/fbouma/archive/2013/08/13/windows-store-account-getting-rid-of-it-is-as-hard-as-signing-up.aspx\"}, {\"v\": \"Windows Store dev account: getting rid of it is as hard as signing up \"}, {\"v\": 51}]}, {\"c\": [{\"v\": \"http://www.forbes.com/sites/andyellwood/2012/01/18/being-a-regular/\"}, {\"v\": \"Being A Regular\"}, {\"v\": 28}]}, {\"c\": [{\"v\": \"\"}, {\"v\": \"Ask HN: The Road to Becoming an Angel or VC\"}, {\"v\": 12}]}, {\"c\": [{\"v\": \"http://personalmba.com/best-business-books/\"}, {\"v\": \"Best Business Books\"}, {\"v\": 20}]}, {\"c\": [{\"v\": \"http://www.couch.io/migrating-to-couchdb\"}, {\"v\": \"Migrating to CouchDB\"}, {\"v\": 48}]}], \"cols\": [{\"type\": \"string\", \"id\": \"url\", \"label\": \"url\"}, {\"type\": \"string\", \"id\": \"title\", \"label\": \"title\"}, {\"type\": \"number\", \"id\": \"score\", \"label\": \"score\"}]},\n",
       "            {\n",
       "              pageSize: 25,\n",
       "              cssClassNames:  {\n",
       "                tableRow: 'gchart-table-row',\n",
       "                headerRow: 'gchart-table-headerrow',\n",
       "                oddTableRow: 'gchart-table-oddrow',\n",
       "                selectedTableRow: 'gchart-table-selectedrow',\n",
       "                hoverTableRow: 'gchart-table-hoverrow',\n",
       "                tableCell: 'gchart-table-cell',\n",
       "                headerCell: 'gchart-table-headercell',\n",
       "                rowNumberCell: 'gchart-table-rownumcell'\n",
       "              }\n",
       "            },\n",
       "            {source_index: 0, fields: 'url,title,score'},\n",
       "            0,\n",
       "            10);\n",
       "        }\n",
       "      );\n",
       "    </script>\n",
       "  "
      ],
      "text/plain": [
       "QueryResultsTable job_xEjVjj18rbx51BN1gSFipUuGdSE"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%bq query\n",
    "SELECT\n",
    "  url, title, score\n",
    "FROM\n",
    "  `bigquery-public-data.hacker_news.stories`\n",
    "WHERE\n",
    "  LENGTH(title) > 10\n",
    "  AND score > 10\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some regular expression parsing in BigQuery to get the source of the newspaper article from the URL. For example, if the url is http://mobile.nytimes.com/...., I want to be left with <i>nytimes</i>. To ensure that the parsing works for all URLs of interest, I'll group by the source to make sure there are no weird names left. This was an iterative process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query=\"\"\"\n",
    "SELECT\n",
    "  ARRAY_REVERSE(SPLIT(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.'))[OFFSET(1)] AS source,\n",
    "  COUNT(title) AS num_articles\n",
    "FROM\n",
    "  `bigquery-public-data.hacker_news.stories`\n",
    "WHERE\n",
    "  REGEXP_CONTAINS(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.com$')\n",
    "  AND LENGTH(title) > 10\n",
    "GROUP BY\n",
    "  source\n",
    "ORDER BY num_articles DESC\n",
    "LIMIT 10\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>num_articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blogspot</td>\n",
       "      <td>41386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>github</td>\n",
       "      <td>36525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>techcrunch</td>\n",
       "      <td>30891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>youtube</td>\n",
       "      <td>30848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nytimes</td>\n",
       "      <td>28787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>medium</td>\n",
       "      <td>18422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>google</td>\n",
       "      <td>18235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>wordpress</td>\n",
       "      <td>17667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>arstechnica</td>\n",
       "      <td>13749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>wired</td>\n",
       "      <td>12841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        source  num_articles\n",
       "0     blogspot         41386\n",
       "1       github         36525\n",
       "2   techcrunch         30891\n",
       "3      youtube         30848\n",
       "4      nytimes         28787\n",
       "5       medium         18422\n",
       "6       google         18235\n",
       "7    wordpress         17667\n",
       "8  arstechnica         13749\n",
       "9        wired         12841"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import google.datalab.bigquery as bq\n",
    "df = bq.Query(query).execute().result().to_dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have good parsing of the URL to get the source, let's put together a dataset of source and titles. This will be our labeled dataset for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>github</td>\n",
       "      <td>Opinionated Dress Color Simulator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>techcrunch</td>\n",
       "      <td>Meteor Raises $20M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>techcrunch</td>\n",
       "      <td>DataSift Raises $15M To Help Businesses Mine A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>github</td>\n",
       "      <td>Writing Cross-Platform Games in Rust Using Piston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>github</td>\n",
       "      <td>DAws   Advanced Web Shell</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       source                                              title\n",
       "0      github                  Opinionated Dress Color Simulator\n",
       "1  techcrunch                                 Meteor Raises $20M\n",
       "2  techcrunch  DataSift Raises $15M To Help Businesses Mine A...\n",
       "3      github  Writing Cross-Platform Games in Rust Using Piston\n",
       "4      github                          DAws   Advanced Web Shell"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\"\"\"\n",
    "SELECT source, REGEXP_REPLACE(title, '[^a-zA-Z0-9 $.-]', ' ') AS title FROM\n",
    "(SELECT\n",
    "  ARRAY_REVERSE(SPLIT(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.'))[OFFSET(1)] AS source,\n",
    "  title\n",
    "FROM\n",
    "  `bigquery-public-data.hacker_news.stories`\n",
    "WHERE\n",
    "  REGEXP_CONTAINS(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.com$')\n",
    "  AND LENGTH(title) > 10\n",
    ")\n",
    "WHERE (source = 'github' OR source = 'nytimes' OR source = 'techcrunch')\n",
    "\"\"\"\n",
    "df = bq.Query(query + \" LIMIT 10\").execute().result().to_dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ML training, we will need to split our dataset into training and evaluation datasets (and perhaps an independent test dataset if we are going to do model or feature selection based on the evaluation dataset).  A simple way to do this is to use the hash of a well-distributed column in our data (See https://www.oreilly.com/learning/repeatable-sampling-of-data-sets-in-bigquery-for-machine-learning).\n",
    "<p>\n",
    "So, let's do that and save the results as CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>github</td>\n",
       "      <td>Awesome per directory history for ZSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>github</td>\n",
       "      <td>PHP class which implements the Elo rating system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>github</td>\n",
       "      <td>Comic Sans Everything</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>github</td>\n",
       "      <td>Amazing Flat version of Twitter Bootstrap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>github</td>\n",
       "      <td>A year of fun and hard work on learning Dart</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source                                             title\n",
       "0  github             Awesome per directory history for ZSH\n",
       "1  github  PHP class which implements the Elo rating system\n",
       "2  github                             Comic Sans Everything\n",
       "3  github         Amazing Flat version of Twitter Bootstrap\n",
       "4  github      A year of fun and hard work on learning Dart"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf = bq.Query(query + \" AND MOD(ABS(FARM_FINGERPRINT(title)),4) > 0\").execute().result().to_dataframe()\n",
    "evaldf  = bq.Query(query + \" AND MOD(ABS(FARM_FINGERPRINT(title)),4) = 0\").execute().result().to_dataframe()\n",
    "traindf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "github        27445\n",
       "techcrunch    23131\n",
       "nytimes       21586\n",
       "Name: source, dtype: int64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf['source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "github        9080\n",
       "techcrunch    7760\n",
       "nytimes       7201\n",
       "Name: source, dtype: int64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaldf['source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "traindf.to_csv('train.csv', header=False, index=False, encoding='utf-8', sep='\\t')\n",
    "evaldf.to_csv('eval.csv', header=False, index=False, encoding='utf-8', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "github\tAwesome per directory history for ZSH\r\n",
      "github\tPHP class which implements the Elo rating system\r\n",
      "github\tComic Sans Everything\r\n"
     ]
    }
   ],
   "source": [
    "!head -3 train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  24041 eval.csv\n",
      "  72162 train.csv\n",
      "  72164 vocab.csv\n",
      " 168367 total\n"
     ]
    }
   ],
   "source": [
    "!wc -l *.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://eval.csv [Content-Type=text/csv]...\n",
      "/ [0 files][    0.0 B/  1.4 MiB]                                                \r",
      "/ [0 files][263.4 KiB/  1.4 MiB]                                                \r",
      "-\r",
      "- [1 files][  1.4 MiB/  1.4 MiB]                                                \r",
      "\\\r",
      "Copying file://train.csv [Content-Type=text/csv]...\n",
      "\\ [1 files][  1.4 MiB/  5.4 MiB]                                                \r",
      "|\r",
      "| [1 files][  5.4 MiB/  5.4 MiB]                                                \r",
      "| [2 files][  5.4 MiB/  5.4 MiB]                                                \r",
      "/\r",
      "Copying file://vocab.csv [Content-Type=text/csv]...\n",
      "/ [2 files][  5.4 MiB/  9.4 MiB]                                                \r",
      "-\r",
      "- [3 files][  9.4 MiB/  9.4 MiB]                                                \r",
      "\\\r\n",
      "Operation completed over 3 objects/9.4 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gsutil cp *.csv gs://${BUCKET}/txtcls1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> TensorFlow code </h2>\n",
    "\n",
    "Please explore the code in this <a href=\"txtcls1/trainer\">directory</a> -- <a href=\"txtcls1/trainer/model.py\">model.py</a> contains the key TensorFlow model and <a href=\"txtcls1/trainer/task.py\">task.py</a> has a main() that launches off the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def init(bucket, num_epochs):\n",
      "def save_vocab(trainfile, txtcolname, outfilename):\n",
      "def read_dataset(prefix, batch_size=20):\n",
      "def cnn_model(features, target, mode):\n",
      "def linear_model(features, target, mode):\n",
      "def serving_input_fn():\n",
      "def get_train():\n",
      "def get_valid():\n",
      "def experiment_fn(output_dir):\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "grep \"^def\" txtcls1/trainer/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure the code works locally on a small dataset for a few epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucket=cloud-training-demos-ml\n",
      "6355 words in gs://cloud-training-demos-ml/txtcls1/train.csv being written to gs://cloud-training-demos-ml/txtcls1/vocab_words\n",
      "lookup_words=Tensor(\"word_to_index_Lookup:0\", shape=(?, ?), dtype=int64)\n",
      "words_sliced=Tensor(\"Slice:0\", shape=(?, 20), dtype=int64)\n",
      "words_embed=Tensor(\"words/embedding_lookup:0\", shape=(?, 20, 10), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying gs://cloud-training-demos-ml/txtcls1/train.csv...\n",
      "/ [0 files][    0.0 B/  4.0 MiB]                                                \r",
      "-\r",
      "- [0 files][  1.3 MiB/  4.0 MiB]                                                \r",
      "\\\r",
      "|\r",
      "| [0 files][  4.0 MiB/  4.0 MiB]                                                \r",
      "| [1 files][  4.0 MiB/  4.0 MiB]                                                \r\n",
      "Operation completed over 1 objects/4.0 MiB.                                      \n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f1cdac6ce10>, '_model_dir': 'outputdir/', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_session_config': None, '_tf_random_seed': None, '_environment': 'local', '_num_worker_replicas': 0, '_task_id': 0, '_save_summary_steps': 100, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_evaluation_master': '', '_master': ''}\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/monitors.py:268: __init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\n",
      "Instructions for updating:\n",
      "Monitors are deprecated. Please use tf.train.SessionRunHook.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n",
      "    \"__main__\", fname, loader, pkg_name)\n",
      "  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n",
      "    exec code in run_globals\n",
      "  File \"/content/training-data-analyst/blogs/textclassification/txtcls1/trainer/task.py\", line 73, in <module>\n",
      "    learn_runner.run(model.experiment_fn, output_dir)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/learn_runner.py\", line 210, in run\n",
      "    return _execute_schedule(experiment, schedule)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/learn_runner.py\", line 47, in _execute_schedule\n",
      "    return task()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/experiment.py\", line 495, in train_and_evaluate\n",
      "    self.train(delay_secs=0)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/experiment.py\", line 275, in train\n",
      "    hooks=self._train_monitors + extra_hooks)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/experiment.py\", line 665, in _call_train\n",
      "    monitors=hooks)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py\", line 289, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 455, in fit\n",
      "    loss = self._train_model(input_fn=input_fn, hooks=hooks)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 955, in _train_model\n",
      "    model_fn_ops = self._get_train_ops(features, labels)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1162, in _get_train_ops\n",
      "    return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.TRAIN)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1133, in _call_model_fn\n",
      "    model_fn_results = self._model_fn(features, labels, **kwargs)\n",
      "  File \"/content/training-data-analyst/blogs/textclassification/txtcls1/trainer/model.py\", line 205, in linear_model\n",
      "    words, 5, [3, EMBEDDING_SIZE] , padding='VALID')\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 181, in func_with_args\n",
      "    return func(*args, **current_args)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 946, in convolution\n",
      "    _reuse=reuse)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/convolutional.py\", line 292, in __init__\n",
      "    name=name, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/convolutional.py\", line 105, in __init__\n",
      "    self.kernel_size = utils.normalize_tuple(kernel_size, rank, 'kernel_size')\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/utils.py\", line 84, in normalize_tuple\n",
      "    str(n) + ' integers. Received: ' + str(value))\n",
      "ValueError: The `kernel_size` argument must be a tuple of 1 integers. Received: [3, 10]\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "echo \"bucket=${BUCKET}\"\n",
    "rm -rf outputdir\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/txtcls1\n",
    "python -m trainer.task \\\n",
    "   --bucket=${BUCKET} \\\n",
    "   --output_dir=outputdir \\\n",
    "   --job-dir=./tmp --num_epochs=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I ran it, I got a 30% accuracy in two epochs. Which is essentially a coin toss or whatever you call something with 1/3 odds ...\n",
    "<p>\n",
    "Once the code works in standalone mode, you can run it on Cloud ML Engine. You can monitor the job from the GCP console in the Cloud Machine Learning Engine section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://cloud-training-demos-ml/txtcls1/trained_model us-central1 txtcls_170620_220206\n",
      "jobId: txtcls_170620_220206\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing gs://cloud-training-demos-ml/txtcls1/trained_model/#1497904932113739...\n",
      "Removing gs://cloud-training-demos-ml/txtcls1/trained_model/__init__.py#1497904590096433...\n",
      "Removing gs://cloud-training-demos-ml/txtcls1/trained_model/export/Servo/1497904940/assets/#1497904944025067...\n",
      "Removing gs://cloud-training-demos-ml/txtcls1/trained_model/checkpoint#1497904933774605...\n",
      "Removing gs://cloud-training-demos-ml/txtcls1/trained_model/eval/events.out.tfevents.1497904939.master-d3c4dcd8c0-0-kkkjr#1497904940174974...\n",
      "Removing gs://cloud-training-demos-ml/txtcls1/trained_model/eval/#1497904922743917...\n",
      "Removing gs://cloud-training-demos-ml/txtcls1/trained_model/events.out.tfevents.1497904906.master-d3c4dcd8c0-0-kkkjr#1497904934854218...\n",
      "Removing gs://cloud-training-demos-ml/txtcls1/trained_model/eval/events.out.tfevents.1497904922.master-d3c4dcd8c0-0-kkkjr#1497904923798317...\n",
      "Removing gs://cloud-training-demos-ml/txtcls1/trained_model/export/Servo/1497904940/assets/vocab_words#1497904944632344...\n",
      "Removing gs://cloud-training-demos-ml/txtcls1/trained_model/export/Servo/1497904940/variables/variables.index#1497904948631720...\n",
      "Removing gs://cloud-training-demos-ml/txtcls1/trained_model/export/#1497904942650271...\n",
      "Removing gs://cloud-training-demos-ml/txtcls1/trained_model/export/Servo/1497904940/variables/#1497904947453252...\n",
      "Removing gs://cloud-training-demos-ml/txtcls1/trained_model/graph.pbtxt#1497904909237020...\n",
      "Removing gs://cloud-training-demos-ml/txtcls1/trained_model/model.ckpt-1.data-00000-of-00001#1497904912955428...\n",
      "Removing gs://cloud-training-demos-ml/txtcls1/trained_model/model.ckpt-1.index#1497904913277156...\n",
      "Removing gs://cloud-training-demos-ml/txtcls1/trained_model/export/Servo/1497904940/saved_model.pb#1497904949300237...\n",
      "Removing gs://cloud-training-demos-ml/txtcls1/trained_model/model.ckpt-1.meta#1497904915189957...\n",
      "Removing gs://cloud-training-demos-ml/txtcls1/trained_model/export/Servo/#1497904943091239...\n",
      "Removing gs://cloud-training-demos-ml/txtcls1/trained_model/export/Servo/1497904940/variables/variables.data-00000-of-00001#1497904948355518...\n",
      "Removing gs://cloud-training-demos-ml/txtcls1/trained_model/export/Servo/1497904940/#1497904943505335...\n",
      "Removing gs://cloud-training-demos-ml/txtcls1/trained_model/model.ckpt-3580.data-00000-of-00001#1497904932658172...\n",
      "Removing gs://cloud-training-demos-ml/txtcls1/trained_model/model.ckpt-3580.index#1497904933081910...\n",
      "Removing gs://cloud-training-demos-ml/txtcls1/trained_model/model.ckpt-3580.meta#1497904934577588...\n",
      "/ [1/25 objects]   4% Done                                                      \r",
      "/ [2/25 objects]   8% Done                                                      \r",
      "/ [3/25 objects]  12% Done                                                      \r",
      "/ [4/25 objects]  16% Done                                                      \r",
      "/ [5/25 objects]  20% Done                                                      \r",
      "/ [6/25 objects]  24% Done                                                      \r",
      "Removing gs://cloud-training-demos-ml/txtcls1/trained_model/model.py#1497904590312090...\n",
      "Removing gs://cloud-training-demos-ml/txtcls1/trained_model/task.py#1497904590513051...\n",
      "/ [7/25 objects]  28% Done                                                      \r",
      "/ [8/25 objects]  32% Done                                                      \r",
      "/ [9/25 objects]  36% Done                                                      \r",
      "/ [10/25 objects]  40% Done                                                     \r",
      "/ [11/25 objects]  44% Done                                                     \r",
      "/ [12/25 objects]  48% Done                                                     \r",
      "/ [13/25 objects]  52% Done                                                     \r",
      "/ [14/25 objects]  56% Done                                                     \r",
      "/ [15/25 objects]  60% Done                                                     \r",
      "/ [16/25 objects]  64% Done                                                     \r",
      "/ [17/25 objects]  68% Done                                                     \r",
      "/ [18/25 objects]  72% Done                                                     \r",
      "/ [19/25 objects]  76% Done                                                     \r",
      "/ [20/25 objects]  80% Done                                                     \r",
      "/ [21/25 objects]  84% Done                                                     \r",
      "/ [22/25 objects]  88% Done                                                     \r",
      "/ [23/25 objects]  92% Done                                                     \r",
      "/ [24/25 objects]  96% Done                                                     \r",
      "/ [25/25 objects] 100% Done                                                     \r\n",
      "Operation completed over 25 objects.                                             \n",
      "Copying file://txtcls1/trainer/__init__.py [Content-Type=text/x-python]...\n",
      "/ [0 files][    0.0 B/  677.0 B]                                                \r",
      "/ [1 files][  677.0 B/  677.0 B]                                                \r",
      "Copying file://txtcls1/trainer/model.py [Content-Type=text/x-python]...\n",
      "/ [1 files][  677.0 B/  9.2 KiB]                                                \r",
      "/ [2 files][  9.2 KiB/  9.2 KiB]                                                \r",
      "Copying file://txtcls1/trainer/task.py [Content-Type=text/x-python]...\n",
      "/ [2 files][  9.2 KiB/ 11.2 KiB]                                                \r",
      "/ [3 files][ 11.2 KiB/ 11.2 KiB]                                                \r",
      "-\r\n",
      "Operation completed over 3 objects/11.2 KiB.                                     \n",
      "Job [txtcls_170620_220206] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs describe txtcls_170620_220206\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs stream-logs txtcls_170620_220206\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "OUTDIR=gs://${BUCKET}/txtcls1/trained_model\n",
    "JOBNAME=txtcls_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gsutil cp txtcls1/trainer/*.py $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "   --region=$REGION \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=$(pwd)/txtcls1/trainer \\\n",
    "   --job-dir=$OUTDIR \\\n",
    "   --staging-bucket=gs://$BUCKET \\\n",
    "   --scale-tier=BASIC --runtime-version=1.2 \\\n",
    "   -- \\\n",
    "   --bucket=${BUCKET} \\\n",
    "   --output_dir=${OUTDIR} \\\n",
    "   --num_epochs=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training finished with an accuracy of 61%.  Obviously, this was trained on a really small dataset. But I hope the sample works for you to apply to your *real* data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Deploy trained model </h2>\n",
    "<p>\n",
    "Deploying the trained model to act as a REST web service is a simple gcloud call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://cloud-training-demos-ml/txtcls1/trained_model/export/Servo/\n",
      "gs://cloud-training-demos-ml/txtcls1/trained_model/export/Servo/1497904940/\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gsutil ls gs://${BUCKET}/txtcls1/trained_model/export/Servo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting and deploying txtcls v1 from gs://cloud-training-demos-ml/txtcls1/trained_model/export/Servo/1497904940/ ... this will take a few minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating version (this might take a few minutes)......\n",
      "................................................................................done.\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "MODEL_NAME=\"txtcls\"\n",
    "MODEL_VERSION=\"v1\"\n",
    "MODEL_LOCATION=$(gsutil ls gs://${BUCKET}/txtcls1/trained_model/export/Servo/ | tail -1)\n",
    "echo \"Deleting and deploying $MODEL_NAME $MODEL_VERSION from $MODEL_LOCATION ... this will take a few minutes\"\n",
    "#gcloud ml-engine versions delete ${MODEL_VERSION} --model ${MODEL_NAME}\n",
    "#gcloud ml-engine models delete ${MODEL_NAME}\n",
    "gcloud ml-engine models create ${MODEL_NAME} --regions $REGION\n",
    "gcloud ml-engine versions create ${MODEL_VERSION} --model ${MODEL_NAME} --origin ${MODEL_LOCATION}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Use model to predict </h2>\n",
    "<p>\n",
    "Send a JSON request to the endpoint of the service to make it predict which publication the article is more likely to run in. These are actual titles of articles in the New York Times, TechCrunch, and Wired on June 19."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response={u'predictions': [{u'source': u'nytimes', u'prob': [0.6047254800796509, 0.24059338867664337, 0.15468111634254456], u'class': 0}, {u'source': u'nytimes', u'prob': [0.6047254800796509, 0.24059338867664337, 0.15468111634254456], u'class': 0}, {u'source': u'nytimes', u'prob': [0.6047254800796509, 0.24059338867664337, 0.15468111634254456], u'class': 0}]}\n"
     ]
    }
   ],
   "source": [
    "from googleapiclient import discovery\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import json\n",
    "\n",
    "credentials = GoogleCredentials.get_application_default()\n",
    "api = discovery.build('ml', 'v1beta1', credentials=credentials,\n",
    "            discoveryServiceUrl='https://storage.googleapis.com/cloud-ml/discovery/ml_v1beta1_discovery.json')\n",
    "\n",
    "request_data = {'instances':\n",
    "  [\n",
    "      {\n",
    "        'inputs': 'Supreme Court to Hear Major Case on Partisan Districts'\n",
    "      },\n",
    "      {\n",
    "        'inputs': 'Time Warner will spend $100M on Snapchat original shows and ads'\n",
    "      },\n",
    "      {\n",
    "        'inputs': 'This Dark Matter Theory Could Solve a Celestial Conundrum'\n",
    "      },\n",
    "  ]\n",
    "}\n",
    "\n",
    "parent = 'projects/%s/models/%s/versions/%s' % (PROJECT, 'txtcls', 'v1')\n",
    "response = api.projects().predict(body=request_data, name=parent).execute()\n",
    "print \"response={0}\".format(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the model, our son would have clocked in at 7.3 lbs and our daughter at 6.8 lbs.\n",
    "<p>\n",
    "The weights are off by about 0.5 lbs. Pretty cool!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2017 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
