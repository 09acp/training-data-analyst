{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Hyper-parameter tuning </h1>\n",
    "\n",
    "**Learning Objectives**\n",
    "1. Understand various approaches to hyperparameter tuning\n",
    "2. Automate hyperparameter tuning using CMLE HyperTune\n",
    "\n",
    "In the previous notebook we achieved an RMSE of 4.79 using just two features after some feature engineering. Let's see if we can improve upon that by tuning our hyperparameters.\n",
    "\n",
    "Hyperparameters are parameters that are set *prior* to training a model, as opposed to parameters which are learned *during* training. \n",
    "\n",
    "These include learning rate and batch size, but also model design parameters such as type of activation function and number of hidden units.\n",
    "\n",
    "Here are the four most common ways to finding the ideal hyperparameters:\n",
    "1. Manual\n",
    "2. Grid Search\n",
    "3. Random Search\n",
    "4. Bayesian Optimzation\n",
    "\n",
    "**1. Manual**\n",
    "\n",
    "Traditionaly, hyperparameter tuning is a manual trial and error process. A data scientist has some intution about suitable hyperparameters which they use as a starting point, then they observe the result and use that information to try a new set of hyperparameters to try to beat the existing performance. \n",
    "\n",
    "Pros\n",
    "- Educational, builds up your intuition as a data scientist\n",
    "- Inexpensive because only one trial is conducted at a time\n",
    "\n",
    "Cons\n",
    "- Requires alot of time and patience\n",
    "\n",
    "**2. Grid Search**\n",
    "\n",
    "On the other extreme we can use grid search. Define a discrete set of values to try for each hyperparameter then try every possible combination. \n",
    "\n",
    "Pros\n",
    "- Can run hundreds of trials in parallel using the cloud\n",
    "- Gauranteed to find the best solution within the search space\n",
    "\n",
    "Cons\n",
    "- Expensive\n",
    "\n",
    "**3. Random Search**\n",
    "\n",
    "Alternatively define a range for each hyperparamter (e.g. 0-256) and sample uniformly at random from that range. \n",
    "\n",
    "Pros\n",
    "- Can run hundreds of trials in parallel using the cloud\n",
    "- Requires less trials than Grid Search to find a good solution\n",
    "\n",
    "Cons\n",
    "- Expensive (but less so than Grid Search)\n",
    "\n",
    "**4. Bayesian Optimization**\n",
    "\n",
    "Unlike Grid Search and Random Search, Bayesian Optimization takes into account information from  past trials to select parameters for future trials. The details of how this is done is beyond the scope of this notebook, but if you're interested you can read how it works here [here](https://cloud.google.com/blog/products/gcp/hyperparameter-tuning-cloud-machine-learning-engine-using-bayesian-optimization). \n",
    "\n",
    "Pros\n",
    "- Picks values intelligenty based on results from past trials\n",
    "- Less expensive because requires fewer trials to get a good result\n",
    "\n",
    "Cons\n",
    "- Requires sequential trials for best results, takes longer\n",
    "\n",
    "**CMLE HyperTune**\n",
    "\n",
    "CMLE HyperTune, powered by [Google Vizier](https://ai.google/research/pubs/pub46180), uses Bayesian Optimization by default, but [also supports](https://cloud.google.com/ml-engine/docs/tensorflow/hyperparameter-tuning-overview#search_algorithms) Grid Search and Random Search. \n",
    "\n",
    "\n",
    "When tuning just a few hyperparameters (say less than 4), Grid Search and Random Search work well, but when tunining several hyperparameters and the search space is large Bayesian Optimization is best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT='cloud-training-demos' # CHANGE TO YOUR PROJECT\n",
    "BUCKET = PROJECT # CHANGE TO YOUR BUCKET \n",
    "REGION = 'us-central1' # optionally change, see https://cloud.google.com/ml-engine/docs/tensorflow/regions\n",
    "TFVERSION = '1.12' # TF version for CMLE to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move Code into Python Package\n",
    "\n",
    "Let's package our updated code with feature engineering into a package so it's CMLE compatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘taxifaremodel’: File exists\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "mkdir taxifaremodel\n",
    "touch taxifaremodel/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model.py\n",
    "\n",
    "Note that any hyperparameters we want to tune need to be exposed as command line arguments, these include number of embedding dimensions, learning rate, dropout rate, and number of hiddent units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting taxifaremodel/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile taxifaremodel/model.py\n",
    "import tensorflow as tf\n",
    "import shutil\n",
    "print(tf.__version__)\n",
    "\n",
    "NEMBEDS = 3 # number of embedding dimensions, will be overwritten by task.py\n",
    "\n",
    "#1. Train and Evaluate Input Functions\n",
    "CSV_COLUMN_NAMES = ['fare_amount','dayofweek','hourofday','pickuplon','pickuplat','dropofflon','dropofflat','passengers']\n",
    "CSV_DEFAULTS = [[0.0],[1],[0],[-74.0], [40.0], [-74.0], [40.7], [1]]\n",
    "\n",
    "def read_dataset(csv_path):\n",
    "    def _parse_row(row):\n",
    "        # Decode the CSV row into list of TF tensors\n",
    "        fields = tf.decode_csv(row, record_defaults=CSV_DEFAULTS)\n",
    "\n",
    "        # Pack the result into a dictionary\n",
    "        features = dict(zip(CSV_COLUMN_NAMES, fields))\n",
    "        \n",
    "        # NEW: Add engineered features\n",
    "        features = add_engineered_features(features)\n",
    "        \n",
    "        # Separate the label from the features\n",
    "        label = features.pop('fare_amount') # remove label from features and store\n",
    "\n",
    "        return features, label\n",
    "    \n",
    "    # Create a dataset containing the text lines.\n",
    "    dataset = tf.data.Dataset.list_files(csv_path) # (i.e. data_file_*.csv)\n",
    "    dataset = dataset.flat_map(lambda filename:tf.data.TextLineDataset(filename).skip(1))\n",
    "\n",
    "    # Parse each CSV row into correct (features,label) format for Estimator API\n",
    "    dataset = dataset.map(_parse_row)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def train_input_fn(csv_path, batch_size=128):\n",
    "    #1. Convert CSV into tf.data.Dataset  with (features,label) format\n",
    "    dataset = read_dataset(csv_path)\n",
    "      \n",
    "    #2. Shuffle, repeat, and batch the examples.\n",
    "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "   \n",
    "    return dataset\n",
    "\n",
    "def eval_input_fn(csv_path, batch_size=128):\n",
    "    #1. Convert CSV into tf.data.Dataset  with (features,label) format\n",
    "    dataset = read_dataset(csv_path)\n",
    "\n",
    "    #2.Batch the examples.\n",
    "    dataset = dataset.batch(batch_size)\n",
    "   \n",
    "    return dataset\n",
    "  \n",
    "#2. Feature Engineering\n",
    "def add_engineered_features(features):\n",
    "    latdiff = features['pickuplat'] - features['dropofflat']\n",
    "    londiff = features['pickuplon'] - features['dropofflon']\n",
    "    euclidean_dist = tf.sqrt(latdiff**2 + londiff**2)\n",
    "    \n",
    "    features['euclidean_dist'] = euclidean_dist\n",
    "    return features\n",
    "\n",
    "fc_distance = tf.feature_column.numeric_column('euclidean_dist')\n",
    "fc_dayofweek = tf.feature_column.categorical_column_with_identity('dayofweek', num_buckets = 8)\n",
    "fc_hourofday = tf.feature_column.categorical_column_with_identity('hourofday', num_buckets = 24)\n",
    "fc_day_hr = tf.feature_column.crossed_column([fc_dayofweek, fc_hourofday], 24 * 7)\n",
    "fc_day_hr_embedded = tf.feature_column.embedding_column(fc_day_hr, NEMBEDS)\n",
    "\n",
    "feature_cols = [fc_distance,fc_day_hr_embedded]\n",
    "\n",
    "#3. Serving Input Receiver Function\n",
    "def serving_input_receiver_fn():\n",
    "    receiver_tensors = {\n",
    "        'dayofweek' : tf.placeholder(tf.int32, shape=[None]), # shape is vector to allow batch of requests\n",
    "        'hourofday' : tf.placeholder(tf.int32, shape=[None]),\n",
    "        'pickuplon' : tf.placeholder(tf.float32, shape=[None]), \n",
    "        'pickuplat' : tf.placeholder(tf.float32, shape=[None]),\n",
    "        'dropofflat' : tf.placeholder(tf.float32, shape=[None]),\n",
    "        'dropofflon' : tf.placeholder(tf.float32, shape=[None]),\n",
    "        'passengers' : tf.placeholder(tf.int32, shape=[None]),\n",
    "    }\n",
    "    \n",
    "    features = add_engineered_features(receiver_tensors) # 'features' is what is passed on to the model\n",
    "\n",
    "    return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)\n",
    "  \n",
    "#4. Train and Evaluate\n",
    "def train_and_evaluate(params):\n",
    "  OUTDIR = params['output_dir']\n",
    "\n",
    "  model = tf.estimator.DNNRegressor(\n",
    "    hidden_units = [params['hidden_units_1'],params['hidden_units_2']], # specify neural architecture\n",
    "    feature_columns = feature_cols, \n",
    "    model_dir = OUTDIR,\n",
    "    optimizer = tf.train.AdamOptimizer(params['learning_rate']), # NEW\n",
    "    dropout = params['dropout'], # NEW\n",
    "    config = tf.estimator.RunConfig(\n",
    "          tf_random_seed=1, # for reproducibility\n",
    "          save_checkpoints_steps=max(100,params['train_steps']//10) # checkpoint every N steps\n",
    "    ) \n",
    "  )\n",
    "  \n",
    "  # Add custom evaluation metric\n",
    "  def my_rmse(labels, predictions):\n",
    "    pred_values = tf.squeeze(predictions['predictions'],axis=-1)\n",
    "    return {'rmse': tf.metrics.root_mean_squared_error(labels, pred_values)}\n",
    "  model = tf.contrib.estimator.add_metrics(model, my_rmse)  \n",
    "\n",
    "  train_spec=tf.estimator.TrainSpec(\n",
    "                     input_fn = lambda:train_input_fn(params['train_data_path']),\n",
    "                     max_steps = params['train_steps'])\n",
    "\n",
    "  exporter = tf.estimator.FinalExporter('exporter', serving_input_receiver_fn) # export SavedModel once at the end of training\n",
    "  # Note: alternatively use tf.estimator.BestExporter to export at every checkpoint that has lower loss than the previous checkpoint\n",
    "\n",
    "\n",
    "  eval_spec=tf.estimator.EvalSpec(\n",
    "                     input_fn=lambda:eval_input_fn(params['eval_data_path']),\n",
    "                     steps = None,\n",
    "                     start_delay_secs=1, # wait at least N seconds before first evaluation (default 120)\n",
    "                     throttle_secs=1, # wait at least N seconds before each subsequent evaluation (default 600)\n",
    "                     exporters = exporter) # export SavedModel once at the end of training\n",
    "\n",
    "  tf.logging.set_verbosity(tf.logging.INFO) # so loss is printed during training\n",
    "  shutil.rmtree(OUTDIR, ignore_errors = True) # start fresh each time\n",
    "\n",
    "  tf.estimator.train_and_evaluate(model, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task.py\n",
    "\n",
    "When doing hyperparameter tuning we need to make sure the output directory is different for each run, otherwise successive runs will overwrite previous runs. \n",
    "\n",
    "One way to do this is to append the trial id, look for that code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting taxifaremodel/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile taxifaremodel/task.py\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "from . import model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--nembeds',\n",
    "        help = 'Embedding dimensions for day_hr (default: 3)',\n",
    "        type = int,\n",
    "        default = 3\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--dropout',\n",
    "        help = 'Percent of units to drop from last layer (default: 0.0)',\n",
    "        type = float,\n",
    "        default = 0.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--hidden_units_1',\n",
    "        help = 'Units in first hidden layer of DNN (default: 10)',\n",
    "        type = int,\n",
    "        default = 10\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--hidden_units_2',\n",
    "        help = 'Units in second hidden layer of DNN (default: 10)',\n",
    "        type = int,\n",
    "        default = 10\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--learning_rate',\n",
    "        help = 'Learning rate for ADAM optimzer (default: 0.1)',\n",
    "        type = float,\n",
    "        default = 0.1\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--train_data_path',\n",
    "        help = 'GCS or local path to training data',\n",
    "        required = True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--train_steps',\n",
    "        help = 'Steps to run the training job for (default: 1000)',\n",
    "        type = int,\n",
    "        default = 1000\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--eval_data_path',\n",
    "        help = 'GCS or local path to evaluation data',\n",
    "        required = True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--output_dir',\n",
    "        help = 'GCS location to write checkpoints and export models',\n",
    "        required = True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--job-dir',\n",
    "        help='This is not used by our model, but it is required by gcloud',\n",
    "    )\n",
    "    args = parser.parse_args().__dict__\n",
    "    \n",
    "    # NEW: Append trial_id to path so trials don't overwrite each other\n",
    "    # This code can be removed if you are not using hyperparameter tuning\n",
    "    args['output_dir'] = os.path.join(\n",
    "        args['output_dir'],\n",
    "        json.loads(\n",
    "            os.environ.get('TF_CONFIG', '{}')\n",
    "        ).get('task', {}).get('trial', '')\n",
    "    ) \n",
    "    \n",
    "    model.NEMBEDS = args['nembeds'] # NEW: set global because NEMBEDS \n",
    "    \n",
    "    # Run the training job\n",
    "    model.train_and_evaluate(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create HyperTune configuration \n",
    "\n",
    "We specify:\n",
    "1. How many trials to run (`maxTrials`) and how many of those trials can be run in parrallel (`maxParallelTrials`) \n",
    "2. Which metric to optimize (`hyperparameterMetricTag`)\n",
    "3. The search region in which to constrain the hyperparameter search\n",
    "\n",
    "Full specification [here](https://cloud.google.com/ml-engine/reference/rest/v1/projects.jobs#HyperparameterSpec)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hyperparam.yaml\n"
     ]
    }
   ],
   "source": [
    "%writefile hyperparam.yaml\n",
    "trainingInput:\n",
    "  scaleTier: STANDARD_1\n",
    "  hyperparameters:\n",
    "    goal: MINIMIZE\n",
    "    maxTrials: 30\n",
    "    maxParallelTrials: 5\n",
    "    hyperparameterMetricTag: rmse\n",
    "    enableTrialEarlyStopping: True\n",
    "    params:\n",
    "    - parameterName: learning_rate\n",
    "      type: DOUBLE\n",
    "      minValue: .00001\n",
    "      maxValue: 1\n",
    "      scaleType: UNIT_LOG_SCALE\n",
    "    - parameterName: dropout\n",
    "      type: DOUBLE\n",
    "      minValue: 0.0\n",
    "      maxValue: 0.9\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: hidden_units_1\n",
    "      type: INTEGER\n",
    "      minValue: 10\n",
    "      maxValue: 512\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: hidden_units_2\n",
    "      type: INTEGER\n",
    "      minValue: 10\n",
    "      maxValue: 512\n",
    "      scaleType: UNIT_LINEAR_SCALE  \n",
    "    - parameterName: nembeds\n",
    "      type: INTEGER\n",
    "      minValue: 2\n",
    "      maxValue: 24\n",
    "      scaleType: UNIT_LINEAR_SCALE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Run the training job </h1>\n",
    "\n",
    "Same as before with the addition of `--config=hyperpam.yaml` to reference the file we just created.\n",
    "\n",
    "**This will take 1 hour.** Go to [cloud console](https://pantheon.corp.google.com/mlengine/jobs) and click on the job id. As trials are completed, the choosen hyperparameters and resulting objective value (RMSE in this case) will be shown. Trials will sorted from best to worst. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\n",
      "Job [taxifare_190107_232158] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs describe taxifare_190107_232158\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs stream-logs taxifare_190107_232158\n",
      "jobId: taxifare_190107_232158\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "OUTDIR='gs://{}/taxifare/trained_hp_tune'.format(BUCKET)\n",
    "!gsutil -m rm -rf {OUTDIR} # start fresh each time\n",
    "!gcloud ml-engine jobs submit training taxifare_$(date -u +%y%m%d_%H%M%S) \\\n",
    "   --package-path=taxifaremodel \\\n",
    "   --module-name=taxifaremodel.task \\\n",
    "   --config=hyperparam.yaml \\\n",
    "   --job-dir=gs://{BUCKET}/taxifare \\\n",
    "   --python-version=3.5 \\\n",
    "   --runtime-version={TFVERSION} \\\n",
    "   --region={REGION} \\\n",
    "   -- \\\n",
    "   --train_data_path=gs://{BUCKET}/taxifare/smallinput/taxi-train.csv \\\n",
    "   --eval_data_path=gs://{BUCKET}/taxifare/smallinput/taxi-valid.csv  \\\n",
    "   --train_steps=5000 \\\n",
    "   --output_dir={OUTDIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "- objectiveValue: 4.185\n",
    "- learning_rate: 0.029\n",
    "- dropout: \n",
    "- hidden_units_1: 26\n",
    "- hidden_units_2: 256\n",
    "- nembeds: 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our ideal hyperparameters let's run on our larger dataset to see if it helps. \n",
    "\n",
    "Note the passing of hyperparameter values via command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTDIR='gs://{}/taxifare/trained_large_tuned'.format(BUCKET)\n",
    "!gsutil -m rm -rf {OUTDIR} # start fresh each time\n",
    "!gcloud ml-engine jobs submit training taxifare_large_$(date -u +%y%m%d_%H%M%S) \\\n",
    "   --package-path=taxifaremodel \\\n",
    "   --module-name=taxifaremodel.task \\\n",
    "   --job-dir=gs://{BUCKET}/taxifare \\\n",
    "   --python-version=3.5 \\\n",
    "   --runtime-version=1.12 \\\n",
    "   --region={REGION} \\\n",
    "   --scale-tier=STANDARD_1 \\\n",
    "   -- \\\n",
    "   --train_data_path=gs://cloud-training-demos/taxifare/large/taxi-train*.csv \\\n",
    "   --eval_data_path=gs://cloud-training-demos/taxifare/small/taxi-valid.csv  \\\n",
    "   --train_steps=50000 \\\n",
    "   --output_dir={OUTDIR} \\\n",
    "   --learning_rate=?? \\\n",
    "   --dropout=?? \\\n",
    "   --hidden_units_1=?? \\\n",
    "   --hidden_units_2=?? \\\n",
    "   --nembeds=??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Our final RMSE, after feature engineering, hyperparameter tuning, and running on a large dataset is **??**. \n",
    "\n",
    "**Challenge Excercise**\n",
    "\n",
    "Try to beat this! Some tips:\n",
    "\n",
    "- We only used two features in our model, try adding more. \n",
    "- There are more hyperparameters we could tune, number of hidden layers for instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2016 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
