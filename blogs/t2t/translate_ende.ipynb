{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation using tensor2tensor on Cloud ML Engine\n",
    "\n",
    "This notebook illustrates using the <a href=\"https://github.com/tensorflow/tensor2tensor\">tensor2tensor</a> library to do from-scratch, distributed training of a English-German translator. Then, the trained model is deployed to Cloud ML Engine and used to translate new pieces of text.\n",
    "<p/>\n",
    "### Install tensor2tensor, and specify Google Cloud Platform project and bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "pip install tensor2tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT = 'cloud-training-demos' # REPLACE WITH YOUR PROJECT ID\n",
    "BUCKET = 'cloud-training-demos-ml' # REPLACE WITH YOUR BUCKET NAME\n",
    "REGION = 'us-central1' # REPLACE WITH YOUR BUCKET REGION e.g. us-central1\n",
    "\n",
    "# for bash\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "wget http://data.statmt.org/wmt17/translation-task/training-parallel-nc-v12.tgz\n",
    "wget http://data.statmt.org/wmt17/translation-task/dev.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "gsutil cp -m training-parallel-nc-v12.tgz dev.tgz gs://${BUCKET}/translate_ende/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a Problem\n",
    "The Problem in tensor2tensor is where you specify parameters like the size of your vocabulary and where to get the training data from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "rm -rf ende\n",
    "mkdir ende"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile ende/__init__.py\n",
    "from . import problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%writefile ende/problem.py\n",
    "import tensorflow as tf\n",
    "from tensor2tensor.data_generators import generator_utils\n",
    "from tensor2tensor.data_generators import problem\n",
    "from tensor2tensor.data_generators import text_encoder\n",
    "from tensor2tensor.data_generators import wsj_parsing\n",
    "import tensor2tensor.data_generators.wmt as wmt\n",
    "from tensor2tensor.utils import registry\n",
    "\n",
    "#TOPDIR=\"gs://{}/translate_ende/\".format(\"BUCKET_NAME\")\n",
    "TOPDIR=\"file:///content/training-data-analyst/blogs/t2t\"\n",
    "\n",
    "_ENDE_TRAIN_DATASETS = [\n",
    "    [\n",
    "        \"{}/training-parallel-nc-v12.tgz\".format(TOPDIR),\n",
    "        (\"training/news-commentary-v12.de-en.en\",\n",
    "         \"training/news-commentary-v12.de-en.de\")\n",
    "    ],\n",
    "]\n",
    "_ENDE_TEST_DATASETS = [\n",
    "    [\n",
    "        \"{}/dev.tgz\".format(TOPDIR),\n",
    "        (\"dev/newstest2013.en\", \"dev/newstest2013.de\")\n",
    "    ],\n",
    "]\n",
    "\n",
    "@registry.register_problem\n",
    "class MyTranslateProblem(wmt.TranslateProblem):\n",
    "  @property\n",
    "  def targeted_vocab_size(self):\n",
    "    return 2**13  # 8192\n",
    "\n",
    "  def generator(self, data_dir, tmp_dir, train):\n",
    "    symbolizer_vocab = generator_utils.get_or_generate_vocab(\n",
    "        data_dir, tmp_dir, self.vocab_file, self.targeted_vocab_size, sources=_ENDE_TRAIN_DATASETS)\n",
    "    datasets = _ENDE_TRAIN_DATASETS if train else _ENDE_TEST_DATASETS\n",
    "    tag = \"train\" if train else \"dev\"\n",
    "    data_path = wmt._compile_data(tmp_dir, datasets, \"wmt_ende_tok_%s\" % tag)\n",
    "    return wmt.token_generator(data_path + \".lang1\", data_path + \".lang2\",\n",
    "                           symbolizer_vocab, text_encoder.EOS_ID)\n",
    "\n",
    "  @property\n",
    "  def input_space_id(self):\n",
    "    return problem.SpaceID.EN_TOK\n",
    "\n",
    "  @property\n",
    "  def target_space_id(self):\n",
    "    return problem.SpaceID.DE_TOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "PROBLEM=my_translate_problem\n",
    "#PROBLEM=translate_ende_wmt8k\n",
    "DATA_DIR=./t2t_data\n",
    "TMP_DIR=$DATA_DIR/tmp\n",
    "rm -rf $DATA_DIR $TMP_DIR\n",
    "mkdir -p $DATA_DIR $TMP_DIR\n",
    "# Generate data\n",
    "t2t-datagen \\\n",
    "  --t2t_usr_dir=./ende \\\n",
    "  --problem=$PROBLEM \\\n",
    "  --data_dir=$DATA_DIR \\\n",
    "  --tmp_dir=$TMP_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
