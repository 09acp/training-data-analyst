{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation using tensor2tensor on Cloud ML Engine\n",
    "\n",
    "This notebook illustrates using the <a href=\"https://github.com/tensorflow/tensor2tensor\">tensor2tensor</a> library to do from-scratch, distributed training of a English-German translator. Then, the trained model is deployed to Cloud ML Engine and used to translate new pieces of text.\n",
    "<p/>\n",
    "### Install tensor2tensor, and specify Google Cloud Platform project and bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "pip install tensor2tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT = 'cloud-training-demos' # REPLACE WITH YOUR PROJECT ID\n",
    "BUCKET = 'cloud-training-demos-ml' # REPLACE WITH YOUR BUCKET NAME\n",
    "REGION = 'us-central1' # REPLACE WITH YOUR BUCKET REGION e.g. us-central1\n",
    "\n",
    "# for bash\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "wget http://data.statmt.org/wmt17/translation-task/training-parallel-nc-v12.tgz\n",
    "wget http://data.statmt.org/wmt17/translation-task/dev.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up problem\n",
    "The Problem in tensor2tensor is where you specify parameters like the size of your vocabulary and where to get the training data from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "rm -rf ende\n",
    "mkdir ende"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%writefile ende/problem.py\n",
    "import tensorflow as tf\n",
    "from tensor2tensor.data_generators import generator_utils\n",
    "from tensor2tensor.data_generators import problem\n",
    "from tensor2tensor.data_generators import text_encoder\n",
    "from tensor2tensor.data_generators import wsj_parsing\n",
    "import tensor2tensor.data_generators.wmt as wmt\n",
    "from tensor2tensor.utils import registry\n",
    "\n",
    "#TOPDIR=\"gs://{}/translate_ende/\".format(\"BUCKET_NAME\")\n",
    "TOPDIR=\"file:///content/training-data-analyst/blogs/t2t\"  # Make sure this matches the !pwd above\n",
    "\n",
    "_ENDE_TRAIN_DATASETS = [\n",
    "    [\n",
    "        \"{}/training-parallel-nc-v12.tgz\".format(TOPDIR),\n",
    "        (\"training/news-commentary-v12.de-en.en\",\n",
    "         \"training/news-commentary-v12.de-en.de\")\n",
    "    ],\n",
    "]\n",
    "_ENDE_TEST_DATASETS = [\n",
    "    [\n",
    "        \"{}/dev.tgz\".format(TOPDIR),\n",
    "        (\"dev/newstest2013.en\", \"dev/newstest2013.de\")\n",
    "    ],\n",
    "]\n",
    "\n",
    "@registry.register_problem\n",
    "class MyTranslateProblem(wmt.TranslateProblem):\n",
    "  @property\n",
    "  def targeted_vocab_size(self):\n",
    "    return 2**13  # 8192\n",
    "\n",
    "  def generator(self, data_dir, tmp_dir, train):\n",
    "    symbolizer_vocab = generator_utils.get_or_generate_vocab(\n",
    "        data_dir, tmp_dir, self.vocab_file, self.targeted_vocab_size, sources=_ENDE_TRAIN_DATASETS)\n",
    "    datasets = _ENDE_TRAIN_DATASETS if train else _ENDE_TEST_DATASETS\n",
    "    tag = \"train\" if train else \"dev\"\n",
    "    data_path = wmt._compile_data(tmp_dir, datasets, \"wmt_ende_tok_%s\" % tag)\n",
    "    return wmt.token_generator(data_path + \".lang1\", data_path + \".lang2\",\n",
    "                           symbolizer_vocab, text_encoder.EOS_ID)\n",
    "\n",
    "  @property\n",
    "  def input_space_id(self):\n",
    "    return problem.SpaceID.EN_TOK\n",
    "\n",
    "  @property\n",
    "  def target_space_id(self):\n",
    "    return problem.SpaceID.DE_TOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile ende/__init__.py\n",
    "from . import problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ende/setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ende/setup.py\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "REQUIRED_PACKAGES = [\n",
    "]\n",
    "\n",
    "setup(\n",
    "    name='ende',\n",
    "    version='0.1',\n",
    "    author = 'Google',\n",
    "    author_email = 'training-feedback@cloud.google.com',\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='My Translate Problem',\n",
    "    requires=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate training data \n",
    "\n",
    "Our problem (translation) requires the creation of text sequences from the training dataset.  This is done using t2t-datagen and the Problem defined in the previous section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "PROBLEM=my_translate_problem\n",
    "#PROBLEM=translate_ende_wmt8k\n",
    "DATA_DIR=./t2t_data\n",
    "TMP_DIR=$DATA_DIR/tmp\n",
    "rm -rf $DATA_DIR $TMP_DIR\n",
    "mkdir -p $DATA_DIR $TMP_DIR\n",
    "# Generate data\n",
    "t2t-datagen \\\n",
    "  --t2t_usr_dir=./ende \\\n",
    "  --problem=$PROBLEM \\\n",
    "  --data_dir=$DATA_DIR \\\n",
    "  --tmp_dir=$TMP_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide Cloud ML Engine access to data\n",
    "\n",
    "Copy the data to Google Cloud Storage, and then provide access to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "DATA_DIR=./t2t_data\n",
    "gsutil -m rm -r gs://${BUCKET}/translate_ende/\n",
    "gsutil -m cp ${DATA_DIR}/my_translate_problem* gs://${BUCKET}/translate_ende/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "PROJECT_ID=$PROJECT\n",
    "AUTH_TOKEN=$(gcloud auth print-access-token)\n",
    "SVC_ACCOUNT=$(curl -X GET -H \"Content-Type: application/json\" \\\n",
    "    -H \"Authorization: Bearer $AUTH_TOKEN\" \\\n",
    "    https://ml.googleapis.com/v1/projects/${PROJECT_ID}:getConfig \\\n",
    "    | python -c \"import json; import sys; response = json.load(sys.stdin); \\\n",
    "    print response['serviceAccount']\")\n",
    "\n",
    "echo \"Authorizing the Cloud ML Service account $SVC_ACCOUNT to access files in $BUCKET\"\n",
    "gsutil -m defacl ch -u $SVC_ACCOUNT:R gs://$BUCKET\n",
    "gsutil -m acl ch -u $SVC_ACCOUNT:R -r gs://$BUCKET  # error message (if bucket is empty) can be ignored\n",
    "gsutil -m acl ch -u $SVC_ACCOUNT:W gs://$BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model as a Python package\n",
    "\n",
    "To submit the training job to Cloud Machine Learning Engine, we need a Python module with a main(). We'll use the t2t-trainer that is distributed with tensor2tensor as the main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "wget https://raw.githubusercontent.com/tensorflow/tensor2tensor/4ffae909f97c05e9dcef4bedf7f403bb62fde7b8/tensor2tensor/bin/t2t-trainer\n",
    "mv t2t-trainer ende/t2t-trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__.py  __init__.pyc  problem.py  problem.pyc  setup.py  t2t-trainer.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls ende"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "gsutil -m cp gs://${BUCKET}/translate_ende/data/my_translate_problem-train-0008*   gs://${BUCKET}/translate_ende/subset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:uid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO:tensorflow:Creating experiment, storing model files in ./trained_model\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n",
      "    \"__main__\", fname, loader, pkg_name)\n",
      "  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n",
      "    exec code in run_globals\n",
      "  File \"/content/training-data-analyst/blogs/t2t/ende/t2t-trainer.py\", line 94, in <module>\n",
      "    tf.app.run()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 48, in run\n",
      "    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\n",
      "  File \"/content/training-data-analyst/blogs/t2t/ende/t2t-trainer.py\", line 90, in main\n",
      "    schedule=FLAGS.schedule)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensor2tensor/utils/trainer_utils.py\", line 351, in run\n",
      "    hparams=hparams)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/learn_runner.py\", line 193, in run\n",
      "    experiment = wrapped_experiment_fn(run_config=run_config, hparams=hparams)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/learn_runner.py\", line 79, in wrapped_experiment_fn\n",
      "    experiment = experiment_fn(run_config, hparams)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensor2tensor/utils/trainer_utils.py\", line 122, in experiment_fn\n",
      "    run_config=run_config)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensor2tensor/utils/trainer_utils.py\", line 134, in create_experiment\n",
      "    run_config=run_config)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensor2tensor/utils/trainer_utils.py\", line 182, in create_experiment_components\n",
      "    hparams = add_problem_hparams(hparams, FLAGS.problems)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensor2tensor/utils/trainer_utils.py\", line 243, in add_problem_hparams\n",
      "    p_hparams = problem.get_hparams(hparams)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensor2tensor/data_generators/problem.py\", line 262, in get_hparams\n",
      "    self.get_feature_encoders(data_dir)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensor2tensor/data_generators/problem.py\", line 252, in get_feature_encoders\n",
      "    self._encoders = self.feature_encoders(data_dir)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensor2tensor/data_generators/problem.py\", line 601, in feature_encoders\n",
      "    encoder = text_encoder.SubwordTextEncoder(vocab_filename)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensor2tensor/data_generators/text_encoder.py\", line 393, in __init__\n",
      "    self._load_from_file(filename)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensor2tensor/data_generators/text_encoder.py\", line 703, in _load_from_file\n",
      "    self._load_from_file_object(f)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensor2tensor/data_generators/text_encoder.py\", line 693, in _load_from_file_object\n",
      "    self._init_subtokens_from_list(subtoken_strings)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensor2tensor/data_generators/text_encoder.py\", line 666, in _init_subtokens_from_list\n",
      "    self._max_subtoken_len = max([len(s) for s in subtoken_strings])\n",
      "ValueError: max() arg is an empty sequence\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "OUTDIR=./trained_model\n",
    "rm -rf $OUTDIR\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/ende\n",
    "python -m ende.t2t-trainer \\\n",
    "  --data_dir=gs://${BUCKET}/translate_ende/subset \\\n",
    "  --problems=my_translate_problem \\\n",
    "  --model=transformer \\\n",
    "  --hparams_set=transformer_base_single_gpu \\\n",
    "  --output_dir=$OUTDIR --job-dir=./tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on Cloud ML Engine\n",
    "\n",
    "Once we have a working Python package, training on a Cloud ML Engine GPU is straightforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "OUTDIR=gs://${BUCKET}/translate_ende/model\n",
    "JOBNAME=t2t_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "   --region=$REGION \\\n",
    "   --staging-bucket=gs://$BUCKET \\\n",
    "   --scale-tier=BASIC_GPU \\\n",
    "   --module-name=ende.t2t-trainer \\\n",
    "   --package-path=${PWD}/ende \\\n",
    "   --job-dir=$OUTDIR \\\n",
    "   -- \\\n",
    "  --data_dir=gs://${BUCKET}/translate_ende/data \\\n",
    "  --problems=my_translate_problem \\\n",
    "  --model=transformer \\\n",
    "  --hparams_set=transformer_base_single_gpu \\\n",
    "  --output_dir=$OUTDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
