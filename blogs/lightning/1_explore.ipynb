{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring lightning prediction setup\n",
    "\n",
    "This notebook gets a set of grids and plots them to make sure we are pulling out training data correctly.\n",
    "Read the [design doc](https://docs.google.com/document/d/1wmJN6G2f74aOZrqm2FJEkoeIQ_bvPz58FdAZr5SqSOk/edit) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source activate py2env\n",
    "conda install -y pytz\n",
    "pip uninstall -y google-cloud-dataflow\n",
    "pip install --upgrade retrying snappy pyresample netcdf4  google-cloud-storage apache-beam[gcp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing the pip install, Reset Session before continuing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out the local module\n",
    "\n",
    "Make sure the local module works. This might take a couple of minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source activate py2env\n",
    "python -m ltgpred.goesutil.goesio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Infrared data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import ltgpred.goesutil.goesio as goesio\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latlonres = 0.02\n",
    "griddef = goesio.create_conus_griddef(latlonres)\n",
    "irblob = goesio.get_ir_blob_paths(2018, 134, 20)[0] # top of the hour\n",
    "ref = goesio.read_ir_data(irblob, griddef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ref);\n",
    "print(ref[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read lightning data\n",
    "Lightning data is actually just event observations every 20s. So we have to accumulate in a grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightning blobs for 15 minutes as of the IR image\n",
    "influence_km = 5\n",
    "irdt = goesio.get_timestamp_from_filename(irblob)\n",
    "ltg_blobs = goesio.get_ltg_blob_paths(irdt, timespan_minutes=15)\n",
    "ltg = goesio.create_ltg_grid(ltg_blobs, griddef, influence_km)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ltg);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship between IR and lightning\n",
    "\n",
    "Mean lightning as the reflectivity threshold is increased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(name, data):\n",
    "  print('{}: shape={}, mean={}, min={}, max={} count={}'.format(name, data.shape, np.mean(data), np.min(data), np.max(data), np.sum(~np.isnan(data))))\n",
    "\n",
    "print_stats('ref', ref)\n",
    "print_stats('ltg for all pixels', ltg)\n",
    "print_stats('ref for ltg pixels', ref[ltg > 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average reflectivity associated with all pixels is 0.43 while the reflectivity associated with lightning pixels is 0.72."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for thresh in np.arange(0, 1, 0.1):\n",
    "  print_stats('ltg where ref > {}'.format(thresh), ltg[ref > thresh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(griddef.lats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we get to a reflectivity threshold of 0.9, then 57% of the pixels are associated with lightning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training examples from IR and lightning\n",
    "\n",
    "See if there is signal by doing a bit of feature engineering on the ref array and then using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 32\n",
    "strideN = N\n",
    "halfN = N//2\n",
    "N15 = N + halfN\n",
    "\n",
    "def get_prediction_grid_centers(ref): \n",
    "  cy, cx = np.meshgrid(np.arange(N15, ref.shape[0]-N15, strideN),\n",
    "                       np.arange(N15, ref.shape[1]-N15, strideN))\n",
    "  cy = cy.ravel()\n",
    "  cx = cx.ravel()\n",
    "  return zip(cy, cx)\n",
    "\n",
    "def rawdata_input_fn(ref, ltg, griddef):\n",
    "  for cy, cx in get_prediction_grid_centers(ref):\n",
    "    # restrict to grids where there is lightning in the area\n",
    "    interesting = np.sum(ltg[cy-N15:cy+N15, cx-N15:cx+N15]) > 0.5\n",
    "    if interesting:\n",
    "      label = np.sum(ltg[cy-halfN:cy+halfN, cx-halfN:cx+halfN]) > 0.5\n",
    "      example = {\n",
    "          'lon': griddef.lons[cy][cx],\n",
    "          'lat': griddef.lats[cy][cx], \n",
    "          'ref_smallbox': ref[cy-halfN:cy+halfN, cx-halfN:cx+halfN],\n",
    "          'ref_bigbox': ref[cy-N15:cy+N15, cx-N15:cx+N15],\n",
    "          'has_ltg': label #1.0 if label else 0.0\n",
    "      }\n",
    "      yield example\n",
    "\n",
    "def create_prediction_df(ref, ltg, griddef):\n",
    "  data = []\n",
    "  for example in rawdata_input_fn(ref, ltg, griddef):\n",
    "    data.append([example['lat'],\n",
    "                 example['lon'],\n",
    "                 np.mean(example['ref_smallbox']), # mean within subgrid\n",
    "                 np.max(example['ref_smallbox']),\n",
    "                 np.mean(example['ref_bigbox']),\n",
    "                 np.max(example['ref_bigbox']),\n",
    "                 example['has_ltg']\n",
    "                ])\n",
    "\n",
    "  import pandas as pd\n",
    "  df = pd.DataFrame(data, columns=['lat', 'lon', 'meanref_smallbox', 'maxref_smallbox', 'meanref_bigbox', 'maxref_bigbox', 'ltg'])\n",
    "  print('For lightning grids {}'.format(df[df['ltg']].describe()))\n",
    "  print('For no-lightning grids {}'.format(df[~df['ltg']].describe()))\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "model = LogisticRegression()\n",
    "df = create_prediction_df(ref, ltg, griddef)\n",
    "x = df.drop(['lat', 'lon', 'ltg'], axis=1)\n",
    "model = model.fit(x, df['ltg'] > 0.5)\n",
    "print(model.coef_, model.intercept_)\n",
    "print('Model accuracy={}%'.format(100*model.score(x, df['ltg'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can detect lightning reasonably well based on just the IR temperature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting lightning\n",
    "\n",
    "How about if we try to predict lightning 30 minutes into the future?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "ltg30_blobs = goesio.get_ltg_blobs(irdt + timedelta(minutes=30), timespan_minutes=15)\n",
    "ltg30 = goesio.create_ltg_grid(ltg30_blobs, griddef, influence_km)\n",
    "\n",
    "model = LogisticRegression()\n",
    "df = create_prediction_df(ref, ltg30, griddef)\n",
    "x = df.drop(['lat', 'lon', 'ltg'], axis=1)\n",
    "model = model.fit(x, df['ltg'])\n",
    "print(model.coef_, model.intercept_)\n",
    "print('Model accuracy={}%'.format(100*model.score(x, df['ltg'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad at all!  We got accuracy of 74% predicting lightning 30 minutes into the future. Of course, this is training & validation on the same hour of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the prediction\n",
    "\n",
    "The prediction grid has a resolution that is N times coarser than the original because we were predicting only at the centers (we could always do it at a finer resolution though, by reducing strideN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted'] = model.predict_proba(x)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyresample as pr\n",
    "swath_def = pr.geometry.SwathDefinition(lats=df['lat'], lons=df['lon'])\n",
    "ltgpred = pr.kd_tree.resample_nearest(swath_def, df['predicted'].values, griddef, radius_of_influence=1000*latlonres*100*N, epsilon=0.5)\n",
    "plt.imshow(ltgpred);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam pipeline to create training dataset\n",
    "\n",
    "The above was on just one hour of data.  Here's the start of a Beam pipeline that will generate a list of training and evaluation dataset hours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "def generate_hours(starthour, endhour, startday, endday, startyear, endyear, is_train):\n",
    "  for h in xrange(starthour, endhour+1):\n",
    "    for d in xrange(startday, endday+1):\n",
    "      for y in xrange(startyear, endyear+1):\n",
    "        data = {\n",
    "          'hour': h,\n",
    "          'day': d,\n",
    "          'year': y\n",
    "        }\n",
    "        if hash(str(data)) % 10 < 7:\n",
    "          if is_train:\n",
    "            yield data\n",
    "        else:\n",
    "          if not is_train:\n",
    "            yield data\n",
    "\n",
    "OUTPUT_DIR = './preproc'\n",
    "RUNNER = 'DirectRunner'\n",
    "options = {\n",
    "      'staging_location': os.path.join(OUTPUT_DIR, 'tmp', 'staging'),\n",
    "      'temp_location': os.path.join(OUTPUT_DIR, 'tmp'),\n",
    "      'job_name': 'hello',\n",
    "      'project': 'cloud-training-demos',\n",
    "      'teardown_policy': 'TEARDOWN_ALWAYS',\n",
    "      'no_save_main_session': True\n",
    "}\n",
    "opts = beam.pipeline.PipelineOptions(flags = [], **options)\n",
    "shutil.rmtree(OUTPUT_DIR, ignore_errors=True)\n",
    "p = beam.Pipeline(RUNNER, options=opts)\n",
    "\n",
    "for step in ['train', 'eval']:\n",
    "  (p \n",
    "   | '{}_hours'.format(step) >> beam.Create(generate_hours(12, 14, 183, 184, 2018, 2018, step == 'train'))\n",
    "   | '{}_write'.format(step) >> beam.io.Write(beam.io.WriteToText(os.path.join(OUTPUT_DIR, step)))\n",
    "  )\n",
    "\n",
    "job = p.run()\n",
    "job.wait_until_finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head preproc/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's invoke the above functions, and put it into our Python module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "source activate py2env\n",
    "python -m ltgpred.preproc.create_dataset \\\n",
    "   --outdir=./preproc \\\n",
    "   --startday 183 --endday 183  --starthour 13  --endhour 13 --lightning_validity 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More beam trials ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import datetime\n",
    "[1, 2, 3] | beam.Map(lambda x : datetime.datetime(2018,x,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import numpy as np\n",
    "\n",
    "class MeanStddev(beam.CombineFn):\n",
    "  def create_accumulator(self):\n",
    "    return (0.0, 0.0, 0) # x, x^2, count\n",
    "\n",
    "  def add_input(self, sum_count, input):\n",
    "    (sum, sumsq, count) = sum_count\n",
    "    return sum + input, sumsq + input*input, count + 1\n",
    "\n",
    "  def merge_accumulators(self, accumulators):\n",
    "    sums, sumsqs, counts = zip(*accumulators)\n",
    "    return sum(sums), sum(sumsqs), sum(counts)\n",
    "\n",
    "  def extract_output(self, sum_count):\n",
    "    (sum, sumsq, count) = sum_count\n",
    "    if count:\n",
    "      mean = sum / count\n",
    "      variance = (sumsq / count) - mean*mean\n",
    "      # -ve value could happen due to rounding\n",
    "      stddev = np.sqrt(variance) if variance > 0 else 0\n",
    "      return {\n",
    "        'mean': mean,\n",
    "        'variance': variance,\n",
    "        'stddev': stddev,\n",
    "        'count': count\n",
    "      }\n",
    "    else:\n",
    "      return {\n",
    "        'mean': float('NaN'),\n",
    "        'variance': float('NaN'),\n",
    "        'stddev': float('NaN'),\n",
    "        'count': 0\n",
    "      }\n",
    "    \n",
    "    \n",
    "#[1.3, 3.0, 4.2] | beam.CombineGlobally(MeanStddev())\n",
    "\n",
    "#[('a', 1.3), ('a', 3.0), ('b', 4.2)] | beam.CombinePerKey(MeanStddev())\n",
    "\n",
    "[\n",
    "  {'a': 1.3, 'b': 2.3, 'c': 4.5},\n",
    "  {'a': 2.3, 'b': 3.3, 'c': 5.4},  \n",
    "] | beam.FlatMap(lambda x : [(f, x[f]) for f in ['a', 'b']]) | beam.CombinePerKey(MeanStddev())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "gsutil cat gs://cloud-training-demos-ml/lightning/preproc/stats/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = np.ma.array([[0.3, 10, 0.8], [-2.8, np.nan, np.inf]], \n",
    "                    mask=[[False, True, False], [False, False, False]])\n",
    "print(value)\n",
    "x = np.ma.filled(value.flatten(), np.nan)\n",
    "print(x)\n",
    "x = np.nan_to_num(x)\n",
    "print(x)\n",
    "x = np.clip(x, -1, 1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Beam pipeline at scale\n",
    "\n",
    "Run on all available days in 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source activate py2env\n",
    "\n",
    "PROJECT='cloud-training-demos'\n",
    "BUCKET='cloud-training-demos-ml'\n",
    "LATLONRES=0.02\n",
    "TRAIN_RADIUS=32\n",
    "LABEL_RADIUS=2\n",
    "STRIDE=4 # use 2*label_patch_radius\n",
    "OUTDIR=gs://${BUCKET}/lightning/preproc_${LATLONRES}_${TRAIN_RADIUS}_${LABEL_RADIUS}\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "\n",
    "python -m ltgpred.preproc.create_dataset \\\n",
    "   --outdir=$OUTDIR \\\n",
    "   --startyear 2018 --endyear 2018 --startday 45 --endday 350 --project=$PROJECT \\\n",
    "   --train_patch_radius=$TRAIN_RADIUS --label_patch_radius=$LABEL_RADIUS --stride=$STRIDE --latlonres=$LATLONRES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "* Instead of using just 4 features of the IR grid (mean, max of two rectangular subgrids), use all the pixels\n",
    "* Train models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2017 Google Inc. Licensed under the Apache License, Version 2.0 (the \\\"License\\\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \\\"AS IS\\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
