{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xfcSMZEIFA_l"
   },
   "source": [
    "# ML with TensorFlow Extended (TFX) -- Part 2\n",
    "The puprpose of this tutorial is to show how to do end-to-end ML with TFX libraries on Google Cloud Platform. This tutorial covers:\n",
    "1. Data analysis and schema generation with **TF Data Validation**.\n",
    "2. Data preprocessing with **TF Transform**.\n",
    "3. Model training with **TF Estimator**.\n",
    "4. Model evaluation with **TF Model Analysis**.\n",
    "\n",
    "This notebook has been tested in Jupyter on the Deep Learning VM.\n",
    "\n",
    "## 0. Setup Python and Cloud environment\n",
    "\n",
    "Apache Beam support for Python 3 is in alpha at the moment, so install from source to get latest bug fixes in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade grpcio_tools tensorflow_data_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "git clone https://github.com/apache/beam\n",
    "cd beam/sdks/python\n",
    "python3 setup.py sdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q --upgrade './beam/sdks/python/dist/apache-beam-2.13.0.dev0.tar.gz[gcp]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tornado version: 5.1.1\n",
      "Python version: 3.5.3\n",
      "TF version: 1.13.1\n",
      "TFT version: 0.13.0\n",
      "TFDV version: 0.13.1\n",
      "Apache Beam version: 2.13.0.dev\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "import platform\n",
    "import tensorflow as tf\n",
    "import tensorflow_data_validation as tfdv\n",
    "import tensorflow_transform as tft\n",
    "import tornado\n",
    "\n",
    "print('tornado version: {}'.format(tornado.version))\n",
    "print('Python version: {}'.format(platform.python_version()))\n",
    "print('TF version: {}'.format(tf.__version__))\n",
    "print('TFT version: {}'.format(tft.__version__))\n",
    "print('TFDV version: {}'.format(tfdv.__version__))\n",
    "print('Apache Beam version: {}'.format(beam.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'cloud-training-demos'    # Replace with your PROJECT\n",
    "BUCKET = 'cloud-training-demos-ml'  # Replace with your BUCKET\n",
    "REGION = 'us-central1'              # Choose an available region for Cloud MLE\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION\n",
    "\n",
    "## ensure we're using python2 env\n",
    "os.environ['CLOUDSDK_PYTHON'] = 'python2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n",
      "Updated property [ml_engine/local_python].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION\n",
    "\n",
    "## ensure we predict locally with our current Python environment\n",
    "gcloud config set ml_engine/local_python `which python`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img valign=\"middle\" src=\"images/tfx.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l9u699PmHJXU"
   },
   "source": [
    "### UCI Adult Dataset: https://archive.ics.uci.edu/ml/datasets/adult\n",
    "Predict whether income exceeds $50K/yr based on census data. Also known as \"Census Income\" dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR='gs://cloud-samples-data/ml-engine/census/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ksuSTsysHfZV",
    "outputId": "87adfbf0-be77-4d81-9162-5a2f9feffd90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   3974304  2018-05-10T22:21:14Z  gs://cloud-samples-data/ml-engine/census/data/adult.data.csv\n",
      "TOTAL: 1 objects, 3974304 bytes (3.79 MiB)\n",
      "   1986465  2018-05-10T22:21:14Z  gs://cloud-samples-data/ml-engine/census/data/adult.test.csv\n",
      "TOTAL: 1 objects, 1986465 bytes (1.89 MiB)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "TRAIN_DATA_FILE = os.path.join(DATA_DIR, 'adult.data.csv')\n",
    "EVAL_DATA_FILE = os.path.join(DATA_DIR, 'adult.test.csv')\n",
    "!gsutil ls -l $TRAIN_DATA_FILE\n",
    "!gsutil ls -l $EVAL_DATA_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADER = ['age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
    "               'marital_status', 'occupation', 'relationship', 'race', 'gender',\n",
    "               'capital_gain', 'capital_loss', 'hours_per_week',\n",
    "               'native_country', 'income_bracket']\n",
    "\n",
    "TARGET_FEATURE_NAME = 'income_bracket'\n",
    "TARGET_LABELS = [' <=50K', ' >50K']\n",
    "WEIGHT_COLUMN_NAME = 'fnlwgt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kzEuipT4G0JE"
   },
   "source": [
    "## 2. Data Preprocessing\n",
    "For data preprocessing and transformation, we use [TensorFlow Transform](https://www.tensorflow.org/tfx/guide/tft) to perform the following:\n",
    "1. Implement transformation logic in **preprocess_fn**\n",
    "2. **Analyze and transform** training data.\n",
    "4. **Transform** evaluation data.\n",
    "5. Save transformed **data**, transform **schema**, and trasform **logic**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uzEWJDIbRcGQ"
   },
   "source": [
    "### 2.1 Implement preprocess_fn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I6svvdw3RhG2"
   },
   "outputs": [],
   "source": [
    "def make_preprocessing_fn(raw_schema):\n",
    "\n",
    "  def preprocessing_fn(input_features):\n",
    "\n",
    "    processed_features = {}\n",
    "\n",
    "    for feature in raw_schema.feature:\n",
    "      feature_name = feature.name\n",
    "      \n",
    "      if feature_name in ['income_bracket', 'fnlwgt']:\n",
    "        processed_features[feature_name] = input_features[feature_name]\n",
    "        continue\n",
    "\n",
    "      if feature.type == 1:\n",
    "        # Extract vocabulary and integerize categorical features.\n",
    "        processed_features[feature_name+\"_integerized\"] = tft.compute_and_apply_vocabulary(input_features[feature_name], vocab_filename=feature_name)\n",
    "      else:\n",
    "        # normalize numeric features.\n",
    "        processed_features[feature_name+\"_scaled\"] = tft.scale_to_z_score(input_features[feature_name])\n",
    "\n",
    "    # Bucketize age using quantiles. \n",
    "    quantiles = tft.quantiles(input_features[\"age\"], num_buckets=5, epsilon=0.01)\n",
    "    processed_features[\"age_bucketized\"] = tft.apply_buckets(\n",
    "      input_features[\"age\"], bucket_boundaries=quantiles)\n",
    "\n",
    "    return processed_features\n",
    "\n",
    "  return preprocessing_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4rMiQew8Z8V7"
   },
   "source": [
    "### 2.2 Implement the Beam pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YSjlcdLNZ7FX"
   },
   "outputs": [],
   "source": [
    "def run_pipeline(args):\n",
    "  import tensorflow_transform as tft\n",
    "  import tensorflow_transform.beam as tft_beam\n",
    "  import tensorflow_data_validation as tfdv\n",
    "  from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "  from tensorflow_transform.tf_metadata import dataset_schema\n",
    "  from tensorflow_transform.tf_metadata import schema_utils\n",
    "    \n",
    "  pipeline_options = beam.pipeline.PipelineOptions(flags=[], **args)\n",
    "    \n",
    "  raw_schema_location = args['raw_schema_location']\n",
    "  raw_train_data_location = args['raw_train_data_location']\n",
    "  raw_eval_data_location = args['raw_eval_data_location']\n",
    "  transformed_train_data_location = args['transformed_train_data_location']\n",
    "  transformed_eval_data_location = args['transformed_eval_data_location']\n",
    "  transform_artefact_location = args['transform_artefact_location']\n",
    "  temporary_dir = args['temporary_dir']\n",
    "  runner = args['runner']\n",
    "    \n",
    "  print (\"Raw schema location: {}\".format(raw_schema_location))\n",
    "  print (\"Raw train data location: {}\".format(raw_train_data_location))\n",
    "  print (\"Raw evaluation data location: {}\".format(raw_eval_data_location))\n",
    "  print (\"Transformed train data location: {}\".format(transformed_train_data_location))\n",
    "  print (\"Transformed evaluation data location: {}\".format(transformed_eval_data_location))\n",
    "  print (\"Transform artefact location: {}\".format(transform_artefact_location))\n",
    "  print (\"Temporary directory: {}\".format(temporary_dir))\n",
    "  print (\"Runner: {}\".format(runner))\n",
    "  print (\"\")\n",
    "\n",
    "  # Load TFDV schema and create tft schema from it.\n",
    "  source_raw_schema = tfdv.load_schema_text(raw_schema_location)\n",
    "  raw_feature_spec = schema_utils.schema_as_feature_spec(source_raw_schema).feature_spec\n",
    "  raw_metadata = dataset_metadata.DatasetMetadata(\n",
    "    dataset_schema.from_feature_spec(raw_feature_spec))\n",
    "\n",
    "  with beam.Pipeline(runner, options=pipeline_options) as pipeline:\n",
    "    with tft_beam.Context(temporary_dir):\n",
    "      \n",
    "      converter = tft.coders.CsvCoder(column_names=HEADER, \n",
    "        schema=raw_metadata.schema)\n",
    "\n",
    "      ###### analyze & transform trainining data ###############################\n",
    "\n",
    "      # Read raw training csv data.\n",
    "      step = 'Train'\n",
    "      print (\"Reading and parsing raw training data...\")\n",
    "      raw_train_data = (\n",
    "        pipeline\n",
    "          | '{} - Read Raw Data'.format(step) >> beam.io.textio.ReadFromText(raw_train_data_location)\n",
    "          | '{} - Remove Empty Rows'.format(step) >> beam.Filter(lambda line: line)\n",
    "          | '{} - Decode CSV Data'.format(step) >> beam.Map(converter.decode)\n",
    "        )\n",
    "      \n",
    "      # Create a train dataset from the data and schema.\n",
    "      raw_train_dataset = (raw_train_data, raw_metadata)\n",
    "\n",
    "      # Analyze and transform raw_train_dataset to produced transformed_train_dataset and transform_fn.\n",
    "      print (\"Analyzing and transforming raw training data...\")\n",
    "      transformed_train_dataset, transform_fn = (\n",
    "        raw_train_dataset \n",
    "        | '{} - Analyze & Transform'.format(step) >> tft_beam.AnalyzeAndTransformDataset(\n",
    "              make_preprocessing_fn(source_raw_schema))\n",
    "      )\n",
    "  \n",
    "      # Get data and schema separately from the transformed_train_dataset.\n",
    "      transformed_train_data, transformed_metadata = transformed_train_dataset\n",
    "\n",
    "      # write transformed train data to sink.\n",
    "      print (\"Writing transformed training data...\")\n",
    "      _ = (\n",
    "        transformed_train_data \n",
    "          | '{} - Write Transformed Data'.format(step) >> beam.io.tfrecordio.WriteToTFRecord(\n",
    "            file_path_prefix=transformed_train_data_location,\n",
    "            file_name_suffix=\".tfrecords\",\n",
    "            coder=tft.coders.ExampleProtoCoder(transformed_metadata.schema))\n",
    "        )\n",
    "\n",
    "      ###### transform evaluation data #########################################\n",
    "\n",
    "      # Read raw training csv data.\n",
    "      step = 'Eval'\n",
    "      print (\"Reading and parsing raw evaluation data...\")\n",
    "      raw_eval_data = (\n",
    "        pipeline\n",
    "          | '{} - Read Raw Data'.format(step) >> beam.io.textio.ReadFromText(raw_eval_data_location)\n",
    "          | '{} - Remove Empty Rows'.format(step) >> beam.Filter(lambda line: line)\n",
    "          | '{} - Decode CSV Data'.format(step) >> beam.Map(converter.decode)\n",
    "        )\n",
    "      \n",
    "      # Create a eval dataset from the data and schema.\n",
    "      raw_eval_dataset = (raw_eval_data, raw_metadata)\n",
    "\n",
    "      # Transform eval data based on produced transform_fn.\n",
    "      print (\"Transforming raw evaluation data...\")\n",
    "      transformed_eval_dataset = (\n",
    "        (raw_eval_dataset, transform_fn) \n",
    "          | '{} - Transform'.format(step) >> tft_beam.TransformDataset()\n",
    "      )\n",
    "\n",
    "      # Get data from the transformed_eval_dataset.\n",
    "      transformed_eval_data, _ = transformed_eval_dataset\n",
    "\n",
    "      # Write transformed eval data to sink.\n",
    "      print (\"Writing transformed evaluation data...\")\n",
    "      _ = (\n",
    "          transformed_eval_data \n",
    "          | '{} - Write Transformed Data'.format(step) >> beam.io.tfrecordio.WriteToTFRecord(\n",
    "              file_path_prefix=transformed_eval_data_location,\n",
    "              file_name_suffix=\".tfrecords\",\n",
    "              coder=tft.coders.ExampleProtoCoder(transformed_metadata.schema))\n",
    "        )\n",
    "\n",
    "      ###### write transformation metadata #######################################################\n",
    "\n",
    "      # Write transform_fn.\n",
    "      print (\"Writing transform artefacts...\")\n",
    "      _ = (\n",
    "          transform_fn \n",
    "          | 'Write Transform Artefacts' >> tft_beam.WriteTransformFn(\n",
    "              transform_artefact_location)\n",
    "      )\n",
    "\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2QQzYPXGhqwP"
   },
   "source": [
    "### 1.4 Run data tranformation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mesh-tensorflow==0.0.5\n",
      "tensorflow==1.13.1\n",
      "tensorflow-data-validation==0.13.1\n",
      "tensorflow-datasets==1.0.1\n",
      "tensorflow-estimator==1.13.0\n",
      "tensorflow-metadata==0.13.0\n",
      "tensorflow-probability==0.6.0\n",
      "tensorflow-serving-api==1.13.0rc1\n",
      "tensorflow-transform==0.13.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip freeze | grep tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile setup.py\n",
    "from setuptools import setup, find_packages\n",
    "\n",
    "setup(name='tfxdemo',\n",
    "      version='1.0',\n",
    "      packages=find_packages(),\n",
    "      install_requires=['tensorflow-transform==0.13.0', \n",
    "                        'tensorflow-data-validation==0.13.1'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Snd1a6_yhq9H"
   },
   "outputs": [],
   "source": [
    "#runner = 'DirectRunner'; OUTPUT_DIR = 'output'   # on-prem\n",
    "runner = 'DataflowRunner'; OUTPUT_DIR = 'gs://{}/census/tfx/'.format(BUCKET)  # on GCP\n",
    "\n",
    "RAW_SCHEMA_LOCATION = 'raw_schema.pbtxt'\n",
    "TRANSFORM_ARTEFACTS_DIR = os.path.join(OUTPUT_DIR,'transform')\n",
    "TRANSFORMED_DATA_DIR = os.path.join(OUTPUT_DIR,'transformed')\n",
    "TEMP_DIR = os.path.join(OUTPUT_DIR, 'tmp')\n",
    "\n",
    "args = {\n",
    "    \n",
    "    'runner': runner,\n",
    "\n",
    "    'raw_schema_location': RAW_SCHEMA_LOCATION,\n",
    "\n",
    "    'raw_train_data_location': TRAIN_DATA_FILE,\n",
    "    'raw_eval_data_location': EVAL_DATA_FILE,\n",
    "\n",
    "    'transformed_train_data_location':  os.path.join(TRANSFORMED_DATA_DIR, \"train\"),\n",
    "    'transformed_eval_data_location':  os.path.join(TRANSFORMED_DATA_DIR, \"eval\"),\n",
    "    'transform_artefact_location':  TRANSFORM_ARTEFACTS_DIR,\n",
    "    \n",
    "    'temporary_dir': TEMP_DIR,\n",
    "    'project': PROJECT,\n",
    "    'temp_location': TEMP_DIR,\n",
    "    'staging_location': os.path.join(OUTPUT_DIR, 'staging'),\n",
    "    'max_num_workers': 8,\n",
    "    'save_main_session': False,\n",
    "    'setup_file': './setup.py'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "qNyztdP3jjaz",
    "outputId": "2c75be8e-cfe5-4834-d0ae-2f07bfecf373"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing gs://cloud-training-demos-ml/census/tfx/ contents...\n",
      "Running TF Transform pipeline...\n",
      "\n",
      "Raw schema location: raw_schema.pbtxt\n",
      "Raw train data location: gs://cloud-samples-data/ml-engine/census/data/adult.data.csv\n",
      "Raw evaluation data location: gs://cloud-samples-data/ml-engine/census/data/adult.test.csv\n",
      "Transformed train data location: gs://cloud-training-demos-ml/census/tfx/transformed/train\n",
      "Transformed evaluation data location: gs://cloud-training-demos-ml/census/tfx/transformed/eval\n",
      "Transform artefact location: gs://cloud-training-demos-ml/census/tfx/transform\n",
      "Temporary directory: gs://cloud-training-demos-ml/census/tfx/tmp\n",
      "Runner: DataflowRunner\n",
      "\n",
      "Reading and parsing raw training data...\n",
      "Analyzing and transforming raw training data...\n",
      "Writing transformed training data...\n",
      "Reading and parsing raw evaluation data...\n",
      "Transforming raw evaluation data...\n",
      "Writing transformed evaluation data...\n",
      "Writing transform artefacts...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CalledProcessError(1, ['/usr/bin/python3', '-m', 'pip', 'download', '--dest', '/tmp/tmp39twcfx0', 'apache-beam==2.13.0.dev0', '--no-deps', '--no-binary', ':all:'])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/apache_beam/runners/portability/stager.py\u001b[0m in \u001b[0;36m_download_pypi_sdk_package\u001b[0;34m(temp_dir, fetch_binary, language_version_tag, language_implementation_tag, abi_tag, platform_tag)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m       \u001b[0mprocesses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mprocesses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCalledProcessError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/apache_beam/utils/processes.py\u001b[0m in \u001b[0;36mcheck_output\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'shell'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/subprocess.py\u001b[0m in \u001b[0;36mcheck_output\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n\u001b[0;32m--> 316\u001b[0;31m                **kwargs).stdout\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    397\u001b[0m             raise CalledProcessError(retcode, process.args,\n\u001b[0;32m--> 398\u001b[0;31m                                      output=stdout, stderr=stderr)\n\u001b[0m\u001b[1;32m    399\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mCompletedProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['/usr/bin/python3', '-m', 'pip', 'download', '--dest', '/tmp/tmp39twcfx0', 'apache-beam==2.13.0.dev0', '--no-deps', '--no-binary', ':all:']' returned non-zero exit status 1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-451cb7ac92e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running TF Transform pipeline...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pipeline is done.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-fdd31e3e3855>\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    118\u001b[0m           \u001b[0mtransform_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m           | 'Write Transform Artefacts' >> tft_beam.WriteTransformFn(\n\u001b[0;32m--> 120\u001b[0;31m               transform_artefact_location)\n\u001b[0m\u001b[1;32m    121\u001b[0m       )\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/apache_beam/pipeline.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    424\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_until_finish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mvisit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisitor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/apache_beam/pipeline.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, test_runner_api)\u001b[0m\n\u001b[1;32m    404\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_runner_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_fake_coders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m           self._options).run(False)\n\u001b[0m\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTypeOptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mruntime_type_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/apache_beam/pipeline.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, test_runner_api)\u001b[0m\n\u001b[1;32m    417\u001b[0m       \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmpdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/apache_beam/runners/dataflow/dataflow_runner.py\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m(self, pipeline, options)\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0;31m# raise an exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     result = DataflowPipelineResult(\n\u001b[0;32m--> 428\u001b[0;31m         self.dataflow_client.create_job(self.job), self)\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[0;31m# TODO(BEAM-4274): Circular import runners-metrics. Requires refactoring.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/apache_beam/utils/retry.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexn\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mretry_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/apache_beam/runners/dataflow/internal/apiclient.py\u001b[0m in \u001b[0;36mcreate_job\u001b[0;34m(self, job)\u001b[0m\n\u001b[1;32m    518\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcreate_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m     \u001b[0;34m\"\"\"Creates job description. May stage and/or submit for remote execution.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_job_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m     \u001b[0;31m# Stage and submit the job when necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/apache_beam/runners/dataflow/internal/apiclient.py\u001b[0m in \u001b[0;36mcreate_job_description\u001b[0;34m(self, job)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;31m# Stage other resources for the SDK harness\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m     \u001b[0mresources\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stage_resources\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m     job.proto.environment = Environment(\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/apache_beam/runners/dataflow/internal/apiclient.py\u001b[0m in \u001b[0;36m_stage_resources\u001b[0;34m(self, options)\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[0mtemp_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtempfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdtemp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m         staging_location=google_cloud_options.staging_location)\n\u001b[0m\u001b[1;32m    481\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresources\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/apache_beam/runners/portability/stager.py\u001b[0m in \u001b[0;36mstage_job_resources\u001b[0;34m(self, options, build_setup_args, temp_dir, populate_requirements_cache, staging_location)\u001b[0m\n\u001b[1;32m    227\u001b[0m         resources.extend(\n\u001b[1;32m    228\u001b[0m             self._stage_beam_sdk(sdk_remote_location, staging_location,\n\u001b[0;32m--> 229\u001b[0;31m                                  temp_dir))\n\u001b[0m\u001b[1;32m    230\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0msetup_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msdk_location\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'container'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;31m# Use the SDK that's built into the container, rather than re-staging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/apache_beam/runners/portability/stager.py\u001b[0m in \u001b[0;36m_stage_beam_sdk\u001b[0;34m(self, sdk_remote_location, staging_location, temp_dir)\u001b[0m\n\u001b[1;32m    480\u001b[0m       \"\"\"\n\u001b[1;32m    481\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msdk_remote_location\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'pypi'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m       \u001b[0msdk_local_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_pypi_sdk_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m       \u001b[0msdk_sources_staged_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m           \u001b[0m_desired_sdk_filename_in_staging_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msdk_local_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/apache_beam/runners/portability/stager.py\u001b[0m in \u001b[0;36m_download_pypi_sdk_package\u001b[0;34m(temp_dir, fetch_binary, language_version_tag, language_implementation_tag, abi_tag, platform_tag)\u001b[0m\n\u001b[1;32m    574\u001b[0m       \u001b[0mprocesses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mprocesses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCalledProcessError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msdk_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexpected_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CalledProcessError(1, ['/usr/bin/python3', '-m', 'pip', 'download', '--dest', '/tmp/tmp39twcfx0', 'apache-beam==2.13.0.dev0', '--no-deps', '--no-binary', ':all:'])"
     ]
    }
   ],
   "source": [
    "if tf.gfile.Exists(OUTPUT_DIR):\n",
    "  print(\"Removing {} contents...\".format(OUTPUT_DIR))\n",
    "  tf.gfile.DeleteRecursively(OUTPUT_DIR)\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "print(\"Running TF Transform pipeline...\")\n",
    "print(\"\")\n",
    "run_pipeline(args)\n",
    "print(\"\")\n",
    "print(\"Pipeline is done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PfQO1IjmsZQb"
   },
   "source": [
    "### Check the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "SmBSS38GsZbx",
    "outputId": "8876bb26-57ab-40dd-aac9-35210344e09b"
   },
   "outputs": [],
   "source": [
    "!gsutil ls {OUTPUT_DIR}/* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "sCexlD9EtV2I",
    "outputId": "5713ef76-ccc6-4578-9b8f-72df71ff62a8"
   },
   "outputs": [],
   "source": [
    "!gsutil ls {OUTPUT_DIR}/transform/transform_fn/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lYsjUQt2tbEV"
   },
   "outputs": [],
   "source": [
    "!gsutil cat output/transform/transformed_metadata/schema.pbtxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gsi_Hsh89Cl7"
   },
   "source": [
    "## License"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0fOWx1yI9Dyn"
   },
   "source": [
    "Copyright 2019 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0.\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\n",
    "---\n",
    "This is not an official Google product. The sample code provided for educational purposes only.\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "02-tfx_end_to_end",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
