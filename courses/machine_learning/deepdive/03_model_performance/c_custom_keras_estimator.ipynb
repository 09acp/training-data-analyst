{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Estimator with Keras\n",
    "\n",
    "**Learning Objectives**\n",
    "- Learn how to create custom estimator using tf.keras\n",
    "    \n",
    "Up until now we've been limited in our model architectures to premade estimators. But what if we want more control over the model? \n",
    "\n",
    "We can use the popular Keras API to create a custom model and then convert it to an estimator using `tf.keras.estimator.model_to_estimator()`. \n",
    "\n",
    "This gives us access to all the flexibility of Keras for creating deep learning models, but also the production readiness of the estimator framework!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py3env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import shutil\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Input Functions\n",
    "\n",
    "Same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_COLUMN_NAMES = [\"fare_amount\",\"dayofweek\",\"hourofday\",\"pickuplon\",\"pickuplat\",\"dropofflon\",\"dropofflat\"]\n",
    "CSV_DEFAULTS = [[0.0],[1],[0],[-74.0], [40.0], [-74.0], [40.7]]\n",
    "\n",
    "def read_dataset(csv_path):\n",
    "    def parse_row(row):\n",
    "        # Decode the CSV row into list of TF tensors\n",
    "        fields = tf.decode_csv(records = row, record_defaults = CSV_DEFAULTS)\n",
    "\n",
    "        # Pack the result into a dictionary\n",
    "        features = dict(zip(CSV_COLUMN_NAMES, fields))\n",
    "        \n",
    "        # NEW: Add engineered features\n",
    "        features = add_engineered_features(features)\n",
    "        \n",
    "        # Separate the label from the features\n",
    "        label = features.pop(\"fare_amount\") # remove label from features and store\n",
    "\n",
    "        return features, label\n",
    "    \n",
    "    # Create a dataset containing the text lines.\n",
    "    dataset = tf.data.Dataset.list_files(file_pattern = csv_path) # (i.e. data_file_*.csv)\n",
    "    dataset = dataset.flat_map(map_func = lambda filename: tf.data.TextLineDataset(filenames = filename).skip(count = 1))\n",
    "\n",
    "    # Parse each CSV row into correct (features,label) format for Estimator API\n",
    "    dataset = dataset.map(map_func = parse_row)\n",
    "    \n",
    "    return dataset\n",
    "  \n",
    "def create_feature_keras_input(features, label):\n",
    "  features = tf.feature_column.input_layer(features = features, feature_columns = create_feature_columns())\n",
    "  return features, label\n",
    "\n",
    "def train_input_fn(csv_path, batch_size = 128):\n",
    "    #1. Convert CSV into tf.data.Dataset with (features, label) format\n",
    "    dataset = read_dataset(csv_path)\n",
    "      \n",
    "    #2. Shuffle, repeat, and batch the examples.\n",
    "    dataset = dataset.shuffle(buffer_size = 1000).repeat(count = None).batch(batch_size = batch_size)\n",
    "    \n",
    "    #3. Create single feature tensor for input to Keras Model\n",
    "    dataset = dataset.map(map_func = create_feature_keras_input)\n",
    "   \n",
    "    return dataset\n",
    "\n",
    "def eval_input_fn(csv_path, batch_size = 128):\n",
    "    #1. Convert CSV into tf.data.Dataset with (features, label) format\n",
    "    dataset = read_dataset(csv_path)\n",
    "\n",
    "    #2.Batch the examples.\n",
    "    dataset = dataset.batch(batch_size = batch_size)\n",
    "    \n",
    "    #3. Create single feature tensor for input to Keras Model\n",
    "    dataset = dataset.map(map_func = create_feature_keras_input)\n",
    "   \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_engineered_features(features):\n",
    "    features[\"latdiff\"] = features[\"pickuplat\"] - features[\"dropofflat\"] # East/West\n",
    "    features[\"londiff\"] = features[\"pickuplon\"] - features[\"dropofflon\"] # North/South\n",
    "    features[\"euclidean_dist\"] = tf.sqrt(x = features[\"latdiff\"]**2 + features[\"londiff\"]**2)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_columns():\n",
    "  # One hot encode dayofweek and hourofday\n",
    "  fc_dayofweek = tf.feature_column.categorical_column_with_identity(key = \"dayofweek\", num_buckets = 8)\n",
    "  fc_hourofday = tf.feature_column.categorical_column_with_identity(key = \"hourofday\", num_buckets = 24)\n",
    "\n",
    "  # Cross features to get combination of day and hour\n",
    "  fc_day_hr = tf.feature_column.crossed_column(keys = [fc_dayofweek, fc_hourofday], hash_bucket_size = 24 * 7)\n",
    "\n",
    "  # Bucketize latitudes and longitudes\n",
    "  NBUCKETS = 16\n",
    "  latbuckets = np.linspace(38.0, 42.0, NBUCKETS).tolist()\n",
    "  lonbuckets = np.linspace(-76.0, -72.0, NBUCKETS).tolist()\n",
    "  fc_bucketized_plat = tf.feature_column.bucketized_column(source_column = tf.feature_column.numeric_column(key = \"pickuplon\"), boundaries = lonbuckets)\n",
    "  fc_bucketized_plon = tf.feature_column.bucketized_column(source_column = tf.feature_column.numeric_column(key = \"pickuplat\"), boundaries = latbuckets)\n",
    "  fc_bucketized_dlat = tf.feature_column.bucketized_column(source_column = tf.feature_column.numeric_column(key = \"dropofflon\"), boundaries = lonbuckets)\n",
    "  fc_bucketized_dlon = tf.feature_column.bucketized_column(source_column = tf.feature_column.numeric_column(key = \"dropofflat\"), boundaries = latbuckets)\n",
    "\n",
    "  feature_columns = [\n",
    "    #1. Engineered using tf.feature_column module\n",
    "    tf.feature_column.indicator_column(categorical_column = fc_day_hr), # 168 columns\n",
    "    fc_bucketized_plat, # 16 + 1 = 17 columns\n",
    "    fc_bucketized_plon, # 16 + 1 = 17 columns\n",
    "    fc_bucketized_dlat, # 16 + 1 = 17 columns\n",
    "    fc_bucketized_dlon, # 16 + 1 = 17 columns\n",
    "    #2. Engineered in input functions\n",
    "    tf.feature_column.numeric_column(key = \"latdiff\"), # 1 column\n",
    "    tf.feature_column.numeric_column(key = \"londiff\"), # 1 column\n",
    "    tf.feature_column.numeric_column(key = \"euclidean_dist\") # 1 column\n",
    "  ]\n",
    "  \n",
    "  return feature_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the number of feature columns that will be input to our Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_feature_columns = 239\n"
     ]
    }
   ],
   "source": [
    "num_feature_columns = 168 + (16 + 1) * 4 + 3\n",
    "print(\"num_feature_columns = {}\".format(num_feature_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Custom Keras Model\n",
    "\n",
    "Build a Keras model as described [here](https://www.tensorflow.org/guide/keras)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keras_model():\n",
    "  model = tf.keras.Sequential()\n",
    "  model.add(tf.keras.layers.Dense(units = 64, activation = \"relu\", input_shape = (num_feature_columns,)))\n",
    "  model.add(tf.keras.layers.Dense(units = 64, activation = \"relu\"))\n",
    "  model.add(tf.keras.layers.Dense(units = 64, activation = \"relu\"))\n",
    "  model.add(tf.keras.layers.Dense(units = 64, activation = \"relu\"))\n",
    "  model.add(tf.keras.layers.Dense(units = 8, activation = \"relu\"))\n",
    "  model.add(tf.keras.layers.Dense(units = 1, activation = None))\n",
    "\n",
    "  def rmse(y_true, y_pred): # Root Mean Squared Error\n",
    "    return tf.sqrt(x = tf.reduce_mean(input_tensor = tf.square(x = y_pred - y_true)))\n",
    "\n",
    "  model.compile(optimizer = tf.train.AdamOptimizer(),\n",
    "                loss = \"mean_squared_error\",\n",
    "                metrics = [rmse])\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Custom Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create serving input function\n",
    "def serving_input_fn():\n",
    "  feature_placeholders = {\n",
    "    \"dayofweek\": tf.placeholder(dtype = tf.int32, shape = [None]),\n",
    "    \"hourofday\": tf.placeholder(dtype = tf.int32, shape = [None]),\n",
    "    \"pickuplon\": tf.placeholder(dtype = tf.float32, shape = [None]),\n",
    "    \"pickuplat\": tf.placeholder(dtype = tf.float32, shape = [None]),\n",
    "    \"dropofflon\": tf.placeholder(dtype = tf.float32, shape = [None]),\n",
    "    \"dropofflat\": tf.placeholder(dtype = tf.float32, shape = [None]),\n",
    "  }\n",
    "  \n",
    "  features = {key: tensor for key, tensor in feature_placeholders.items()}\n",
    "    \n",
    "  return tf.estimator.export.ServingInputReceiver(features = features, receiver_tensors = feature_placeholders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate\n",
    "\n",
    "Note the use of `tf.keras.estimator.model_to_estimator` to create our estimator. It takes as arguments the compiled keras model, the OUTDIR, and optionally a `tf.estimator.Runconfig`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>TensorBoard was started successfully with pid 975935. Click <a href=\"/_proxy/47379/\" target=\"_blank\">here</a> to access it.</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "975935"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.datalab.ml import TensorBoard\n",
    "TensorBoard().start(\"taxi_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(output_dir):\n",
    "  estimator = tf.keras.estimator.model_to_estimator(\n",
    "      keras_model = create_keras_model(),\n",
    "      model_dir = output_dir,\n",
    "      config = tf.estimator.RunConfig(\n",
    "            tf_random_seed = 1, # for reproducibility\n",
    "            save_checkpoints_steps = 100 # checkpoint every N steps\n",
    "      )\n",
    "  )\n",
    "\n",
    "  train_spec=tf.estimator.TrainSpec(\n",
    "                     input_fn = lambda: train_input_fn(csv_path = \"./taxi-train.csv\"),\n",
    "                     max_steps = 500)\n",
    "\n",
    "  exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n",
    "\n",
    "  eval_spec=tf.estimator.EvalSpec(\n",
    "                     input_fn = lambda: eval_input_fn(csv_path = \"./taxi-valid.csv\"),\n",
    "                     steps = None,\n",
    "                     start_delay_secs = 10, # wait at least N seconds before first evaluation (default 120)\n",
    "                     throttle_secs = 10, # wait at least N seconds before each subsequent evaluation (default 600)\n",
    "                     exporters = None) # export SavedModel once at the end of training\n",
    "\n",
    "  tf.logging.set_verbosity(tf.logging.INFO) # so loss is printed during training\n",
    "  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using the Keras model provided.\n",
      "INFO:tensorflow:Using config: {'_service': None, '_experimental_distribute': None, '_save_checkpoints_steps': 100, '_train_distribute': None, '_protocol': None, '_device_fn': None, '_save_summary_steps': 100, '_save_checkpoints_secs': None, '_model_dir': 'taxi_trained', '_master': '', '_tf_random_seed': 1, '_keep_checkpoint_every_n_hours': 10000, '_num_worker_replicas': 1, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_eval_distribute': None, '_evaluation_master': '', '_task_type': 'worker', '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_id': 0, '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f0d73065438>, '_log_step_count_steps': 100}\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 100 or save_checkpoints_secs None.\n",
      "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/ops/sparse_ops.py:1165: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='taxi_trained/keras/keras_model.ckpt', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})\n",
      "INFO:tensorflow:Warm-starting from: ('taxi_trained/keras/keras_model.ckpt',)\n",
      "INFO:tensorflow:Warm-starting variable: dense_4/bias; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: dense_4/kernel; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: dense/bias; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: dense_1/kernel; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: dense_2/bias; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: dense_3/kernel; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: dense_5/kernel; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: dense_1/bias; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: dense/kernel; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: dense_3/bias; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: dense_2/kernel; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: dense_5/bias; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into taxi_trained/model.ckpt.\n",
      "INFO:tensorflow:loss = 203.6753, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 100 into taxi_trained/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-02-22-22:15:41\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from taxi_trained/model.ckpt-100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-02-22-22:15:50\n",
      "INFO:tensorflow:Saving dict for global step 100: global_step = 100, loss = 85.82757, rmse = 9.096476\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 100: taxi_trained/model.ckpt-100\n",
      "INFO:tensorflow:global_step/sec: 7.43119\n",
      "INFO:tensorflow:loss = 71.53804, step = 101 (13.460 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 200 into taxi_trained/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-02-22-22:15:55\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from taxi_trained/model.ckpt-200\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-02-22-22:16:04\n",
      "INFO:tensorflow:Saving dict for global step 200: global_step = 200, loss = 85.873856, rmse = 9.087907\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 200: taxi_trained/model.ckpt-200\n",
      "INFO:tensorflow:global_step/sec: 7.22963\n",
      "INFO:tensorflow:loss = 59.44657, step = 201 (13.834 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 300 into taxi_trained/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-02-22-22:16:09\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from taxi_trained/model.ckpt-300\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-02-22-22:16:18\n",
      "INFO:tensorflow:Saving dict for global step 300: global_step = 300, loss = 86.28287, rmse = 9.105211\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 300: taxi_trained/model.ckpt-300\n",
      "INFO:tensorflow:global_step/sec: 7.14876\n",
      "INFO:tensorflow:loss = 81.652565, step = 301 (13.987 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 400 into taxi_trained/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-02-22-22:16:23\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from taxi_trained/model.ckpt-400\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-02-22-22:16:31\n",
      "INFO:tensorflow:Saving dict for global step 400: global_step = 400, loss = 85.64857, rmse = 9.077681\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 400: taxi_trained/model.ckpt-400\n",
      "INFO:tensorflow:global_step/sec: 7.50816\n",
      "INFO:tensorflow:loss = 88.648865, step = 401 (13.318 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 500 into taxi_trained/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-02-22-22:16:35\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from taxi_trained/model.ckpt-500\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-02-22-22:16:44\n",
      "INFO:tensorflow:Saving dict for global step 500: global_step = 500, loss = 85.534485, rmse = 9.077223\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 500: taxi_trained/model.ckpt-500\n",
      "INFO:tensorflow:Loss for final step: 101.85396.\n",
      "CPU times: user 1min 38s, sys: 48.1 s, total: 2min 26s\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "OUTDIR = \"taxi_trained\"\n",
    "shutil.rmtree(OUTDIR, ignore_errors = True) # start fresh each time\n",
    "train_and_evaluate(OUTDIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(TensorBoard.list()) > 0:\n",
    "  [TensorBoard().stop(pid)for pid in TensorBoard.list()[\"pid\"]]\n",
    "else:\n",
    "  print(\"No TensorBoard instances to stop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2019 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
