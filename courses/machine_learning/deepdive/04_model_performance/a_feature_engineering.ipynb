{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Feature Engineering\n",
    "**Learning Objectives**\n",
    "  * Improve the accuracy of a model by using feature engineering\n",
    "  * Understand there's two places to do feature engineering in Tensorflow\n",
    "    1. In the input functions\n",
    "    2. Using the `tf.feature_column` module\n",
    "    \n",
    "Up until now we've been focusing on Tensorflow mechanics to make sure our code works, we have neglected model performance, which at this point is **9.26 RMSE**. \n",
    "\n",
    "In this notebook we'll attempt to improve on that using feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py3env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import shutil\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Load Raw Data\n",
    "\n",
    "These are the same files created in the `create_datasets.ipynb` notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://cloud-training-demos/taxifare/small/taxi-test.csv...\n",
      "Copying gs://cloud-training-demos/taxifare/small/taxi-train.csv...              \n",
      "Copying gs://cloud-training-demos/taxifare/small/taxi-valid.csv...              \n",
      "| [3 files][ 11.3 MiB/ 11.3 MiB]                                                \n",
      "Operation completed over 3 objects/11.3 MiB.                                     \n",
      "-rw-r--r-- 1 root root 1867645 Jan  6 17:27 taxi-test.csv\n",
      "-rw-r--r-- 1 root root 8289592 Jan  6 17:27 taxi-train.csv\n",
      "-rw-r--r-- 1 root root 1737393 Jan  6 17:27 taxi-valid.csv\n"
     ]
    }
   ],
   "source": [
    "!gsutil cp gs://cloud-training-demos/taxifare/small/*.csv .\n",
    "!ls -l *.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Train and Evaluate Input Functions\n",
    "\n",
    "These are the same as before with one additional line of code: a call to `add_engineered_features()` from within the `_parse_row()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_COLUMN_NAMES = ['fare_amount','dayofweek','hourofday','pickuplon','pickuplat','dropofflon','dropofflat','passengers']\n",
    "CSV_DEFAULTS = [[0.0],[1],[0],[-74.0], [40.0], [-74.0], [40.7], [1]]\n",
    "\n",
    "def read_dataset(csv_path):\n",
    "    def _parse_row(row):\n",
    "        # Decode the CSV row into list of TF tensors\n",
    "        fields = tf.decode_csv(row, record_defaults=CSV_DEFAULTS)\n",
    "\n",
    "        # Pack the result into a dictionary\n",
    "        features = dict(zip(CSV_COLUMN_NAMES, fields))\n",
    "        \n",
    "        # NEW: Add engineered features\n",
    "        features = add_engineered_features(features)\n",
    "        \n",
    "        # Separate the label from the features\n",
    "        label = features.pop('fare_amount') # remove label from features and store\n",
    "\n",
    "        return features, label\n",
    "    \n",
    "    # Create a dataset containing the text lines.\n",
    "    dataset = tf.data.Dataset.list_files(csv_path) # (i.e. data_file_*.csv)\n",
    "    dataset = dataset.flat_map(lambda filename:tf.data.TextLineDataset(filename).skip(1))\n",
    "\n",
    "    # Parse each CSV row into correct (features,label) format for Estimator API\n",
    "    dataset = dataset.map(_parse_row)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def train_input_fn(csv_path, batch_size=128):\n",
    "    #1. Convert CSV into tf.data.Dataset  with (features,label) format\n",
    "    dataset = read_dataset(csv_path)\n",
    "      \n",
    "    #2. Shuffle, repeat, and batch the examples.\n",
    "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "   \n",
    "    return dataset\n",
    "\n",
    "def eval_input_fn(csv_path, batch_size=128):\n",
    "    #1. Convert CSV into tf.data.Dataset  with (features,label) format\n",
    "    dataset = read_dataset(csv_path)\n",
    "\n",
    "    #2.Batch the examples.\n",
    "    dataset = dataset.batch(batch_size)\n",
    "   \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Feature Engineering: Feature Columns\n",
    "\n",
    "There are two places in Tensorflow where we can do feature engineering. The first is using the `tf.feature_column` package. This allows us easily \n",
    "\n",
    "- bucketize continuous features\n",
    "- one hot encode categorical features\n",
    "- create feature crosses\n",
    "- embed categorical features\n",
    "\n",
    "For details on the possible `tf.feature_column` transformations and when to use each see the [official guide](https://www.tensorflow.org/guide/feature_columns).\n",
    "\n",
    "Let's use `tf.feature_column` to create a feature that shows the combination of day of week and hour of day. This will allow our model to easily learn the difference between say Wednesday at 5pm (rush hour, expect higher fares) and Sunday at 5pm (light traffic, expect lower fares)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. One hot encode dayofweek and hourofday, so they can be crossed\n",
    "fc_dayofweek = tf.feature_column.categorical_column_with_identity('dayofweek', num_buckets = 8)\n",
    "fc_hourofday = tf.feature_column.categorical_column_with_identity('hourofday', num_buckets = 24)\n",
    "\n",
    "# 2. Cross features to get combination of day and hour\n",
    "fc_day_hr = tf.feature_column.crossed_column([fc_dayofweek, fc_hourofday], 24 * 7)\n",
    "\n",
    "# 3. Embed sparse vector into dense representation so that it can be used with DNN\n",
    "fc_day_hr_embedded = tf.feature_column.embedding_column(fc_day_hr, 3)\n",
    "\n",
    "feature_cols = [fc_day_hr_embedded]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Feature Engineering: Input Functions\n",
    "\n",
    "While feature columns are very powerful, what happens when we want to something that there isn't a feature column for?\n",
    "\n",
    "Recall the input functions recieve csv data, format it, then pass it batch by batch to the model.Here we can inject arbitrary tensorflow code to manipulate the data.\n",
    "\n",
    "However, we need to be careful that any transformations we do in one input function, we do for all, otherwise we'll have [training-serving skew](https://developers.google.com/machine-learning/guides/rules-of-ml/#training-serving_skew).\n",
    "\n",
    "To guard against this we encapsulate all feature engineering in a single function, `add_engineered_features()`, and call this function from every input function.\n",
    "\n",
    "So what feature should we engineer?\n",
    "\n",
    "Currently our model has pickup and drop off latitudes and longitudes as features, but feeding this in directly to the model isn't very useful. Let's calculate the euclidean distance between the pickup and dropoff points and feed that as a new feature to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_engineered_features(features):\n",
    "    latdiff = features['pickuplat'] - features['dropofflat']\n",
    "    londiff = features['pickuplon'] - features['dropofflon']\n",
    "    euclidean_dist = tf.sqrt(latdiff**2 + londiff**2)\n",
    "    \n",
    "    features['euclidean_dist'] = euclidean_dist\n",
    "    return features\n",
    "\n",
    "fc_distance = tf.feature_column.numeric_column('euclidean_dist')\n",
    "feature_cols.append(fc_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Serving Input Receiver Function \n",
    "\n",
    "Same as before with one addition: the received tensors are wrapped with `add_engineered_features()` before passing on to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_receiver_fn():\n",
    "    receiver_tensors = {\n",
    "        'dayofweek' : tf.placeholder(tf.int32, shape=[None]), # shape is vector to allow batch of requests\n",
    "        'hourofday' : tf.placeholder(tf.int32, shape=[None]),\n",
    "        'pickuplon' : tf.placeholder(tf.float32, shape=[None]), \n",
    "        'pickuplat' : tf.placeholder(tf.float32, shape=[None]),\n",
    "        'dropofflat' : tf.placeholder(tf.float32, shape=[None]),\n",
    "        'dropofflon' : tf.placeholder(tf.float32, shape=[None]),\n",
    "        'passengers' : tf.placeholder(tf.int32, shape=[None]),\n",
    "    }\n",
    "    \n",
    "    features = add_engineered_features(receiver_tensors) # 'features' is what is passed on to the model\n",
    "    \n",
    "    return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Monitoring with TensorBoard \n",
    "\n",
    "*Warning:* There is an issue with DataLab that causes TensorBoard to only work correctly the first time it's launched per session. So if you have issues try resetting the kernel and launching TensorBoard again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>TensorBoard was started successfully with pid 74808. Click <a href=\"/_proxy/57311/\" target=\"_blank\">here</a> to access it.</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "74808"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.datalab.ml import TensorBoard\n",
    "TensorBoard().start('taxi_trained')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Train and Evaluate\n",
    "\n",
    "Same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "OUTDIR = 'taxi_trained/500'\n",
    "shutil.rmtree(OUTDIR, ignore_errors = True) # start fresh each time\n",
    "\n",
    "model = tf.estimator.DNNRegressor(\n",
    "    hidden_units = [10,10], # specify neural architecture\n",
    "    feature_columns = feature_cols, \n",
    "    model_dir = OUTDIR,\n",
    "    config = tf.estimator.RunConfig(\n",
    "          tf_random_seed=1, # for reproducibility\n",
    "          save_checkpoints_steps=100 # checkpoint every N steps\n",
    "    ) \n",
    ")\n",
    "\n",
    "# Add custom evaluation metric\n",
    "def my_rmse(labels, predictions):\n",
    "    pred_values = tf.squeeze(predictions['predictions'],axis=-1)\n",
    "    return {'rmse': tf.metrics.root_mean_squared_error(labels, pred_values)}\n",
    "model = tf.contrib.estimator.add_metrics(model, my_rmse) \n",
    "    \n",
    "train_spec=tf.estimator.TrainSpec(\n",
    "                   input_fn = lambda:train_input_fn('./taxi-train.csv'),\n",
    "                   max_steps = 500)\n",
    "\n",
    "exporter = tf.estimator.FinalExporter('exporter', serving_input_receiver_fn) # export SavedModel once at the end of training\n",
    "# Note: alternatively use tf.estimator.BestExporter to export at every checkpoint that has lower loss than the previous checkpoint\n",
    "\n",
    "eval_spec=tf.estimator.EvalSpec(\n",
    "                   input_fn=lambda:eval_input_fn('./taxi-valid.csv'),\n",
    "                   steps = None,\n",
    "                   start_delay_secs=1, # wait at least N seconds before first evaluation (default 120)\n",
    "                   throttle_secs=1, # wait at least N seconds before each subsequent evaluation (default 600)\n",
    "                   exporters = exporter) # export SavedModel once at the end of training\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO) # so loss is printed during training\n",
    "shutil.rmtree(OUTDIR, ignore_errors = True) # start fresh each time\n",
    "tf.estimator.train_and_evaluate(model, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Our RMSE is now 6.67, our first significan improvement! If we look at the RMSE trend in TensorBoard it appears the model is still learning, so training past 500 steps would likely lower the RMSE even more. Let's run again, this time for 10x as many steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "OUTDIR = 'taxi_trained/5000'\n",
    "shutil.rmtree(OUTDIR, ignore_errors = True) # start fresh each time\n",
    "\n",
    "model = tf.estimator.DNNRegressor(\n",
    "    hidden_units = [10,10], # specify neural architecture\n",
    "    feature_columns = feature_cols, \n",
    "    model_dir = OUTDIR,\n",
    "    config = tf.estimator.RunConfig(\n",
    "          tf_random_seed=1, # for reproducibility\n",
    "          save_checkpoints_steps=500 # checkpoint every N steps\n",
    "    ) \n",
    ")\n",
    "\n",
    "# Add custom evaluation metric\n",
    "def my_rmse(labels, predictions):\n",
    "    pred_values = tf.squeeze(predictions['predictions'],axis=-1)\n",
    "    return {'rmse': tf.metrics.root_mean_squared_error(labels, pred_values)}\n",
    "model = tf.contrib.estimator.add_metrics(model, my_rmse) \n",
    "    \n",
    "train_spec=tf.estimator.TrainSpec(\n",
    "                   input_fn = lambda:train_input_fn('./taxi-train.csv'),\n",
    "                   max_steps = 5000)\n",
    "\n",
    "exporter = tf.estimator.FinalExporter('exporter', serving_input_receiver_fn) # export SavedModel once at the end of training\n",
    "\n",
    "eval_spec=tf.estimator.EvalSpec(\n",
    "                   input_fn=lambda:eval_input_fn('./taxi-valid.csv'),\n",
    "                   steps = None,\n",
    "                   start_delay_secs=1, # wait at least N seconds before first evaluation (default 120)\n",
    "                   throttle_secs=1, # wait at least N seconds before each subsequent evaluation (default 600)\n",
    "                   exporters = exporter) # export SavedModel once at the end of training\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO) # so loss is printed during training\n",
    "shutil.rmtree(OUTDIR, ignore_errors = True) # start fresh each time\n",
    "tf.estimator.train_and_evaluate(model, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results: 5000 steps\n",
    "\n",
    "Our RMSE is now 4.79! It looks like RMSE is still reducing, but training is getting slow so we should move to the cloud if we want to train longer.\n",
    "\n",
    "Also we haven't explored our hyperparameters much. Is our neural architecture of [10,10] optimal? Is our learning rate ideal?\n",
    "\n",
    "In the next notebook we'll show a way to choose the ideal values for all of these hyperparemeters automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(TensorBoard.list())>0:\n",
    "  [TensorBoard().stop(pid)for pid in TensorBoard.list()['pid']]\n",
    "else: print('No TensorBoard instances to stop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Exercise\n",
    "\n",
    "Modify your solution to the challenge exercise in c_dataset.ipynb appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2019 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
