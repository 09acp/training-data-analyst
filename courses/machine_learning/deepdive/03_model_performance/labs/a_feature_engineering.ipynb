{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Feature Engineering\n",
    "**Learning Objectives**\n",
    "  * Improve the accuracy of a model by using feature engineering\n",
    "  * Understand there's two places to do feature engineering in Tensorflow\n",
    "    1. Using the `tf.feature_column` module\n",
    "    2. In the input functions\n",
    "    \n",
    "Up until now we've been focusing on Tensorflow mechanics to make sure our code works, we have neglected model performance, which at this point is **9.26 RMSE**. \n",
    "\n",
    "In this notebook we'll attempt to improve on that using feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze | grep tensorflow==1.12.0 || pip install tensorflow==1.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import shutil\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Load Raw Data\n",
    "\n",
    "These are the same files created in the `create_datasets.ipynb` notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp gs://cloud-training-demos/taxifare/small/*.csv .\n",
    "!ls -l *.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Train and Evaluate Input Functions\n",
    "\n",
    "These are the same as before with one additional line of code: a call to `add_engineered_features()` from within the `_parse_row()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_COLUMN_NAMES = ['fare_amount','dayofweek','hourofday','pickuplon','pickuplat','dropofflon','dropofflat']\n",
    "CSV_DEFAULTS = [[0.0],[1],[0],[-74.0], [40.0], [-74.0], [40.7]]\n",
    "\n",
    "def read_dataset(csv_path):\n",
    "    def _parse_row(row):\n",
    "        # Decode the CSV row into list of TF tensors\n",
    "        fields = tf.decode_csv(row, record_defaults=CSV_DEFAULTS)\n",
    "\n",
    "        # Pack the result into a dictionary\n",
    "        features = dict(zip(CSV_COLUMN_NAMES, fields))\n",
    "        \n",
    "        # NEW: Add engineered features\n",
    "        features = add_engineered_features(features)\n",
    "        \n",
    "        # Separate the label from the features\n",
    "        label = features.pop('fare_amount') # remove label from features and store\n",
    "\n",
    "        return features, label\n",
    "    \n",
    "    # Create a dataset containing the text lines.\n",
    "    dataset = tf.data.Dataset.list_files(csv_path) # (i.e. data_file_*.csv)\n",
    "    dataset = dataset.flat_map(lambda filename:tf.data.TextLineDataset(filename).skip(1))\n",
    "\n",
    "    # Parse each CSV row into correct (features,label) format for Estimator API\n",
    "    dataset = dataset.map(_parse_row)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def train_input_fn(csv_path, batch_size=128):\n",
    "    #1. Convert CSV into tf.data.Dataset  with (features,label) format\n",
    "    dataset = read_dataset(csv_path)\n",
    "      \n",
    "    #2. Shuffle, repeat, and batch the examples.\n",
    "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "   \n",
    "    return dataset\n",
    "\n",
    "def eval_input_fn(csv_path, batch_size=128):\n",
    "    #1. Convert CSV into tf.data.Dataset  with (features,label) format\n",
    "    dataset = read_dataset(csv_path)\n",
    "\n",
    "    #2.Batch the examples.\n",
    "    dataset = dataset.batch(batch_size)\n",
    "   \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Feature Engineering: Feature Columns\n",
    "\n",
    "There are two places in Tensorflow where we can do feature engineering. The first is using the `tf.feature_column` package. This allows us easily \n",
    "\n",
    "- bucketize continuous features\n",
    "- one hot encode categorical features\n",
    "- create feature crosses\n",
    "\n",
    "For details on the possible `tf.feature_column` transformations and when to use each see the [official guide](https://www.tensorflow.org/guide/feature_columns).\n",
    "\n",
    "Let's use `tf.feature_column` to create a feature that shows the combination of day of week and hour of day. This will allow our model to easily learn the difference between say Wednesday at 5pm (rush hour, expect higher fares) and Sunday at 5pm (light traffic, expect lower fares).\n",
    "\n",
    "Let's also use it to bucketize our latitudes and longitudes because treating them as continuous numbers is misleading to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Complete the code below by adding the necessary `tf.feature_column`. Have a look at the various feature columns [on this page](https://www.tensorflow.org/api_docs/python/tf/feature_column) and follow the links to the relevant documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. One hot encode dayofweek and hourofday\n",
    "fc_dayofweek = # TODO: Create a categorical feature column for 'dayofweek'\n",
    "fc_hourofday = # TODO: Create a categorical feature column for 'hourofday'\n",
    "\n",
    "# 2. Cross features to get combination of day and hour\n",
    "fc_day_hr = # TODO: Create a new feature column by crossing the categorical features 'fc_dayofweek' and 'fc_hourofday' you created above\n",
    "\n",
    "# 3. Bucketize latitudes and longitudes\n",
    "NBUCKETS = 16\n",
    "latbuckets = np.linspace(38.0, 42.0, NBUCKETS).tolist()\n",
    "lonbuckets = np.linspace(-76.0, -72.0, NBUCKETS).tolist()\n",
    "fc_bucketized_plat = # TODO: Create a bucketized feature column for the pickup latitude. Be careful to note that the 'source_column' for a bucketized feature column should a feature column itself. \n",
    "fc_bucketized_plon = # TODO: Create a bucketized feature column for the pickup longitude. Be careful to note that the 'source_column' for a bucketized feature column should a feature column itself. \n",
    "fc_bucketized_dlat = # TODO: Create a bucketized feature column for the dropoff latitude. Be careful to note that the 'source_column' for a bucketized feature column should a feature column itself. \n",
    "fc_bucketized_dlon = # TODO: Create a bucketized feature column for the dropoff longitude. Be careful to note that the 'source_column' for a bucketized feature column should a feature column itself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Feature Engineering: Input Functions\n",
    "\n",
    "While feature columns are very powerful, what happens when we want to something that there isn't a feature column for?\n",
    "\n",
    "Recall the input functions recieve csv data, format it, then pass it batch by batch to the model. We can also use input functions to inject arbitrary tensorflow code to manipulate the data.\n",
    "\n",
    "However, we need to be careful that any transformations we do in one input function, we do for all, otherwise we'll have [training-serving skew](https://developers.google.com/machine-learning/guides/rules-of-ml/#training-serving_skew).\n",
    "\n",
    "To guard against this we encapsulate all input function feature engineering in a single function, `add_engineered_features()`, and call this function from every input function.\n",
    "\n",
    "Let's calculate the euclidean distance between the pickup and dropoff points and feed that as a new feature to our model. \n",
    "\n",
    "Also it may be useful to know which cardinal direction that distance is in. I suspect that distance is cheaper to travel North/South because in Manhatten streets that run North/South have less stops than streets that run East/West."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Create some new engineered features using the `add_engineered_features` function below. This function takes a *dictionary* of features and returns the dictionary with some additional features added.\n",
    "We want to engineer a new feature which captures the Euclidean distance (the straight-line distance) between the pickup and dropoff. Complete the code below to compute the \n",
    "difference in the latitude and the difference in the longitude. Then, use these to compute the Euclidean distance and add that to the features dictionary as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_engineered_features(features):\n",
    "    features['latdiff'] = # TODO: Compute the difference in the pickup latitude and the dropoff latitude\n",
    "    features['londiff'] = # TODO: Compute the difference in the pickup longitude and the dropoff longitude\n",
    "    features['euclidean_dist'] = # TODO: Use 'latdiff' and 'londiff' to compute the Euclidean distance between the pickup and dropoff\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Gather list of Feature Columns\n",
    "\n",
    "Ultimately our estimator expects a list of feature columns, so let's gather all our engineered features into a single list.\n",
    "\n",
    "We cannot pass categorical or crossed feature columns directly into a DNN, Tensorflow will give us an error. We must first wrap them using either `indicator_column()` or `embedding_column()`. The former will pass through the one-hot encoded representation as is, the latter will embed the feature into a dense representation of specified dimensionality (the 4th root of the number of categories is a good starting point for number of dimensions). Read more about indicator and embedding columns [here](https://www.tensorflow.org/guide/feature_columns#indicator_and_embedding_columns).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Create a list containing all the necessary feature columns for our model, including the new feature columns we created above. \n",
    "\n",
    "Hint: You will need to use an `indicator_column()` to wrap any categorial or crossed feature columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    "  #1. Engineered using tf.feature_column module\n",
    "  # TODO: Add here all the features columns we created above using the tf.feature_column module\n",
    "  #2. Engineered in input functions\n",
    "  # TODO: Add here new feature columns from the values that were created with our 'add_engineered_features' funcion.\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Serving Input Receiver Function \n",
    "\n",
    "Same as before except the received tensors are wrapped with `add_engineered_features()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "When building the `serving_input_receiver_fn()` we need to add our engineered features to the dictionary of `receiver_tensors` we will recieve at inference time. Complete the code to achieve this by using the `add_engineered_features` function from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_receiver_fn():\n",
    "    receiver_tensors = {\n",
    "        'dayofweek' : tf.placeholder(tf.int32, shape=[None]), # shape is vector to allow batch of requests\n",
    "        'hourofday' : tf.placeholder(tf.int32, shape=[None]),\n",
    "        'pickuplon' : tf.placeholder(tf.float32, shape=[None]), \n",
    "        'pickuplat' : tf.placeholder(tf.float32, shape=[None]),\n",
    "        'dropofflat' : tf.placeholder(tf.float32, shape=[None]),\n",
    "        'dropofflon' : tf.placeholder(tf.float32, shape=[None]),\n",
    "    }\n",
    "    \n",
    "    features = # TODO: Add your code here. The result should be a dictionary.\n",
    "    \n",
    "    return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Train and Evaluate\n",
    "\n",
    "Same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.datalab.ml import TensorBoard\n",
    "TensorBoard().start('taxi_trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "OUTDIR = 'taxi_trained/500'\n",
    "shutil.rmtree(OUTDIR, ignore_errors = True) # start fresh each time\n",
    "\n",
    "model = tf.estimator.DNNRegressor(\n",
    "    hidden_units = [10,10], # specify neural architecture\n",
    "    feature_columns = feature_cols, \n",
    "    model_dir = OUTDIR,\n",
    "    config = tf.estimator.RunConfig(\n",
    "          tf_random_seed=1, # for reproducibility\n",
    "          save_checkpoints_steps=100 # checkpoint every N steps\n",
    "    ) \n",
    ")\n",
    "\n",
    "# Add custom evaluation metric\n",
    "def my_rmse(labels, predictions):\n",
    "    pred_values = tf.squeeze(predictions['predictions'],axis=-1)\n",
    "    return {'rmse': tf.metrics.root_mean_squared_error(labels, pred_values)}\n",
    "model = tf.contrib.estimator.add_metrics(model, my_rmse) \n",
    "    \n",
    "train_spec=tf.estimator.TrainSpec(\n",
    "                   input_fn = lambda:train_input_fn('./taxi-train.csv'),\n",
    "                   max_steps = 500)\n",
    "\n",
    "exporter = tf.estimator.FinalExporter('exporter', serving_input_receiver_fn) # export SavedModel once at the end of training\n",
    "# Note: alternatively use tf.estimator.BestExporter to export at every checkpoint that has lower loss than the previous checkpoint\n",
    "\n",
    "eval_spec=tf.estimator.EvalSpec(\n",
    "                   input_fn=lambda:eval_input_fn('./taxi-valid.csv'),\n",
    "                   steps = None,\n",
    "                   start_delay_secs=1, # wait at least N seconds before first evaluation (default 120)\n",
    "                   throttle_secs=1, # wait at least N seconds before each subsequent evaluation (default 600)\n",
    "                   exporters = exporter) # export SavedModel once at the end of training\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO) # so loss is printed during training\n",
    "shutil.rmtree(OUTDIR, ignore_errors = True) # start fresh each time\n",
    "tf.estimator.train_and_evaluate(model, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Our RMSE is now **5.94**, our first significant improvement! If we look at the RMSE trend in TensorBoard it appears the model is still learning, so training past 500 steps would likely lower the RMSE even more. Let's run again, this time for 10x as many steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "OUTDIR = 'taxi_trained/5000'\n",
    "shutil.rmtree(OUTDIR, ignore_errors = True) # start fresh each time\n",
    "\n",
    "model = tf.estimator.DNNRegressor(\n",
    "    hidden_units = [10,10], # specify neural architecture\n",
    "    feature_columns = feature_cols, \n",
    "    model_dir = OUTDIR,\n",
    "    config = tf.estimator.RunConfig(\n",
    "          tf_random_seed=1, # for reproducibility\n",
    "          save_checkpoints_steps=500 # checkpoint every N steps\n",
    "    ) \n",
    ")\n",
    "\n",
    "# Add custom evaluation metric\n",
    "def my_rmse(labels, predictions):\n",
    "    pred_values = tf.squeeze(predictions['predictions'],axis=-1)\n",
    "    return {'rmse': tf.metrics.root_mean_squared_error(labels, pred_values)}\n",
    "model = tf.contrib.estimator.add_metrics(model, my_rmse) \n",
    "    \n",
    "train_spec=tf.estimator.TrainSpec(\n",
    "                   input_fn = lambda:train_input_fn('./taxi-train.csv'),\n",
    "                   max_steps = 5000)\n",
    "\n",
    "exporter = tf.estimator.FinalExporter('exporter', serving_input_receiver_fn) # export SavedModel once at the end of training\n",
    "\n",
    "eval_spec=tf.estimator.EvalSpec(\n",
    "                   input_fn=lambda:eval_input_fn('./taxi-valid.csv'),\n",
    "                   steps = None,\n",
    "                   start_delay_secs=1, # wait at least N seconds before first evaluation (default 120)\n",
    "                   throttle_secs=1, # wait at least N seconds before each subsequent evaluation (default 600)\n",
    "                   exporters = exporter) # export SavedModel once at the end of training\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO) # so loss is printed during training\n",
    "shutil.rmtree(OUTDIR, ignore_errors = True) # start fresh each time\n",
    "tf.estimator.train_and_evaluate(model, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results: 5000 steps\n",
    "\n",
    "Our RMSE is now **4.13**! It looks like RMSE may still be reducing, but training is getting slow so we should move to the cloud if we want to train longer.\n",
    "\n",
    "Also we haven't explored our hyperparameters much. Is our neural architecture of [10,10] optimal? \n",
    "\n",
    "In the next notebook we'll explore this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(TensorBoard.list())>0:\n",
    "  [TensorBoard().stop(pid)for pid in TensorBoard.list()['pid']]\n",
    "else: print('No TensorBoard instances to stop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2019 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
