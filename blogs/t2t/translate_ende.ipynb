{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation using tensor2tensor on Cloud ML Engine\n",
    "\n",
    "This notebook illustrates using the <a href=\"https://github.com/tensorflow/tensor2tensor\">tensor2tensor</a> library to do from-scratch, distributed training of a English-German translator. Then, the trained model is deployed to Cloud ML Engine and used to translate new pieces of text.\n",
    "<p/>\n",
    "### Install tensor2tensor, and specify Google Cloud Platform project and bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "pip install tensor2tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT = 'cloud-training-demos' # REPLACE WITH YOUR PROJECT ID\n",
    "BUCKET = 'cloud-training-demos-ml' # REPLACE WITH YOUR BUCKET NAME\n",
    "REGION = 'us-central1' # REPLACE WITH YOUR BUCKET REGION e.g. us-central1\n",
    "\n",
    "# for bash\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load bucket with data\n",
    "\n",
    "We'll put the training dataset on cloud storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "wget http://data.statmt.org/wmt17/translation-task/training-parallel-nc-v12.tgz\n",
    "wget http://data.statmt.org/wmt17/translation-task/dev.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "gsutil cp -m training-parallel-nc-v12.tgz dev.tgz gs://${BUCKET}/translate_ende/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a Problem\n",
    "The Problem in tensor2tensor is where you specify parameters like the size of your vocabulary and where to get the training data from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensor2tensor.data_generators import generator_utils\n",
    "from tensor2tensor.data_generators import problem\n",
    "from tensor2tensor.data_generators import text_encoder\n",
    "from tensor2tensor.data_generators import wsj_parsing\n",
    "from tensor2tensor.data_generators.wmt import TranslateProblem\n",
    "from tensor2tensor.utils import registry\n",
    "\n",
    "_ENDE_TRAIN_DATASETS = [\n",
    "    [\n",
    "        \"./training-parallel-nc-v12.tgz\",\n",
    "        (\"training/news-commentary-v12.de-en.en\",\n",
    "         \"training/news-commentary-v12.de-en.de\")\n",
    "    ],\n",
    "]\n",
    "_ENDE_TEST_DATASETS = [\n",
    "    [\n",
    "        \"./dev.tgz\",\n",
    "        (\"dev/newstest2013.en\", \"dev/newstest2013.de\")\n",
    "    ],\n",
    "]\n",
    "\n",
    "@registry.register_problem\n",
    "class MyTranslateProblem(TranslateProblem):\n",
    "  @property\n",
    "  def targeted_vocab_size(self):\n",
    "    return 2**13  # 8192\n",
    "\n",
    "  def generator(self, data_dir, tmp_dir, train):\n",
    "    symbolizer_vocab = generator_utils.get_or_generate_vocab(\n",
    "        data_dir, tmp_dir, self.vocab_file, self.targeted_vocab_size)\n",
    "    datasets = _ENDE_TRAIN_DATASETS if train else _ENDE_TEST_DATASETS\n",
    "    tag = \"train\" if train else \"dev\"\n",
    "    data_path = _compile_data(tmp_dir, datasets, \"wmt_ende_tok_%s\" % tag)\n",
    "    return token_generator(data_path + \".lang1\", data_path + \".lang2\",\n",
    "                           symbolizer_vocab, EOS)\n",
    "\n",
    "  @property\n",
    "  def input_space_id(self):\n",
    "    return problem.SpaceID.EN_TOK\n",
    "\n",
    "  @property\n",
    "  def target_space_id(self):\n",
    "    return problem.SpaceID.DE_TOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/t2t-datagen\", line 213, in <module>\n",
      "    tf.app.run()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 48, in run\n",
      "    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\n",
      "  File \"/usr/local/bin/t2t-datagen\", line 160, in main\n",
      "    raise ValueError(error_msg)\n",
      "ValueError: You must specify one of the supported problems to generate data for:\n",
      "  * algorithmic_addition_binary40\n",
      "  * algorithmic_addition_decimal40\n",
      "  * algorithmic_algebra_inverse\n",
      "  * algorithmic_cipher_shift200\n",
      "  * algorithmic_cipher_shift5\n",
      "  * algorithmic_cipher_vigenere200\n",
      "  * algorithmic_cipher_vigenere5\n",
      "  * algorithmic_identity_binary40\n",
      "  * algorithmic_identity_decimal40\n",
      "  * algorithmic_multiplication_binary40\n",
      "  * algorithmic_multiplication_decimal40\n",
      "  * algorithmic_reverse_binary40\n",
      "  * algorithmic_reverse_decimal40\n",
      "  * algorithmic_reverse_nlplike32k\n",
      "  * algorithmic_reverse_nlplike8k\n",
      "  * algorithmic_shift_decimal40\n",
      "  * audio_timit_characters_test\n",
      "  * audio_timit_characters_tune\n",
      "  * audio_timit_tokens8k_test\n",
      "  * audio_timit_tokens8k_tune\n",
      "  * audio_timit_tokens_32k_test\n",
      "  * audio_timit_tokens_8k_test\n",
      "  * image_celeba_tune\n",
      "  * image_cifar10\n",
      "  * image_cifar10_plain\n",
      "  * image_cifar10_tune\n",
      "  * image_fsns\n",
      "  * image_imagenet\n",
      "  * image_imagenet32\n",
      "  * image_mnist\n",
      "  * image_mnist_tune\n",
      "  * image_ms_coco_characters\n",
      "  * image_ms_coco_tokens32k\n",
      "  * image_ms_coco_tokens8k\n",
      "  * img2img_imagenet\n",
      "  * inference_snli32k\n",
      "  * languagemodel_lm1b32k\n",
      "  * languagemodel_lm1b_characters\n",
      "  * languagemodel_ptb10k\n",
      "  * languagemodel_ptb_characters\n",
      "  * languagemodel_wiki_full32k\n",
      "  * languagemodel_wiki_scramble128\n",
      "  * languagemodel_wiki_scramble1k50\n",
      "  * languagemodel_wiki_scramble8k50\n",
      "  * parsing_english_ptb16k\n",
      "  * parsing_english_ptb16k\n",
      "  * parsing_english_ptb8k\n",
      "  * parsing_english_ptb8k\n",
      "  * parsing_icelandic16k\n",
      "  * programming_desc2code_cpp\n",
      "  * programming_desc2code_py\n",
      "  * sentiment_imdb\n",
      "  * summarize_cnn_dailymail32k\n",
      "  * translate_encs_wmt32k\n",
      "  * translate_encs_wmt_characters\n",
      "  * translate_ende_wmt32k\n",
      "  * translate_ende_wmt8k\n",
      "  * translate_ende_wmt_bpe32k\n",
      "  * translate_ende_wmt_characters\n",
      "  * translate_enfr_wmt32k\n",
      "  * translate_enfr_wmt8k\n",
      "  * translate_enfr_wmt_characters\n",
      "  * translate_enmk_setimes32k\n",
      "  * translate_enzh_wmt8k\n",
      "TIMIT and parsing need data_sets specified with --timit_paths and --parsing_path.\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "PROBLEM=MyTranslateProblem\n",
    "DATA_DIR=./t2t_data\n",
    "TMP_DIR=/tmp/t2t_datagen\n",
    "rm -rf $DATA_DIR $TMP_DIR\n",
    "mkdir -p $DATA_DIR $TMP_DIR\n",
    "# Generate data\n",
    "t2t-datagen \\\n",
    "  --data_dir=$DATA_DIR \\\n",
    "  --tmp_dir=$TMP_DIR \\\n",
    "  --problem=$PROBLEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\n",
      "Registry contents:\n",
      "------------------\n",
      "\n",
      "  Models:\n",
      "    aligned:\n",
      "      * aligned\n",
      "    attention:\n",
      "      * attention_lm\n",
      "      * attention_lm_moe\n",
      "    blue:\n",
      "      * blue_net\n",
      "    byte:\n",
      "      * byte_net\n",
      "    cycle:\n",
      "      * cycle_gan\n",
      "    diagonal:\n",
      "      * diagonal_neural_gpu\n",
      "    gene:\n",
      "      * gene_expression_conv\n",
      "    lstm:\n",
      "      * lstm_seq2seq\n",
      "      * lstm_seq2seq_attention\n",
      "    multi:\n",
      "      * multi_model\n",
      "    neural:\n",
      "      * neural_gpu\n",
      "    shake:\n",
      "      * shake_shake\n",
      "    slice:\n",
      "      * slice_net\n",
      "    transformer:\n",
      "      * transformer\n",
      "      * transformer_ae\n",
      "      * transformer_alt\n",
      "      * transformer_decoder\n",
      "      * transformer_encoder\n",
      "      * transformer_moe\n",
      "      * transformer_revnet\n",
      "    xception:\n",
      "      * xception\n",
      "\n",
      "  HParams:\n",
      "    aligned:\n",
      "      * aligned_8k\n",
      "      * aligned_base\n",
      "      * aligned_local\n",
      "      * aligned_local_1k\n",
      "      * aligned_local_expert\n",
      "      * aligned_memory_efficient\n",
      "      * aligned_moe\n",
      "      * aligned_no_att\n",
      "      * aligned_no_timing\n",
      "      * aligned_pos_emb\n",
      "      * aligned_pseudolocal\n",
      "      * aligned_pseudolocal_256\n",
      "    attention:\n",
      "      * attention_lm_attention_moe_tiny\n",
      "      * attention_lm_base\n",
      "      * attention_lm_moe_24b_diet\n",
      "      * attention_lm_moe_32b_diet\n",
      "      * attention_lm_moe_base\n",
      "      * attention_lm_moe_base_ae\n",
      "      * attention_lm_moe_base_hybrid\n",
      "      * attention_lm_moe_base_local\n",
      "      * attention_lm_moe_base_long_seq\n",
      "      * attention_lm_moe_large\n",
      "      * attention_lm_moe_large_diet\n",
      "      * attention_lm_moe_memory_efficient\n",
      "      * attention_lm_moe_small\n",
      "      * attention_lm_moe_tiny\n",
      "      * attention_lm_moe_translation\n",
      "      * attention_lm_moe_unscramble_base\n",
      "      * attention_lm_no_moe_small\n",
      "      * attention_lm_small\n",
      "      * attention_lm_translation\n",
      "      * attention_lm_translation_full_attention\n",
      "      * attention_lm_translation_l12\n",
      "    basic:\n",
      "      * basic_1\n",
      "    bluenet:\n",
      "      * bluenet_base\n",
      "      * bluenet_tiny\n",
      "    bytenet:\n",
      "      * bytenet_base\n",
      "    cycle:\n",
      "      * cycle_gan_small\n",
      "    gene:\n",
      "      * gene_expression_conv_base\n",
      "    lstm:\n",
      "      * lstm_attention\n",
      "      * lstm_seq2seq\n",
      "    multimodel:\n",
      "      * multimodel_base\n",
      "      * multimodel_tiny\n",
      "    neuralgpu:\n",
      "      * neuralgpu_1\n",
      "    shakeshake:\n",
      "      * shakeshake_cifar10\n",
      "    slicenet:\n",
      "      * slicenet_1\n",
      "      * slicenet_1noam\n",
      "      * slicenet_1tiny\n",
      "    transformer:\n",
      "      * transformer_ae_base\n",
      "      * transformer_ae_cifar\n",
      "      * transformer_ae_small\n",
      "      * transformer_alt\n",
      "      * transformer_base\n",
      "      * transformer_base_single_gpu\n",
      "      * transformer_big\n",
      "      * transformer_big_dr1\n",
      "      * transformer_big_dr2\n",
      "      * transformer_big_enfr\n",
      "      * transformer_big_single_gpu\n",
      "      * transformer_dr0\n",
      "      * transformer_dr2\n",
      "      * transformer_ff1024\n",
      "      * transformer_ff4096\n",
      "      * transformer_h1\n",
      "      * transformer_h16\n",
      "      * transformer_h32\n",
      "      * transformer_h4\n",
      "      * transformer_hs1024\n",
      "      * transformer_hs256\n",
      "      * transformer_k128\n",
      "      * transformer_k256\n",
      "      * transformer_l10\n",
      "      * transformer_l2\n",
      "      * transformer_l4\n",
      "      * transformer_l8\n",
      "      * transformer_ls0\n",
      "      * transformer_ls2\n",
      "      * transformer_moe_1b\n",
      "      * transformer_moe_base\n",
      "      * transformer_n_da\n",
      "      * transformer_n_da_l10\n",
      "      * transformer_no_moe\n",
      "      * transformer_parameter_attention_a\n",
      "      * transformer_parameter_attention_b\n",
      "      * transformer_parsing_base\n",
      "      * transformer_parsing_big\n",
      "      * transformer_parsing_ice\n",
      "      * transformer_prepend\n",
      "      * transformer_revnet_base\n",
      "      * transformer_revnet_big\n",
      "      * transformer_small\n",
      "      * transformer_tiny\n",
      "    xception:\n",
      "      * xception_base\n",
      "      * xception_tiny\n",
      "\n",
      "  RangedHParams:\n",
      "    basic1:\n",
      "      * basic1\n",
      "    slicenet1:\n",
      "      * slicenet1\n",
      "    transformer:\n",
      "      * transformer_base\n",
      "\n",
      "  Modalities:\n",
      "    audio:audio:\n",
      "      * audio:audio_spectral_modality\n",
      "    audio:default:\n",
      "      * audio:default\n",
      "    audio:identity:\n",
      "      * audio:identity\n",
      "    class:\n",
      "      * class_label:2d\n",
      "      * class_label:default\n",
      "      * class_label:identity\n",
      "    generic:default:\n",
      "      * generic:default\n",
      "    image:default:\n",
      "      * image:default\n",
      "    image:identity:\n",
      "      * image:identity\n",
      "      * image:identity_no_pad\n",
      "    image:small:\n",
      "      * image:small_image_modality\n",
      "    real:default:\n",
      "      * real:default\n",
      "    real:identity:\n",
      "      * real:identity\n",
      "    real:l2:\n",
      "      * real:l2_loss\n",
      "    real:log:\n",
      "      * real:log_poisson_loss\n",
      "    symbol:default:\n",
      "      * symbol:default\n",
      "    symbol:identity:\n",
      "      * symbol:identity\n",
      "\n",
      "  Problems:\n",
      "    algorithmic:\n",
      "      * algorithmic_addition_binary40\n",
      "      * algorithmic_addition_decimal40\n",
      "      * algorithmic_cipher_shift200\n",
      "      * algorithmic_cipher_shift5\n",
      "      * algorithmic_cipher_vigenere200\n",
      "      * algorithmic_cipher_vigenere5\n",
      "      * algorithmic_identity_binary40\n",
      "      * algorithmic_identity_decimal40\n",
      "      * algorithmic_multiplication_binary40\n",
      "      * algorithmic_multiplication_decimal40\n",
      "      * algorithmic_reverse_binary40\n",
      "      * algorithmic_reverse_decimal40\n",
      "      * algorithmic_reverse_nlplike32k\n",
      "      * algorithmic_reverse_nlplike8k\n",
      "      * algorithmic_shift_decimal40\n",
      "    audio:\n",
      "      * audio_timit_characters_tune\n",
      "      * audio_timit_tokens8k_test\n",
      "      * audio_timit_tokens8k_tune\n",
      "    image:\n",
      "      * image_celeba_tune\n",
      "      * image_cifar10\n",
      "      * image_cifar10_plain\n",
      "      * image_cifar10_tune\n",
      "      * image_fsns\n",
      "      * image_imagenet\n",
      "      * image_imagenet32\n",
      "      * image_mnist\n",
      "      * image_mnist_tune\n",
      "      * image_ms_coco_characters\n",
      "      * image_ms_coco_tokens32k\n",
      "      * image_ms_coco_tokens8k\n",
      "    img2img:\n",
      "      * img2img_imagenet\n",
      "    languagemodel:\n",
      "      * languagemodel_lm1b32k\n",
      "      * languagemodel_lm1b_characters\n",
      "      * languagemodel_ptb10k\n",
      "      * languagemodel_ptb_characters\n",
      "      * languagemodel_wiki_full32k\n",
      "      * languagemodel_wiki_scramble128\n",
      "      * languagemodel_wiki_scramble1k50\n",
      "      * languagemodel_wiki_scramble8k50\n",
      "    parsing:\n",
      "      * parsing_english_ptb16k\n",
      "      * parsing_english_ptb8k\n",
      "      * parsing_icelandic16k\n",
      "    programming:\n",
      "      * programming_desc2code_cpp\n",
      "      * programming_desc2code_py\n",
      "    sentiment:\n",
      "      * sentiment_imdb\n",
      "    summarize:\n",
      "      * summarize_cnn_dailymail32k\n",
      "    translate:\n",
      "      * translate_encs_wmt32k\n",
      "      * translate_encs_wmt_characters\n",
      "      * translate_ende_wmt32k\n",
      "      * translate_ende_wmt8k\n",
      "      * translate_ende_wmt_bpe32k\n",
      "      * translate_ende_wmt_characters\n",
      "      * translate_enfr_wmt32k\n",
      "      * translate_enfr_wmt8k\n",
      "      * translate_enfr_wmt_characters\n",
      "      * translate_enmk_setimes32k\n",
      "      * translate_enzh_wmt8k\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "t2t-trainer --registry_help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
